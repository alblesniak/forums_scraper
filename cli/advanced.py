"""
Zaawansowany interfejs CLI dla forums-scraper z pe≈ÇnƒÖ konfiguracjƒÖ i raportowaniem.
"""

from __future__ import annotations

import json
import subprocess
import sys
import time
from datetime import datetime
from enum import Enum
from pathlib import Path
import os
from typing import Annotated, Dict, List, Optional, Set

import typer
from rich.console import Console
from rich.panel import Panel
from rich.progress import (
    BarColumn, 
    MofNCompleteColumn, 
    Progress, 
    SpinnerColumn, 
    TextColumn, 
    TimeElapsedColumn
)
from rich.table import Table
from rich.text import Text

from core.config import AppConfig, load_config


console = Console()
app = typer.Typer(
    name="forums-scraper",
    help="üï∑Ô∏è Zaawansowany scraper for√≥w religijnych z analizami NLP",
    add_completion=False,
    no_args_is_help=True,
    rich_markup_mode="rich"
)


class AnalysisType(str, Enum):
    """Typy dostƒôpnych analiz."""
    NONE = "none"
    BASIC_TOKENS = "basic_tokens"
    TOKEN_COUNT = "token_count"
    SPACY_FULL = "spacy_full"
    URL_ANALYSIS = "url_analysis"
    DOMAIN_STATS = "domain_stats"
    ALL = "all"


class ForumName(str, Enum):
    """Dostƒôpne fora do scrapowania."""
    DOLINA_MODLITWY = "dolina_modlitwy"
    RADIO_KATOLIK = "radio_katolik"
    WIARA = "wiara"
    Z_CHRYSTUSEM = "z_chrystusem"
    ALL = "all"


# Mapowanie nazw for√≥w na nazwy spider√≥w
FORUM_SPIDER_MAP = {
    ForumName.DOLINA_MODLITWY: "dolina_modlitwy",
    ForumName.RADIO_KATOLIK: "radio_katolik", 
    ForumName.WIARA: "wiara",
    ForumName.Z_CHRYSTUSEM: "z_chrystusem"
}


def create_analysis_config(
    analysis_types: List[AnalysisType],
    spacy_model: str = "pl_core_news_sm",
    include_sentiment: bool = False,
    batch_size: int = 100
) -> Dict:
    """Tworzy konfiguracjƒô analiz na podstawie wybranych typ√≥w."""
    if AnalysisType.NONE in analysis_types:
        return {
            "analysis": {
                "enabled": False,
                "analyzers": []
            }
        }
    
    analyzers = []
    
    if AnalysisType.TOKEN_COUNT in analysis_types or AnalysisType.ALL in analysis_types:
        analyzers.append({
            "name": "token_count",
            "config": {
                "encoding": "cl100k_base"
            }
        })
    
    # BasicTokenizer tylko je≈õli nie ma spaCy lub explicitly requested
    if (AnalysisType.BASIC_TOKENS in analysis_types and 
        AnalysisType.SPACY_FULL not in analysis_types and 
        AnalysisType.ALL not in analysis_types):
        analyzers.append({
            "name": "basic_tokenizer",
            "config": {
                "lowercase": True,
                "remove_punctuation": False,
                "min_token_length": 2
            }
        })
    
    if AnalysisType.SPACY_FULL in analysis_types or AnalysisType.ALL in analysis_types:
        analyzers.append({
            "name": "spacy_analyzer",
            "config": {
                "model": spacy_model,
                "include_sentiment": include_sentiment,
                "batch_size": batch_size,
                "max_length": 1000000
            }
        })
    
    if AnalysisType.URL_ANALYSIS in analysis_types or AnalysisType.ALL in analysis_types:
        analyzers.append({
            "name": "url_analyzer",
            "config": {
                "include_domain_analysis": True,
                "include_url_categorization": True,
                "max_urls_per_post": 50
            }
        })
    
    if AnalysisType.DOMAIN_STATS in analysis_types or AnalysisType.ALL in analysis_types:
        analyzers.append({
            "name": "domain_stats",
            "config": {
                "track_popularity": True
            }
        })
    
    return {
        "analysis": {
            "enabled": True,
            "analyzers": analyzers
        }
    }


def save_config_file(config: Dict, path: Path) -> None:
    """Zapisuje konfiguracjƒô do pliku YAML."""
    import yaml
    
    with open(path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)


def get_available_spiders() -> List[str]:
    """Pobiera listƒô dostƒôpnych spider√≥w."""
    try:
        result = subprocess.run(
            ["scrapy", "list"], 
            capture_output=True, 
            text=True, 
            check=True
        )
        return result.stdout.strip().split('\n')
    except (subprocess.CalledProcessError, FileNotFoundError):
        return list(FORUM_SPIDER_MAP.values())


def display_analysis_summary(analysis_types: List[AnalysisType]) -> None:
    """Wy≈õwietla podsumowanie wybranych analiz."""
    table = Table(title="üî¨ Wybrane analizy", show_header=True, header_style="bold magenta")
    table.add_column("Typ analizy", style="cyan")
    table.add_column("Opis", style="white")
    table.add_column("Wymagania", style="yellow")
    
    for analysis_type in analysis_types:
        if analysis_type == AnalysisType.NONE:
            table.add_row("Brak", "Tylko scrapowanie bez analiz", "Brak")
        elif analysis_type == AnalysisType.TOKEN_COUNT:
            table.add_row("Liczenie token√≥w", "Liczba token√≥w OpenAI", "tiktoken")
        elif analysis_type == AnalysisType.BASIC_TOKENS:
            table.add_row("Podstawowa tokenizacja", "Podzia≈Ç na s≈Çowa + statystyki", "Brak")
        elif analysis_type == AnalysisType.SPACY_FULL:
            table.add_row("Pe≈Çna analiza spaCy", "Lematyzacja, POS, sk≈Çadnia, sentyment", "spacy + model")
        elif analysis_type == AnalysisType.URL_ANALYSIS:
            table.add_row("Analiza URL-√≥w", "Kategoryzacja domen i link√≥w", "Brak")
        elif analysis_type == AnalysisType.DOMAIN_STATS:
            table.add_row("Statystyki domen", "Podstawowe statystyki link√≥w", "Brak")
        elif analysis_type == AnalysisType.ALL:
            table.add_row("Wszystkie analizy", "Kompletna analiza + URL-e", "tiktoken + spacy")
    
    console.print(table)


def display_forum_summary(forums: List[ForumName]) -> None:
    """Wy≈õwietla podsumowanie wybranych for√≥w."""
    table = Table(title="üèõÔ∏è Wybrane fora", show_header=True, header_style="bold blue")
    table.add_column("Forum", style="cyan")
    table.add_column("Spider", style="white")
    table.add_column("Opis", style="green")
    
    forum_descriptions = {
        ForumName.DOLINA_MODLITWY: "Forum katolickie - Dolina Modlitwy",
        ForumName.RADIO_KATOLIK: "Forum Radia Katolik",
        ForumName.WIARA: "Forum Wiara.pl",
        ForumName.Z_CHRYSTUSEM: "Forum Z Chrystusem"
    }
    
    for forum in forums:
        if forum == ForumName.ALL:
            for f in ForumName:
                if f != ForumName.ALL:
                    spider = FORUM_SPIDER_MAP[f]
                    desc = forum_descriptions.get(f, "Brak opisu")
                    table.add_row(f.value, spider, desc)
        else:
            spider = FORUM_SPIDER_MAP[forum]
            desc = forum_descriptions.get(forum, "Brak opisu")
            table.add_row(forum.value, spider, desc)
    
    console.print(table)


@app.command(name="scrape")
def scrape_forums(
    forums: Annotated[List[ForumName], typer.Option(
        "--forum", "-f",
        help="Wybierz fora do scrapowania (mo≈ºna wybraƒá wiele)"
    )] = [ForumName.ALL],
    
    analysis: Annotated[List[AnalysisType], typer.Option(
        "--analysis", "-a",
        help="Rodzaje analiz do wykonania (mo≈ºna wybraƒá wiele)"
    )] = [AnalysisType.BASIC_TOKENS],
    
    config_file: Annotated[Optional[Path], typer.Option(
        "--config", "-c",
        help="≈öcie≈ºka do pliku konfiguracyjnego YAML"
    )] = None,
    
    output_dir: Annotated[Path, typer.Option(
        "--output", "-o",
        help="Katalog wyj≈õciowy dla baz danych"
    )] = Path("data/databases"),
    
    spacy_model: Annotated[str, typer.Option(
        "--spacy-model",
        help="Model spaCy do analizy jƒôzykowej"
    )] = "pl_core_news_lg",
    
    include_sentiment: Annotated[bool, typer.Option(
        "--sentiment/--no-sentiment",
        help="W≈ÇƒÖcz analizƒô sentymentu"
    )] = False,
    
    batch_size: Annotated[int, typer.Option(
        "--batch-size",
        help="Rozmiar batcha dla analiz",
        min=1, max=1000
    )] = 100,
    
    concurrent_requests: Annotated[int, typer.Option(
        "--concurrent",
        help="Liczba r√≥wnoleg≈Çych ≈ºƒÖda≈Ñ",
        min=1, max=64
    )] = 16,
    
    download_delay: Annotated[float, typer.Option(
        "--delay",
        help="Op√≥≈∫nienie miƒôdzy ≈ºƒÖdaniami (sekundy)",
        min=0.0, max=10.0
    )] = 0.5,
    
    dry_run: Annotated[bool, typer.Option(
        "--dry-run",
        help="Tylko poka≈º co zostanie wykonane, nie uruchamiaj"
    )] = False,
    
    verbose: Annotated[bool, typer.Option(
        "--verbose", "-v",
        help="Szczeg√≥≈Çowe logowanie"
    )] = False
):
    """
    üï∑Ô∏è Scrapuj wybrane fora z opcjonalnymi analizami NLP.
    
    Przyk≈Çady u≈ºycia:
    
    ‚Ä¢ Scrapuj wszystkie fora z podstawowƒÖ tokenizacjƒÖ:
      [cyan]forums-scraper scrape[/cyan]
    
    ‚Ä¢ Scrapuj tylko Radio Katolik z pe≈ÇnƒÖ analizƒÖ spaCy:
      [cyan]forums-scraper scrape -f radio_katolik -a spacy_full[/cyan]
    
    ‚Ä¢ Scrapuj wybrane fora z wszystkimi analizami:
      [cyan]forums-scraper scrape -f wiara -f dolina_modlitwy -a all --sentiment[/cyan]
    """
    
    console.print(Panel.fit(
        "üï∑Ô∏è [bold blue]Forums Scraper[/bold blue] - Zaawansowany scraper for√≥w religijnych",
        style="blue"
    ))
    
    # Rozwi≈Ñ 'all' na wszystkie fora
    if ForumName.ALL in forums:
        forums = [f for f in ForumName if f != ForumName.ALL]
    
    # Wy≈õwietl podsumowanie
    display_forum_summary(forums)
    display_analysis_summary(analysis)
    
    # Utw√≥rz katalog wyj≈õciowy
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Utw√≥rz konfiguracjƒô
    if not config_file:
        config = create_analysis_config(analysis, spacy_model, include_sentiment, batch_size)
        config_file = output_dir / "scraper_config.yaml"
        save_config_file(config, config_file)
        console.print(f"üìù Utworzono plik konfiguracyjny: [cyan]{config_file}[/cyan]")
    
    # Wy≈õwietl plan dzia≈Çania
    unified_db_path = output_dir / "forums_unified.db"
    console.print("\nüéØ [bold]Plan dzia≈Çania:[/bold]")
    console.print(f"üìä [bold]Wsp√≥lna baza danych:[/bold] [yellow]{unified_db_path}[/yellow]")
    for i, forum in enumerate(forums, 1):
        spider_name = FORUM_SPIDER_MAP[forum]
        console.print(f"  {i}. [cyan]{forum.value}[/cyan] (spider: {spider_name})")
    
    if dry_run:
        console.print("\n[yellow]üîç Tryb dry-run - nie wykonujƒô scrapowania[/yellow]")
        return
    
    # Potwierd≈∫ wykonanie
    if not typer.confirm("\n‚ùì Czy chcesz kontynuowaƒá?"):
        console.print("[red]‚ùå Anulowano[/red]")
        return
    
    # Wykonaj scrapowanie
    console.print("\nüöÄ [bold green]Rozpoczynam scrapowanie...[/bold green]")
    
    total_forums = len(forums)
    failed_forums = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),
        TimeElapsedColumn(),
        console=console,
    ) as progress:
        
        main_task = progress.add_task("Scrapowanie for√≥w", total=total_forums)
        
        for forum in forums:
            spider_name = FORUM_SPIDER_MAP[forum]
            
            progress.update(
                main_task, 
                description=f"Scrapujƒô [cyan]{forum.value}[/cyan] ({spider_name})"
            )
            
            try:
                # Przygotuj argumenty dla Scrapy
                scrapy_args = [
                    "scrapy", "crawl", spider_name,
                    "-s", f"FS_CONFIG_PATH={config_file}",
                    "-s", f"SQLITE_DATABASE_PATH={unified_db_path}",
                    "-s", f"CONCURRENT_REQUESTS={concurrent_requests}",
                    "-s", f"DOWNLOAD_DELAY={download_delay}",
                ]
                
                if verbose:
                    scrapy_args.extend(["-s", "LOG_LEVEL=INFO"])
                else:
                    scrapy_args.extend(["-s", "LOG_LEVEL=WARNING"])
                
                # Uruchom Scrapy
                console.print(f"‚ñ∂Ô∏è  Uruchamiam: [dim]{' '.join(scrapy_args)}[/dim]")
                
                # Uruchamiaj z rootu projektu, aby scrapy.cfg by≈Ç widoczny
                project_root = Path(__file__).resolve().parents[1]
                env = os.environ.copy()
                env.setdefault("SCRAPY_SETTINGS_MODULE", "forums_scraper.settings")
                result = subprocess.run(
                    scrapy_args,
                    cwd=str(project_root),
                    env=env,
                    capture_output=not verbose,
                    text=True,
                    check=True
                )
                
                console.print(f"‚úÖ [green]{forum.value} - zako≈Ñczono pomy≈õlnie[/green]")
                
            except subprocess.CalledProcessError as e:
                console.print(f"‚ùå [red]{forum.value} - b≈ÇƒÖd: {e}[/red]")
                failed_forums.append(forum.value)
            
            except KeyboardInterrupt:
                console.print(f"\n[yellow]‚èπÔ∏è  Przerwano przez u≈ºytkownika[/yellow]")
                break
                
            progress.advance(main_task)
    
    # Podsumowanie
    console.print(f"\nüéâ [bold green]Scrapowanie zako≈Ñczone![/bold green]")
    
    successful_count = total_forums - len(failed_forums)
    console.print(f"‚úÖ Pomy≈õlnie: [green]{successful_count}/{total_forums}[/green]")
    
    if failed_forums:
        console.print(f"‚ùå Niepowodzenia: [red]{len(failed_forums)}[/red]")
        for forum in failed_forums:
            console.print(f"   ‚Ä¢ {forum}")
    
    # Wy≈õwietl informacje o bazie danych
    console.print(f"\nüìä [bold]Wsp√≥lna baza danych:[/bold]")
    if unified_db_path.exists():
        size_mb = unified_db_path.stat().st_size / 1024 / 1024
        console.print(f"   üìÅ [cyan]{unified_db_path.name}[/cyan] ([yellow]{size_mb:.1f} MB[/yellow])")
        
        # Poka≈º statystyki per forum je≈õli baza istnieje
        try:
            import sqlite3
            with sqlite3.connect(unified_db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT f.spider_name, f.title, COUNT(p.id) as posts_count
                    FROM forums f
                    LEFT JOIN sections s ON f.id = s.forum_id
                    LEFT JOIN threads t ON s.id = t.section_id
                    LEFT JOIN posts p ON t.id = p.thread_id
                    GROUP BY f.id, f.spider_name, f.title
                    ORDER BY posts_count DESC
                """)
                results = cursor.fetchall()
                
                if results:
                    console.print("\nüìà [bold]Statystyki per forum:[/bold]")
                    for spider_name, title, posts_count in results:
                        console.print(f"   ‚Ä¢ [cyan]{spider_name}[/cyan]: [yellow]{posts_count}[/yellow] post√≥w")
                
                # Statystyki domen
                try:
                    cursor.execute("""
                        SELECT category, COUNT(*) as count, SUM(total_references) as total_refs
                        FROM domains 
                        GROUP BY category 
                        ORDER BY total_refs DESC
                    """)
                    domain_results = cursor.fetchall()
                    
                    if domain_results:
                        console.print("\nüåê [bold]Statystyki domen:[/bold]")
                        for category, count, total_refs in domain_results:
                            console.print(f"   ‚Ä¢ [cyan]{category}[/cyan]: [yellow]{count}[/yellow] domen, [green]{total_refs}[/green] odniesie≈Ñ")
                
                except Exception:
                    pass  # Tabele domen mogƒÖ nie istnieƒá
                
        except Exception:
            pass  # Ignoruj b≈Çƒôdy SQL
    else:
        console.print(f"   üìÅ [yellow]{unified_db_path}[/yellow] (zostanie utworzona)")


@app.command(name="list-spiders")
def list_available_spiders():
    """üìã Wy≈õwietl dostƒôpne spidery."""
    console.print("üï∑Ô∏è  [bold]Dostƒôpne spidery:[/bold]")
    
    spiders = get_available_spiders()
    for spider in spiders:
        console.print(f"   ‚Ä¢ [cyan]{spider}[/cyan]")


@app.command(name="list-analyzers") 
def list_available_analyzers():
    """üî¨ Wy≈õwietl dostƒôpne analizatory."""
    from importlib.metadata import entry_points
    
    console.print("üî¨ [bold]Dostƒôpne analizatory:[/bold]")
    
    try:
        eps = entry_points(group="forums_scraper.analyzers")
        for ep in sorted(eps, key=lambda x: x.name):
            console.print(f"   ‚Ä¢ [cyan]{ep.name}[/cyan] - [dim]{ep.value}[/dim]")
    except Exception as e:
        console.print(f"[red]B≈ÇƒÖd podczas pobierania analizator√≥w: {e}[/red]")


@app.command(name="config")
def create_config_file(
    output: Annotated[Path, typer.Option(
        "--output", "-o",
        help="≈öcie≈ºka do pliku konfiguracyjnego"
    )] = Path("scraper_config.yaml"),
    
    analysis: Annotated[List[AnalysisType], typer.Option(
        "--analysis", "-a",
        help="Rodzaje analiz do w≈ÇƒÖczenia"
    )] = [AnalysisType.BASIC_TOKENS],
    
    spacy_model: Annotated[str, typer.Option(
        "--spacy-model",
        help="Model spaCy"
    )] = "pl_core_news_lg",
    
    include_sentiment: Annotated[bool, typer.Option(
        "--sentiment/--no-sentiment",
        help="W≈ÇƒÖcz analizƒô sentymentu"
    )] = False
):
    """üìù Utw√≥rz plik konfiguracyjny dla analiz."""
    
    config = create_analysis_config(analysis, spacy_model, include_sentiment)
    save_config_file(config, output)
    
    console.print(f"üìù [green]Utworzono plik konfiguracyjny:[/green] [cyan]{output}[/cyan]")
    
    # Wy≈õwietl zawarto≈õƒá
    console.print("\nüìã [bold]Zawarto≈õƒá pliku:[/bold]")
    with open(output, 'r', encoding='utf-8') as f:
        content = f.read()
        console.print(f"[dim]{content}[/dim]")


@app.command(name="analyze")
def analyze_offline(
    config: Annotated[Path, typer.Option(
        "--config", "-c",
        exists=True, readable=True,
        help="≈öcie≈ºka do pliku konfiguracyjnego"
    )],
    
    input_file: Annotated[Path, typer.Option(
        "--input", "-i",
        exists=True, readable=True,
        help="Plik wej≈õciowy JSONL"
    )],
    
    output_file: Annotated[Optional[Path], typer.Option(
        "--output", "-o",
        help="Plik wyj≈õciowy JSONL (domy≈õlnie: input.analyzed.jsonl)"
    )] = None
):
    """üî¨ Uruchom analizy offline na pliku JSONL."""
    
    if not output_file:
        output_file = input_file.parent / f"{input_file.stem}.analyzed.jsonl"
    
    console.print(f"üî¨ [bold]Analiza offline[/bold]")
    console.print(f"üìÅ Wej≈õcie: [cyan]{input_file}[/cyan]")
    console.print(f"üìÅ Wyj≈õcie: [cyan]{output_file}[/cyan]")
    console.print(f"‚öôÔ∏è  Konfiguracja: [cyan]{config}[/cyan]")
    
    # Import i uruchomienie analizy (u≈ºywamy istniejƒÖcego kodu)
    from .main import analyze as run_analysis
    
    try:
        run_analysis(config, input_file, output_file)
        console.print(f"‚úÖ [green]Analiza zako≈Ñczona pomy≈õlnie![/green]")
    except Exception as e:
        console.print(f"‚ùå [red]B≈ÇƒÖd podczas analizy: {e}[/red]")
        raise typer.Exit(1)


@app.command(name="status")
def show_status(
    database_path: Annotated[Path, typer.Option(
        "--database", "-d",
        help="≈öcie≈ºka do bazy danych"
    )] = Path("data/databases/forums_unified.db")
):
    """üìä Poka≈º status wsp√≥lnej bazy danych i statystyki."""
    
    console.print("üìä [bold]Status wsp√≥lnej bazy danych[/bold]")
    
    if not database_path.exists():
        console.print(f"[yellow]Baza danych nie istnieje: {database_path}[/yellow]")
        console.print("üí° Uruchom scrapowanie: [cyan]uv run cli scrape[/cyan]")
        return
    
    # Podstawowe informacje o pliku
    size_mb = database_path.stat().st_size / 1024 / 1024
    mtime = datetime.fromtimestamp(database_path.stat().st_mtime)
    
    console.print(f"üìÅ [cyan]{database_path.name}[/cyan]")
    console.print(f"üìè Rozmiar: [yellow]{size_mb:.1f} MB[/yellow]")
    console.print(f"üïí Ostatnia modyfikacja: [green]{mtime.strftime('%Y-%m-%d %H:%M:%S')}[/green]")
    
    # Statystyki z bazy danych
    try:
        import sqlite3
        with sqlite3.connect(database_path) as conn:
            cursor = conn.cursor()
            
            # Statystyki g≈Ç√≥wne
            cursor.execute("SELECT COUNT(*) FROM forums")
            forums_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM posts")
            posts_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM users")
            users_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM threads")
            threads_count = cursor.fetchone()[0]
            
            # Tabela g≈Ç√≥wnych statystyk
            main_table = Table(title="üìà G≈Ç√≥wne statystyki", show_header=True)
            main_table.add_column("Kategoria", style="cyan")
            main_table.add_column("Liczba", style="yellow")
            
            main_table.add_row("Fora", str(forums_count))
            main_table.add_row("Posty", str(posts_count))
            main_table.add_row("U≈ºytkownicy", str(users_count))
            main_table.add_row("WƒÖtki", str(threads_count))
            
            console.print(main_table)
            
            # Statystyki per forum
            cursor.execute("""
                SELECT f.spider_name, f.title, 
                       COUNT(DISTINCT p.id) as posts_count,
                       COUNT(DISTINCT u.id) as users_count,
                       COUNT(DISTINCT t.id) as threads_count
                FROM forums f
                LEFT JOIN sections s ON f.id = s.forum_id
                LEFT JOIN threads t ON s.id = t.section_id
                LEFT JOIN posts p ON t.id = p.thread_id
                LEFT JOIN users u ON p.user_id = u.id
                GROUP BY f.id, f.spider_name, f.title
                ORDER BY posts_count DESC
            """)
            forum_results = cursor.fetchall()
            
            if forum_results:
                forum_table = Table(title="üìä Statystyki per forum", show_header=True)
                forum_table.add_column("Forum", style="cyan")
                forum_table.add_column("Posty", style="yellow")
                forum_table.add_column("U≈ºytkownicy", style="green")
                forum_table.add_column("WƒÖtki", style="magenta")
                
                for spider_name, title, posts, users, threads in forum_results:
                    forum_table.add_row(
                        spider_name or "Nieznane",
                        str(posts),
                        str(users),
                        str(threads)
                    )
                
                console.print(forum_table)
            
            # Statystyki analiz (je≈õli sƒÖ)
            try:
                cursor.execute("SELECT COUNT(*) FROM post_token_stats")
                token_stats_count = cursor.fetchone()[0]
                
                cursor.execute("SELECT COUNT(*) FROM post_linguistic_stats")
                linguistic_stats_count = cursor.fetchone()[0]
                
                if token_stats_count > 0 or linguistic_stats_count > 0:
                    analysis_table = Table(title="üî¨ Statystyki analiz", show_header=True)
                    analysis_table.add_column("Typ analizy", style="cyan")
                    analysis_table.add_column("Przeanalizowane posty", style="yellow")
                    
                    if token_stats_count > 0:
                        analysis_table.add_row("Tokenizacja", str(token_stats_count))
                    
                    if linguistic_stats_count > 0:
                        analysis_table.add_row("Analiza jƒôzykowa", str(linguistic_stats_count))
                    
                    # Sprawd≈∫ NER
                    try:
                        cursor.execute("SELECT COUNT(*) FROM post_ner_stats")
                        ner_stats_count = cursor.fetchone()[0]
                        
                        if ner_stats_count > 0:
                            analysis_table.add_row("Named Entities", str(ner_stats_count))
                    except Exception:
                        pass
                    
                    console.print(analysis_table)
                    
            except Exception:
                pass  # Tabele analiz mogƒÖ nie istnieƒá
            
    except Exception as e:
        console.print(f"[red]B≈ÇƒÖd podczas odczytu bazy danych: {e}[/red]")


def run():
    """Entry point for the CLI."""
    app()


if __name__ == "__main__":
    run()
