# üï∑Ô∏è Forums Scraper

**Zaawansowany scraper for√≥w religijnych z r√≥wnoleg≈Çymi analizami NLP i bazami danych SQLite**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Scrapy](https://img.shields.io/badge/scrapy-2.11+-green.svg)](https://scrapy.org/)
[![spaCy](https://img.shields.io/badge/spaCy-3.4+-orange.svg)](https://spacy.io/)
[![Rich CLI](https://img.shields.io/badge/CLI-Rich%20%2B%20Typer-purple.svg)](https://rich.readthedocs.io/)

## üéØ Opis projektu

Forums Scraper to profesjonalne narzƒôdzie do scrapowania for√≥w religijnych z zaawansowanymi funkcjami analizy tekstu. System zosta≈Ç zaprojektowany z my≈õlƒÖ o badaniach lingwistycznych, analizie sentymentu i modelowaniu temat√≥w w kontek≈õcie dyskusji religijnych.

### üéØ G≈Ç√≥wne cele

- **Badania naukowe**: Analiza dyskursu religijnego w internecie
- **Analiza sentymentu**: Badanie nastroj√≥w w spo≈Çeczno≈õciach religijnych
- **Modelowanie temat√≥w**: Identyfikacja g≈Ç√≥wnych temat√≥w dyskusji
- **Archiwizacja**: D≈Çugoterminowe przechowywanie tre≈õci for√≥w

## ‚ú® Kluczowe funkcjonalno≈õci

### üîÑ **R√≥wnoleg≈Çe analizy NLP**

- **Tokenizacja podstawowa** - szybka analiza bez zale≈ºno≈õci zewnƒôtrznych
- **Liczenie token√≥w OpenAI** - estymacja koszt√≥w API dla modeli jƒôzykowych
- **Pe≈Çna analiza spaCy** - lematyzacja, POS tagging, dependency parsing
- **Analiza sentymentu** - wykrywanie emocji w tek≈õcie
- **Statystyki jƒôzykowe** - czytelno≈õƒá, d≈Çugo≈õƒá zda≈Ñ, bogactwo s≈Çownictwa

### üóÑÔ∏è **Zaawansowana baza danych**

- **Osobne bazy SQLite** dla ka≈ºdego forum
- **Pe≈Çny schemat relacyjny** - fora, sekcje, wƒÖtki, u≈ºytkownicy, posty
- **Tabele analiz** - tokeny, statystyki jƒôzykowe, analiza morfosyntaktyczna
- **Indeksy wydajno≈õciowe** - szybkie zapytania analityczne
- **Automatyczne backupy** - bezpieczne przechowywanie danych

### üéõÔ∏è **Profesjonalny CLI**

- **Rich interface** - kolorowy, interaktywny interfejs
- **Progress tracking** - ≈õledzenie postƒôpu w czasie rzeczywistym
- **Wyb√≥r for√≥w** - elastyczna selekcja ≈∫r√≥de≈Ç danych
- **Konfiguracja analiz** - dostosowanie do potrzeb badawczych
- **Dry-run mode** - testowanie bez wykonywania operacji
- **Status monitoring** - przeglƒÖd stanu baz danych

### ‚öôÔ∏è **Elastyczna architektura**

- **Plugin system** - ≈Çatwe dodawanie nowych analizator√≥w
- **Entry points** - automatyczne wykrywanie dostƒôpnych analiz
- **YAML/CLI configuration** - wygodna konfiguracja
- **Asynchronous processing** - wydajne przetwarzanie r√≥wnoleg≈Çe
- **Error handling** - odporna na b≈Çƒôdy architektura

## üöÄ Instalacja i pierwsze uruchomienie

### Wymagania systemowe

- **Python 3.10+** (sprawd≈∫: `python --version`)
- **4GB RAM** (minimum), **8GB RAM** (zalecane dla spaCy)
- **Po≈ÇƒÖczenie internetowe** dla scrapowania
- **~500MB** miejsca na dysku (zale≈ºnie od liczby for√≥w)

### Krok 1: Pobranie kodu

```bash
# Klonowanie repozytorium
git clone https://github.com/username/forums_scraper.git
cd forums_scraper
```

### Krok 2: Instalacja

#### **Opcja A: Instalacja podstawowa** (tylko scrapowanie)

```bash
pip install -e .
```

#### **Opcja B: Z analizatorami podstawowymi** (+ tiktoken)

```bash
pip install -e ".[analyzers-basic]"
```

#### **Opcja C: Z pe≈Çnymi analizatorami** (+ spaCy)

```bash
pip install -e ".[analyzers-linguistic]"
python -m spacy download pl_core_news_sm
```

#### **Opcja D: Pe≈Çna instalacja** (zalecane)

```bash
pip install -e ".[all]"
python -m spacy download pl_core_news_sm
```

### Krok 3: Weryfikacja

```bash
# Sprawd≈∫ czy CLI dzia≈Ça
fs-cli --help

# Lista dostƒôpnych for√≥w
fs-cli list-spiders

# Lista dostƒôpnych analizator√≥w
fs-cli list-analyzers

# Test bez scrapowania
fs-cli scrape --forum radio_katolik --dry-run
```

### Krok 4: Pierwsze uruchomienie

```bash
# Scrapuj jedno forum z podstawowƒÖ analizƒÖ
fs-cli scrape --forum radio_katolik --analysis basic_tokens

# Sprawd≈∫ wyniki
fs-cli status
```

### RozwiƒÖzywanie problem√≥w instalacji

#### **B≈ÇƒÖd "fs-cli: command not found"**

```bash
# Znajd≈∫ ≈õcie≈ºkƒô do skryptu
find ~/.pyenv -name "fs-cli" 2>/dev/null

# U≈ºyj pe≈Çnej ≈õcie≈ºki (przyk≈Çad)
~/.pyenv/versions/3.11.9/bin/fs-cli --help
```

#### **B≈ÇƒÖd "spaCy model not found"**

```bash
python -m spacy download pl_core_news_sm

# Lub wiƒôkszy model (lepszy, ale wolniejszy)
python -m spacy download pl_core_news_lg
```

#### **B≈ÇƒÖd "tiktoken not found"**

```bash
pip install tiktoken
```

#### **Problemy z pamiƒôciƒÖ**

```bash
# Zmniejsz batch size dla analiz
fs-cli scrape --batch-size 25
```

## üéÆ Przewodnik u≈ºytkownika

### Podstawowe u≈ºycie

#### 1. **Scrapowanie wszystkich for√≥w** (zalecane dla poczƒÖtkujƒÖcych)

```bash
fs-cli scrape
```

- Scrapuje wszystkie 4 fora
- U≈ºywa podstawowej tokenizacji
- Zapisuje do `data/databases/forum_*.db`

#### 2. **Scrapowanie konkretnego forum**

```bash
fs-cli scrape --forum radio_katolik
```

#### 3. **Scrapowanie z analizƒÖ spaCy**

```bash
fs-cli scrape --forum wiara --analysis spacy_full --sentiment
```

### Zaawansowane opcje

#### **Wyb√≥r wielu for√≥w i analiz**

```bash
fs-cli scrape \
  --forum wiara \
  --forum dolina_modlitwy \
  --analysis basic_tokens \
  --analysis spacy_full \
  --sentiment
```

#### **Optymalizacja wydajno≈õci**

```bash
fs-cli scrape \
  --concurrent 32 \
  --delay 0.1 \
  --batch-size 200 \
  --forum radio_katolik
```

#### **Tryb testowy (bez scrapowania)**

```bash
fs-cli scrape --forum wiara --analysis all --dry-run
```

### ZarzƒÖdzanie danymi

#### **Status baz danych**

```bash
fs-cli status
```

Wy≈õwietla:

- Rozmiary plik√≥w baz danych
- Liczba post√≥w w ka≈ºdej bazie
- Daty ostatniej modyfikacji

#### **Tworzenie konfiguracji**

```bash
# Utw√≥rz plik konfiguracyjny
fs-cli config --analysis spacy_full --sentiment --output my_config.yaml

# U≈ºyj w≈Çasnej konfiguracji
fs-cli scrape --config my_config.yaml
```

## üìã Dostƒôpne fora

| Forum               | Kod               | Opis                                   | Szacowana wielko≈õƒá |
| ------------------- | ----------------- | -------------------------------------- | ------------------ |
| **Dolina Modlitwy** | `dolina_modlitwy` | Forum katolickie, modlitwy i duchowo≈õƒá | ~50MB              |
| **Radio Katolik**   | `radio_katolik`   | Forum Radia Katolik, aktualno≈õci       | ~30MB              |
| **Wiara.pl**        | `wiara`           | Najwiƒôksze polskie forum katolickie    | ~100MB             |
| **Z Chrystusem**    | `z_chrystusem`    | Forum ewangelickie                     | ~40MB              |

### Wyb√≥r for√≥w

```bash
# Pojedyncze forum
--forum radio_katolik

# Wiele for√≥w
--forum wiara --forum dolina_modlitwy

# Wszystkie fora (domy≈õlne)
--forum all
```

## üî¨ Typy analiz NLP

### 1. **Podstawowa tokenizacja** (`basic_tokens`)

```yaml
- name: basic_tokenizer
  config:
    lowercase: true # Konwersja na ma≈Çe litery
    min_token_length: 2 # Minimalna d≈Çugo≈õƒá tokena
    remove_punctuation: false # Zachowanie interpunkcji
```

**Wyniki:**

- Lista token√≥w
- Liczba token√≥w (total/unique)
- ≈örednia d≈Çugo≈õƒá tokena

**Zalety:** Szybka, bez zale≈ºno≈õci zewnƒôtrznych
**Wady:** Podstawowa jako≈õƒá tokenizacji

### 2. **Liczenie token√≥w OpenAI** (`token_count`)

```yaml
- name: token_counter
  config:
    encoding: cl100k_base # Model tokenizacji OpenAI
```

**Wyniki:**

- Dok≈Çadna liczba token√≥w OpenAI
- Estymacja koszt√≥w API

**Zalety:** Precyzyjne dla modeli OpenAI
**Wady:** Wymaga biblioteki tiktoken

### 3. **Pe≈Çna analiza spaCy** (`spacy_full`)

```yaml
- name: spacy_analyzer
  config:
    model: pl_core_news_sm # Model jƒôzyka polskiego
    include_sentiment: true # W≈ÇƒÖcz analizƒô sentymentu
    batch_size: 100 # Rozmiar batcha
    max_length: 1000000 # Maks. d≈Çugo≈õƒá tekstu
```

**Wyniki linguistyczne:**

- **Lematyzacja** - forma podstawowa s≈Ç√≥w
- **POS tagging** - czƒô≈õci mowy (rzeczownik, czasownik, etc.)
- **Dependency parsing** - relacje sk≈Çadniowe
- **Named Entity Recognition** - rozpoznawanie nazw w≈Çasnych
- **Analiza sentymentu** - polarno≈õƒá emocjonalna

**Statystyki tekstowe:**

- Liczba zda≈Ñ, s≈Ç√≥w, znak√≥w
- ≈örednia d≈Çugo≈õƒá zdania
- Wska≈∫nik czytelno≈õci
- Wykrywanie jƒôzyka

**Zalety:** Najwy≈ºsza jako≈õƒá analizy
**Wady:** Wymaga spaCy i modelu jƒôzykowego

### 4. **Wszystkie analizy** (`all`)

W≈ÇƒÖcza wszystkie dostƒôpne analizatory jednocze≈õnie.

## üóÑÔ∏è Struktura bazy danych

### Tabele g≈Ç√≥wne

#### **forums** - Informacje o forach

```sql
CREATE TABLE forums (
    id TEXT PRIMARY KEY,
    spider_name TEXT NOT NULL,
    title TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

#### **sections** - Sekcje for√≥w

```sql
CREATE TABLE sections (
    id TEXT PRIMARY KEY,
    forum_id TEXT,
    title TEXT,
    url TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (forum_id) REFERENCES forums (id)
);
```

#### **threads** - WƒÖtki dyskusji

```sql
CREATE TABLE threads (
    id TEXT PRIMARY KEY,
    section_id TEXT,
    title TEXT,
    url TEXT,
    author TEXT,
    replies INTEGER,
    views INTEGER,
    last_post_date TEXT,
    last_post_author TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (section_id) REFERENCES sections (id)
);
```

#### **users** - U≈ºytkownicy for√≥w

```sql
CREATE TABLE users (
    id TEXT PRIMARY KEY,
    username TEXT UNIQUE,
    join_date TEXT,
    posts_count INTEGER,
    religion TEXT,
    gender TEXT,
    localization TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

#### **posts** - Posty u≈ºytkownik√≥w

```sql
CREATE TABLE posts (
    id TEXT PRIMARY KEY,
    thread_id TEXT,
    user_id TEXT,
    post_number INTEGER,
    content TEXT,
    content_urls TEXT,  -- JSON z URL-ami
    post_date TEXT,
    url TEXT,
    username TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (thread_id) REFERENCES threads (id),
    FOREIGN KEY (user_id) REFERENCES users (id)
);
```

### Tabele analiz NLP

#### **post_tokens** - Tokeny post√≥w

```sql
CREATE TABLE post_tokens (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,
    token TEXT,
    position INTEGER,
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_token_stats** - Statystyki token√≥w

```sql
CREATE TABLE post_token_stats (
    post_id TEXT PRIMARY KEY,
    total_tokens INTEGER,
    unique_tokens INTEGER,
    avg_token_length REAL,
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_linguistic_analysis** - Analiza morfosyntaktyczna

```sql
CREATE TABLE post_linguistic_analysis (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,
    token TEXT,
    lemma TEXT,           -- Forma podstawowa
    pos TEXT,             -- Czƒô≈õƒá mowy
    tag TEXT,             -- Szczeg√≥≈Çowy tag
    dep TEXT,             -- Relacja sk≈Çadniowa
    is_alpha BOOLEAN,     -- Czy alfanumeryczny
    is_stop BOOLEAN,      -- Czy stop word
    is_punct BOOLEAN,     -- Czy interpunkcja
    sentiment_score REAL, -- Wynik sentymentu
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_linguistic_stats** - Statystyki jƒôzykowe

```sql
CREATE TABLE post_linguistic_stats (
    post_id TEXT PRIMARY KEY,
    sentence_count INTEGER,      -- Liczba zda≈Ñ
    word_count INTEGER,          -- Liczba s≈Ç√≥w
    char_count INTEGER,          -- Liczba znak√≥w
    avg_sentence_length REAL,    -- ≈örednia d≈Çugo≈õƒá zdania
    readability_score REAL,      -- Wska≈∫nik czytelno≈õci
    sentiment_polarity REAL,     -- Polarno≈õƒá sentymentu (-1 do 1)
    sentiment_subjectivity REAL, -- Subiektywno≈õƒá (0 do 1)
    language_detected TEXT,      -- Wykryty jƒôzyk
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

## üìä Przyk≈Çady analiz SQL

### Podstawowe statystyki

#### **PrzeglƒÖd for√≥w**

```sql
-- Liczba post√≥w na forum
SELECT
    f.title as forum,
    COUNT(p.id) as posts_count,
    COUNT(DISTINCT p.user_id) as users_count,
    MIN(p.created_at) as first_post,
    MAX(p.created_at) as last_post
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
GROUP BY f.id, f.title;
```

#### **Najaktywniejsze sekcje**

```sql
SELECT
    s.title as section,
    COUNT(p.id) as posts_count,
    COUNT(DISTINCT t.id) as threads_count
FROM sections s
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
GROUP BY s.id, s.title
ORDER BY posts_count DESC
LIMIT 10;
```

#### **Top u≈ºytkownicy**

```sql
SELECT
    u.username,
    COUNT(p.id) as posts_count,
    u.join_date,
    u.religion,
    u.gender
FROM users u
JOIN posts p ON u.id = p.user_id
GROUP BY u.id, u.username
ORDER BY posts_count DESC
LIMIT 20;
```

### Analizy jƒôzykowe

#### **Najczƒô≈õciej u≈ºywane s≈Çowa**

```sql
-- Top lematy (formy podstawowe s≈Ç√≥w)
SELECT
    lemma,
    COUNT(*) as frequency,
    COUNT(DISTINCT post_id) as posts_with_word
FROM post_linguistic_analysis
WHERE is_alpha = 1
  AND is_stop = 0
  AND LENGTH(lemma) > 3
GROUP BY lemma
ORDER BY frequency DESC
LIMIT 50;
```

#### **Analiza czƒô≈õci mowy**

```sql
-- Rozk≈Çad czƒô≈õci mowy
SELECT
    pos,
    COUNT(*) as count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage
FROM post_linguistic_analysis
WHERE is_alpha = 1
GROUP BY pos
ORDER BY count DESC;
```

#### **Analiza sentymentu**

```sql
-- Posty z najwy≈ºszym sentymentem pozytywnym
SELECT
    p.content,
    pls.sentiment_polarity,
    pls.sentiment_subjectivity,
    u.username,
    p.post_date
FROM posts p
JOIN post_linguistic_stats pls ON p.id = pls.post_id
JOIN users u ON p.user_id = u.id
WHERE pls.sentiment_polarity > 0.5
ORDER BY pls.sentiment_polarity DESC
LIMIT 10;
```

#### **Statystyki czytelno≈õci**

```sql
-- ≈örednie wska≈∫niki czytelno≈õci po sekcjach
SELECT
    s.title as section,
    COUNT(pls.post_id) as analyzed_posts,
    ROUND(AVG(pls.readability_score), 2) as avg_readability,
    ROUND(AVG(pls.avg_sentence_length), 2) as avg_sentence_len,
    ROUND(AVG(pls.word_count), 0) as avg_words
FROM sections s
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_stats pls ON p.id = pls.post_id
GROUP BY s.id, s.title
HAVING analyzed_posts > 100
ORDER BY avg_readability DESC;
```

#### **Analiza tematyczna przez s≈Çowa kluczowe**

```sql
-- Posty zawierajƒÖce s≈Çowa religijne
SELECT DISTINCT
    p.id,
    p.content,
    u.username,
    s.title as section
FROM posts p
JOIN users u ON p.user_id = u.id
JOIN threads t ON p.thread_id = t.id
JOIN sections s ON t.section_id = s.id
JOIN post_linguistic_analysis pla ON p.id = pla.post_id
WHERE pla.lemma IN ('b√≥g', 'jezus', 'chrystus', 'modlitwa', 'wiara', 'ko≈õci√≥≈Ç')
  AND pla.is_alpha = 1
LIMIT 20;
```

### Analizy temporalne

#### **Aktywno≈õƒá w czasie**

```sql
-- Liczba post√≥w po miesiƒÖcach
SELECT
    strftime('%Y-%m', created_at) as month,
    COUNT(*) as posts_count
FROM posts
WHERE created_at IS NOT NULL
GROUP BY month
ORDER BY month;
```

#### **Ewolucja sentymentu**

```sql
-- ≈öredni sentyment po miesiƒÖcach
SELECT
    strftime('%Y-%m', p.created_at) as month,
    COUNT(pls.post_id) as posts_analyzed,
    ROUND(AVG(pls.sentiment_polarity), 3) as avg_sentiment,
    ROUND(AVG(pls.readability_score), 1) as avg_readability
FROM posts p
JOIN post_linguistic_stats pls ON p.id = pls.post_id
WHERE p.created_at IS NOT NULL
GROUP BY month
HAVING posts_analyzed > 10
ORDER BY month;
```

## ‚öôÔ∏è Konfiguracja zaawansowana

### Plik konfiguracyjny YAML

#### **Podstawowa konfiguracja**

```yaml
# config.yaml
analysis:
  enabled: true
  analyzers:
    - name: basic_tokenizer
      config:
        lowercase: true
        min_token_length: 2

    - name: token_counter
      config:
        encoding: cl100k_base

  concurrency: 4

scrapy:
  concurrent_requests: 16
  download_delay: 0.5
  autothrottle: true
```

#### **Konfiguracja produkcyjna**

```yaml
# production_config.yaml
analysis:
  enabled: true
  analyzers:
    - name: basic_tokenizer
      config:
        lowercase: true
        min_token_length: 2
        remove_punctuation: false

    - name: token_counter
      config:
        encoding: cl100k_base

    - name: spacy_analyzer
      config:
        model: pl_core_news_sm
        include_sentiment: true
        batch_size: 200
        max_length: 1000000

  concurrency: 8

scrapy:
  concurrent_requests: 32
  concurrent_requests_per_domain: 16
  download_delay: 0.2
  randomize_download_delay: true
  autothrottle: true
  autothrottle_start_delay: 0.1
  autothrottle_max_delay: 2.0
  autothrottle_target_concurrency: 24.0
```

### Optymalizacja wydajno≈õci

#### **Dla szybkiego scrapowania**

```bash
fs-cli scrape \
  --concurrent 64 \
  --delay 0.1 \
  --analysis basic_tokens \
  --forum radio_katolik
```

#### **Dla dok≈Çadnej analizy**

```bash
fs-cli scrape \
  --concurrent 8 \
  --delay 1.0 \
  --batch-size 50 \
  --analysis all \
  --sentiment \
  --spacy-model pl_core_news_lg
```

#### **Dla ograniczonych zasob√≥w**

```bash
fs-cli scrape \
  --concurrent 4 \
  --delay 2.0 \
  --batch-size 25 \
  --analysis basic_tokens \
  --forum dolina_modlitwy
```

## üîß RozwiƒÖzywanie problem√≥w

### Czƒôste problemy

#### **1. B≈ÇƒÖd "spaCy model not found"**

```bash
# RozwiƒÖzanie
python -m spacy download pl_core_news_sm

# Lub wiƒôkszy model (lepszy, ale wolniejszy)
python -m spacy download pl_core_news_lg
```

#### **2. B≈ÇƒÖd "tiktoken not found"**

```bash
# RozwiƒÖzanie
pip install tiktoken
```

#### **3. Problemy z pamiƒôciƒÖ podczas analizy spaCy**

```bash
# Zmniejsz batch size
fs-cli scrape --batch-size 25 --analysis spacy_full

# Lub u≈ºyj mniejszego modelu
fs-cli scrape --spacy-model pl_core_news_sm
```

#### **4. Zbyt wolne scrapowanie**

```bash
# Zwiƒôksz r√≥wnoleg≈Ço≈õƒá (ostro≈ºnie!)
fs-cli scrape --concurrent 32 --delay 0.2

# Wy≈ÇƒÖcz analizy dla szybszego scrapowania
fs-cli scrape --analysis none
```

#### **5. B≈Çƒôdy po≈ÇƒÖczenia sieciowego**

```bash
# Zwiƒôksz op√≥≈∫nienia
fs-cli scrape --delay 2.0 --concurrent 8

# Sprawd≈∫ po≈ÇƒÖczenie internetowe
ping google.com
```

### Debugowanie

#### **W≈ÇƒÖcz szczeg√≥≈Çowe logi**

```bash
fs-cli scrape --verbose --forum radio_katolik
```

#### **Testuj bez scrapowania**

```bash
fs-cli scrape --dry-run --analysis all
```

#### **Sprawd≈∫ status baz danych**

```bash
fs-cli status
```

### Monitorowanie wydajno≈õci

#### **Sprawd≈∫ wykorzystanie zasob√≥w**

```bash
# Podczas scrapowania w drugim terminalu
top -p $(pgrep -f fs-cli)
```

#### **Monitoruj rozmiar baz danych**

```bash
watch -n 5 'ls -lh data/databases/*.db'
```

## üèóÔ∏è Architektura systemu

### Struktura projektu

```
forums_scraper/
‚îú‚îÄ‚îÄ üìÑ README.md                    # Kompletna dokumentacja (ten plik)
‚îú‚îÄ‚îÄ ‚öôÔ∏è  pyproject.toml               # Konfiguracja pakietu Python
‚îú‚îÄ‚îÄ üìÇ examples/                    # Przyk≈Çady konfiguracji
‚îÇ   ‚îî‚îÄ‚îÄ forums_scraper.yaml
‚îú‚îÄ‚îÄ üìÇ data/                        # Bazy danych i wyniki
‚îÇ   ‚îî‚îÄ‚îÄ databases/                  # SQLite bazy danych
‚îú‚îÄ‚îÄ üìÇ forums_scraper/              # G≈Ç√≥wny pakiet Python
‚îÇ   ‚îú‚îÄ‚îÄ analyzers_basic/            # üî¨ Analizatory NLP
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ linguistic.py           #   ‚îú‚îÄ‚îÄ SpacyAnalyzer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenizer.py            #   ‚îî‚îÄ‚îÄ TokenCountAnalyzer
‚îÇ   ‚îú‚îÄ‚îÄ fs_cli/                     # üéõÔ∏è Interfejs CLI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced.py             #   ‚îî‚îÄ‚îÄ Rich + Typer UI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py                 #
‚îÇ   ‚îú‚îÄ‚îÄ fs_core/                    # ‚öôÔ∏è Rdze≈Ñ systemu
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py               #   ‚îú‚îÄ‚îÄ Konfiguracja YAML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ protocol.py             #   ‚îú‚îÄ‚îÄ Interfejsy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ registry.py             #   ‚îú‚îÄ‚îÄ Entry points
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ runner.py               #   ‚îî‚îÄ‚îÄ Async runner
‚îÇ   ‚îî‚îÄ‚îÄ scraper/                    # üï∑Ô∏è Silnik Scrapy
‚îÇ       ‚îú‚îÄ‚îÄ items.py                #   ‚îú‚îÄ‚îÄ Modele danych
‚îÇ       ‚îú‚îÄ‚îÄ middlewares.py          #   ‚îú‚îÄ‚îÄ Middleware
‚îÇ       ‚îú‚îÄ‚îÄ pipelines/              #   ‚îú‚îÄ‚îÄ Pipeline'y
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ analysis.py         #   ‚îÇ   ‚îú‚îÄ‚îÄ Analiza
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ database.py         #   ‚îÇ   ‚îî‚îÄ‚îÄ Baza danych
‚îÇ       ‚îú‚îÄ‚îÄ settings.py             #   ‚îú‚îÄ‚îÄ Ustawienia
‚îÇ       ‚îú‚îÄ‚îÄ spiders/                #   ‚îî‚îÄ‚îÄ Spidery for√≥w
‚îÇ       ‚îî‚îÄ‚îÄ utils.py
```

### Przep≈Çyw danych

```mermaid
graph TD
    A[fs-cli scrape] --> B[Scrapy Engine]
    B --> C[Spider]
    C --> D[Web Scraping]
    D --> E[Items]
    E --> F[Analysis Pipeline]
    F --> G[NLP Analyzers]
    G --> H[Database Pipeline]
    H --> I[SQLite Database]

    J[Config YAML] --> F
    K[Entry Points] --> G
```

### Wzorce projektowe

- **Plugin Architecture** - Entry points dla analizator√≥w
- **Pipeline Pattern** - Scrapy pipelines dla przetwarzania
- **Strategy Pattern** - R√≥≈ºne typy analiz
- **Observer Pattern** - Progress reporting
- **Factory Pattern** - Tworzenie analizator√≥w

## üöÄ Rozw√≥j i wk≈Çad

### Dodawanie nowych analizator√≥w

#### **1. Implementacja analizatora**

```python
# my_analyzer.py
from typing import Any, Dict
from forums_scraper.fs_core.protocol import Analyzer

class MyCustomAnalyzer(Analyzer):
    def __init__(self, **config):
        self.config = config

    async def setup(self):
        # Inicjalizacja (≈Çadowanie modeli, etc.)
        pass

    async def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        content = data.get('content', '')

        # Twoja analiza tutaj
        result = self.my_analysis_function(content)

        return {
            'my_analysis': result
        }

    async def close(self):
        # SprzƒÖtanie zasob√≥w
        pass
```

#### **2. Rejestracja w pyproject.toml**

```toml
[project.entry-points."forums_scraper.analyzers"]
my_analyzer = "my_package.my_analyzer:MyCustomAnalyzer"
```

#### **3. U≈ºycie w konfiguracji**

```yaml
analysis:
  enabled: true
  analyzers:
    - name: my_analyzer
      config:
        parameter1: value1
        parameter2: value2
```

### Dodawanie nowych for√≥w

#### **1. Implementacja spidera**

```python
# new_forum_spider.py
import scrapy
from forums_scraper.scraper.items import ForumPostItem

class NewForumSpider(scrapy.Spider):
    name = 'new_forum'
    allowed_domains = ['newforum.com']
    start_urls = ['https://newforum.com']

    def parse(self, response):
        # Implementacja scrapowania
        pass
```

#### **2. Aktualizacja CLI**

```python
# W fs_cli/advanced.py
class ForumName(str, Enum):
    # ... istniejƒÖce fora
    NEW_FORUM = "new_forum"

FORUM_SPIDER_MAP = {
    # ... istniejƒÖce mapowania
    ForumName.NEW_FORUM: "new_forum",
}
```

### Testowanie

```bash
# Testy jednostkowe
python -m pytest tests/

# Testy integracyjne
python -m pytest tests/integration/

# Testy analizator√≥w
python -m pytest tests/analyzers/
```

### Code style

```bash
# Formatowanie kodu
black forums_scraper/
isort forums_scraper/

# Linting
flake8 forums_scraper/
mypy forums_scraper/
```

## üìö Zasoby dodatkowe

### Dokumentacja techniczna

- **README.md** (ten plik) - Kompletna dokumentacja
- [API Documentation](docs/api/) - Dokumentacja API (w przygotowaniu)
- [Database Schema](docs/database.md) - Schemat bazy danych (w przygotowaniu)

### Przyk≈Çady u≈ºycia

- [Jupyter Notebooks](examples/notebooks/) - Analizy przyk≈Çadowe
- [SQL Queries](examples/sql/) - Gotowe zapytania
- [Configuration Files](examples/configs/) - Przyk≈Çadowe konfiguracje

### Spo≈Çeczno≈õƒá

- **Issues** - [GitHub Issues](https://github.com/username/forums_scraper/issues)
- **Discussions** - [GitHub Discussions](https://github.com/username/forums_scraper/discussions)
- **Wiki** - [Project Wiki](https://github.com/username/forums_scraper/wiki)

## üìÑ Licencja

MIT License - zobacz [LICENSE](LICENSE) dla szczeg√≥≈Ç√≥w.

## üôè Podziƒôkowania

- **Scrapy Team** - za doskona≈Çy framework scrapowania
- **spaCy Team** - za zaawansowane narzƒôdzia NLP
- **Rich Team** - za piƒôkny interfejs CLI
- **Spo≈Çeczno≈õƒá Open Source** - za inspiracjƒô i wsparcie

---

**Autor:** alb  
**Wersja:** 0.1.0  
**Python:** 3.10+  
**Licencja:** MIT

_Forums Scraper - Profesjonalne narzƒôdzie do analizy dyskursu religijnego w internecie_ üï∑Ô∏è‚ú®
