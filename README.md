# üï∑Ô∏è Forums & Weeklies Scraper

**Zaawansowany scraper for√≥w religijnych i tygodnik√≥w katolickich z r√≥wnoleg≈Çymi analizami NLP i bazami danych SQLite**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![uv](https://img.shields.io/badge/uv-enabled-red.svg)](https://docs.astral.sh/uv/)
[![Scrapy](https://img.shields.io/badge/scrapy-2.11+-green.svg)](https://scrapy.org/)
[![spaCy](https://img.shields.io/badge/spaCy-3.4+-orange.svg)](https://spacy.io/)
[![Rich CLI](https://img.shields.io/badge/CLI-Rich%20%2B%20Typer-purple.svg)](https://rich.readthedocs.io/)

## üéØ Opis projektu

Forums & Weeklies Scraper to profesjonalne narzƒôdzie do scrapowania for√≥w religijnych i tygodnik√≥w katolickich z zaawansowanymi funkcjami analizy tekstu. System zosta≈Ç zaprojektowany z my≈õlƒÖ o badaniach lingwistycznych, analizie sentymentu i modelowaniu temat√≥w w kontek≈õcie dyskursu religijnego w internecie i mediach.

### üéØ G≈Ç√≥wne cele

- **Badania naukowe**: Analiza dyskursu religijnego w internecie i mediach
- **Analiza sentymentu**: Badanie nastroj√≥w w spo≈Çeczno≈õciach religijnych
- **Modelowanie temat√≥w**: Identyfikacja g≈Ç√≥wnych temat√≥w dyskusji i publikacji
- **Archiwizacja**: D≈Çugoterminowe przechowywanie tre≈õci for√≥w i tygodnik√≥w
- **Analiza por√≥wnawcza**: Por√≥wnanie dyskursu miƒôdzy forami a tygodnikami

## ‚ú® Kluczowe funkcjonalno≈õci

### üîÑ **R√≥wnoleg≈Çe analizy NLP**

- **Tokenizacja podstawowa** - szybka analiza bez zale≈ºno≈õci zewnƒôtrznych
- **Liczenie token√≥w OpenAI** - estymacja koszt√≥w API dla modeli jƒôzykowych
- **Pe≈Çna analiza spaCy** - lematyzacja, POS tagging, dependency parsing
- **Analiza sentymentu** - wykrywanie emocji w tek≈õcie
- **Statystyki jƒôzykowe** - czytelno≈õƒá, d≈Çugo≈õƒá zda≈Ñ, bogactwo s≈Çownictwa

### üóÑÔ∏è **Zaawansowana baza danych**

- **Wsp√≥lne bazy SQLite** - `forums_unified.db` i `weeklies_unified.db`
- **Pe≈Çny schemat relacyjny** - fora, sekcje, wƒÖtki, u≈ºytkownicy, posty
- **Tabele analiz** - tokeny, statystyki jƒôzykowe, analiza morfosyntaktyczna, URL-e i domeny
- **Indeksy wydajno≈õciowe** - szybkie zapytania analityczne
- **Automatyczne backupy** - bezpieczne przechowywanie danych

### üéõÔ∏è **Profesjonalny CLI**

- **Rich interface** - kolorowy, interaktywny interfejs
- **Progress tracking** - ≈õledzenie postƒôpu w czasie rzeczywistym
- **Wyb√≥r ≈∫r√≥de≈Ç** - elastyczna selekcja for√≥w i tygodnik√≥w
- **Konfiguracja analiz** - dostosowanie do potrzeb badawczych
- **Dry-run mode** - testowanie bez wykonywania operacji
- **Status monitoring** - przeglƒÖd stanu baz danych

### ‚öôÔ∏è **Elastyczna architektura**

- **Plugin system** - ≈Çatwe dodawanie nowych analizator√≥w
- **Entry points** - automatyczne wykrywanie dostƒôpnych analiz
- **YAML/CLI configuration** - wygodna konfiguracja
- **Asynchronous processing** - wydajne przetwarzanie r√≥wnoleg≈Çe
- **Error handling** - odporna na b≈Çƒôdy architektura
- **Wsp√≥lny rdze≈Ñ** - `core/` u≈ºywany przez wszystkie scrapery

## üöÄ Instalacja i pierwsze uruchomienie

### Wymagania systemowe

- **Python 3.10+** (sprawd≈∫: `python --version`)
- **uv** - nowoczesny mened≈ºer pakiet√≥w Python ([instalacja](https://docs.astral.sh/uv/getting-started/installation/))
- **4GB RAM** (minimum), **8GB RAM** (zalecane dla spaCy)
- **Po≈ÇƒÖczenie internetowe** dla scrapowania
- **~1GB** miejsca na dysku (zale≈ºnie od liczby for√≥w i tygodnik√≥w)

### Krok 1: Instalacja uv (je≈õli nie masz)

```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows (PowerShell)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Sprawd≈∫ instalacjƒô
uv --version
```

### Krok 2: Pobranie kodu

```bash
# Klonowanie repozytorium
git clone https://github.com/username/forums_scraper.git
cd forums_scraper
```

### Krok 3: Instalacja z uv

#### **Opcja A: Instalacja podstawowa** (tylko scrapowanie)

```bash
uv pip install -e .
```

#### **Opcja B: Z analizatorami podstawowymi** (+ tiktoken)

```bash
uv pip install -e ".[analyzers-basic]"
```

#### **Opcja C: Z pe≈Çnymi analizatorami** (+ spaCy)

```bash
uv pip install -e ".[analyzers-linguistic]"
uv run python -m spacy download pl_core_news_lg
```

#### **Opcja D: Pe≈Çna instalacja** (zalecane)

```bash
uv pip install -e ".[all]"
uv run python -m spacy download pl_core_news_lg
```

#### **Opcja E: Z wirtualnym ≈õrodowiskiem** (najbezpieczniejsze)

```bash
# Utw√≥rz i aktywuj venv
uv venv
source .venv/bin/activate  # Linux/macOS
# lub .venv\Scripts\activate  # Windows

# Zainstaluj z pe≈Çnymi funkcjami
uv pip install -e ".[all]"
uv run python -m spacy download pl_core_news_lg
```

### Krok 4: Weryfikacja

```bash
# Sprawd≈∫ czy CLI dzia≈Ça
uv run cli --help

# Lista dostƒôpnych for√≥w
uv run cli list-spiders

# Lista dostƒôpnych analizator√≥w
uv run cli list-analyzers

# Test bez scrapowania
uv run cli scrape --forum radio_katolik --dry-run
```

### Krok 5: Pierwsze uruchomienie

```bash
# Scrapuj jedno forum z podstawowƒÖ analizƒÖ
uv run cli scrape --forum radio_katolik --analysis basic_tokens

# Sprawd≈∫ wyniki
uv run cli status
```

### RozwiƒÖzywanie problem√≥w instalacji

#### **B≈ÇƒÖd "uv: command not found"**

```bash
# Zainstaluj uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc  # lub ~/.zshrc

# Sprawd≈∫ instalacjƒô
uv --version
```

#### **B≈ÇƒÖd "cli: command not found"**

```bash
# U≈ºyj uv run zamiast bezpo≈õredniego wywo≈Çania
uv run cli --help

# Je≈õli u≈ºywasz venv, upewnij siƒô ≈ºe jest aktywowany
source .venv/bin/activate
uv run cli --help
```

#### **B≈ÇƒÖd "spaCy model not found"**

```bash
uv run python -m spacy download pl_core_news_lg

# Lub wiƒôkszy model (lepszy, ale wolniejszy)
uv run python -m spacy download pl_core_news_lg
```

#### **B≈ÇƒÖd "tiktoken not found"**

```bash
uv uv pip install tiktoken
```

#### **Problemy z pamiƒôciƒÖ**

```bash
# Zmniejsz batch size dla analiz
uv run cli scrape --batch-size 25
```

#### **Problemy z wirtualnym ≈õrodowiskiem**

```bash
# Usu≈Ñ i utw√≥rz ponownie venv
rm -rf .venv
uv venv
source .venv/bin/activate
uv pip install -e ".[all]"
```

### Dlaczego uv?

- **‚ö° Szybko≈õƒá** - do 100x szybsza instalacja pakiet√≥w ni≈º pip
- **üîí Bezpiecze≈Ñstwo** - automatyczna weryfikacja integralno≈õci pakiet√≥w
- **üéØ Prostota** - jednolity interfejs dla wszystkich operacji Python
- **üåç Kompatybilno≈õƒá** - pe≈Çna zgodno≈õƒá z pip i PyPI
- **üíæ Efektywno≈õƒá** - inteligentne cache'owanie i deduplikacja

## üéÆ Przewodnik u≈ºytkownika

### Podstawowe u≈ºycie

#### 1. **Scrapowanie wszystkich for√≥w** (zalecane dla poczƒÖtkujƒÖcych)

```bash
uv run cli scrape
```

- Scrapuje wszystkie 4 fora religijne
- U≈ºywa podstawowej tokenizacji
- Zapisuje do wsp√≥lnej bazy: `data/databases/forums_unified.db`

#### 2. **Scrapowanie tygodnik√≥w katolickich**

```bash
cd weeklies_scraper
scrapy crawl gosc_niedzielny -s SQLITE_DATABASE_PATH=../data/databases/weeklies_unified.db
```

- Scrapuje tygodniki katolickie
- Zapisuje do wsp√≥lnej bazy: `data/databases/weeklies_unified.db`

#### 3. **Scrapowanie konkretnego forum**

```bash
uv run cli scrape --forum radio_katolik
```

#### 4. **Scrapowanie z analizƒÖ spaCy**

```bash
uv run cli scrape --forum wiara --analysis spacy_full --sentiment
```

### Zaawansowane opcje

#### **Wyb√≥r wielu for√≥w i analiz**

```bash
uv run cli scrape \
  --forum wiara \
  --forum dolina_modlitwy \
  --analysis basic_tokens \
  --analysis spacy_full \
  --sentiment
```

#### **Optymalizacja wydajno≈õci**

```bash
uv run cli scrape \
  --concurrent 32 \
  --delay 0.1 \
  --batch-size 200 \
  --forum radio_katolik
```

#### **Tryb testowy (bez scrapowania)**

```bash
uv run cli scrape --forum wiara --analysis all --dry-run
```

### ZarzƒÖdzanie danymi

#### **Status baz danych**

```bash
uv run cli status
```

Wy≈õwietla:

- Rozmiary plik√≥w baz danych (forums_unified.db, weeklies_unified.db)
- Liczba post√≥w/artyku≈Ç√≥w w ka≈ºdej bazie
- Daty ostatniej modyfikacji

#### **Tworzenie konfiguracji**

```bash
# Utw√≥rz plik konfiguracyjny
uv run cli config --analysis spacy_full --sentiment --output my_config.yaml

# U≈ºyj w≈Çasnej konfiguracji
uv run cli scrape --config my_config.yaml
```

## üìã Dostƒôpne ≈∫r√≥d≈Ça danych

### üï∑Ô∏è Fora religijne

| Forum               | Kod               | Opis                                   | Szacowana wielko≈õƒá |
| ------------------- | ----------------- | -------------------------------------- | ------------------ |
| **Dolina Modlitwy** | `dolina_modlitwy` | Forum katolickie, modlitwy i duchowo≈õƒá | ~50MB              |
| **Radio Katolik**   | `radio_katolik`   | Forum Radia Katolik, aktualno≈õci       | ~30MB              |
| **Wiara.pl**        | `wiara`           | Najwiƒôksze polskie forum katolickie    | ~100MB             |
| **Z Chrystusem**    | `z_chrystusem`    | Forum ewangelickie                     | ~40MB              |

### üì∞ Tygodniki katolickie

| Tygodnik                 | Kod                    | Opis                                 | Szacowana wielko≈õƒá |
| ------------------------ | ---------------------- | ------------------------------------ | ------------------ |
| **Go≈õƒá Niedzielny**      | `gosc_niedzielny`      | Tygodnik katolicki - Go≈õƒá Niedzielny | ~20MB              |
| **Niedziela**            | `niedziela`            | Tygodnik katolicki - Niedziela       | ~15MB              |
| **Przewodnik Katolicki** | `przewodnik_katolicki` | Tygodnik katolicki - Przewodnik      | ~18MB              |
| **Idziemy**              | `idziemy`              | Tygodnik katolicki - Idziemy         | ~12MB              |

### Wyb√≥r ≈∫r√≥de≈Ç danych

```bash
# Fora religijne
--forum radio_katolik
--forum wiara --forum dolina_modlitwy
--forum all

# Tygodniki katolickie
cd weeklies_scraper
scrapy crawl gosc_niedzielny -s SQLITE_DATABASE_PATH=../data/databases/weeklies_unified.db
scrapy crawl niedziela -s SQLITE_DATABASE_PATH=../data/databases/weeklies_unified.db
scrapy crawl przewodnik_katolicki -s SQLITE_DATABASE_PATH=../data/databases/weeklies_unified.db
scrapy crawl idziemy -s SQLITE_DATABASE_PATH=../data/databases/weeklies_unified.db
```

## üî¨ Typy analiz NLP

### 1. **Podstawowa tokenizacja** (`basic_tokens`)

```yaml
- name: basic_tokenizer
  config:
    lowercase: true # Konwersja na ma≈Çe litery
    min_token_length: 2 # Minimalna d≈Çugo≈õƒá tokena
    remove_punctuation: false # Zachowanie interpunkcji
```

**Wyniki:**

- Lista token√≥w
- Liczba token√≥w (total/unique)
- ≈örednia d≈Çugo≈õƒá tokena

**Zalety:** Szybka, bez zale≈ºno≈õci zewnƒôtrznych
**Wady:** Podstawowa jako≈õƒá tokenizacji

### 2. **Liczenie token√≥w OpenAI** (`token_count`)

```yaml
- name: token_counter
  config:
    encoding: cl100k_base # Model tokenizacji OpenAI
```

**Wyniki:**

- Dok≈Çadna liczba token√≥w OpenAI
- Estymacja koszt√≥w API

**Zalety:** Precyzyjne dla modeli OpenAI
**Wady:** Wymaga biblioteki tiktoken

### 3. **Pe≈Çna analiza spaCy** (`spacy_full`)

```yaml
- name: spacy_analyzer
  config:
    model: pl_core_news_lg # Model jƒôzyka polskiego (du≈ºy, najlepszy)
    include_sentiment: true # W≈ÇƒÖcz analizƒô sentymentu
    batch_size: 100 # Rozmiar batcha
    max_length: 1000000 # Maks. d≈Çugo≈õƒá tekstu
```

**Komponenty spaCy wykorzystywane:**

- **tok2vec** - wektoryzacja token√≥w (automatycznie)
- **morphologizer** - cechy morfologiczne (przypadek, liczba, rodzaj)
- **parser** - analiza sk≈Çadniowa (dependency parsing)
- **lemmatizer** - lematyzacja (formy podstawowe)
- **tagger** - tagowanie czƒô≈õci mowy (POS)
- **senter** - segmentacja zda≈Ñ
- **ner** - rozpoznawanie nazw w≈Çasnych

**Wyniki linguistyczne:**

- **Lematyzacja** - forma podstawowa s≈Ç√≥w
- **POS tagging** - czƒô≈õci mowy (rzeczownik, czasownik, etc.)
- **Dependency parsing** - relacje sk≈Çadniowe
- **Named Entity Recognition** - osoby, organizacje, miejsca, wydarzenia
- **Analiza sentymentu** - polarno≈õƒá emocjonalna z rozszerzonym s≈Çownictwem religijnym

**Statystyki tekstowe:**

- Liczba zda≈Ñ, s≈Ç√≥w, znak√≥w
- ≈örednia d≈Çugo≈õƒá zdania
- Wska≈∫nik czytelno≈õci
- Wykrywanie jƒôzyka

**Zalety:** Najwy≈ºsza jako≈õƒá analizy
**Wady:** Wymaga spaCy i modelu jƒôzykowego

### 4. **Analiza URL-√≥w** (`url_analysis`)

```yaml
- name: url_analyzer
  config:
    include_domain_analysis: true # Kategoryzacja domen
    include_url_categorization: true # Klasyfikacja URL-√≥w
    max_urls_per_post: 50 # Limit URL-√≥w per post
```

**Funkcje:**

- **Kategoryzacja domen** - religijne, media, spo≈Çeczno≈õciowe, edukacyjne
- **Klasyfikacja URL-√≥w** - artyku≈Çy, wideo, obrazy, social media
- **Ocena wiarygodno≈õci** - trust score dla domen (0.0-1.0)
- **Statystyki** - liczba domen, typy link√≥w per post
- **Deduplikacja** - unikalne domeny w bazie

**Kategorie domen:**

- **Religijne**: catholic.pl, vatican.va, opoka.org.pl, radiomaryja.pl
- **Media**: youtube.com, tvp.pl, gazeta.pl, onet.pl
- **Spo≈Çeczno≈õciowe**: facebook.com, twitter.com, instagram.com
- **Edukacyjne**: wikipedia.org, academia.edu, scholar.google.com

**Zalety:** Szczeg√≥≈Çowa analiza link√≥w, kategorie polskie
**Wady:** Brak

### 5. **Statystyki domen** (`domain_stats`)

```yaml
- name: domain_stats
  config:
    track_popularity: true # ≈öledzenie popularno≈õci domen
```

**Funkcje:**

- Podstawowe liczenie URL-√≥w i domen
- Szybka analiza bez kategoryzacji
- Deduplikacja domen

**Zalety:** Szybka, lekka
**Wady:** Brak kategoryzacji

### 6. **Wszystkie analizy** (`all`)

W≈ÇƒÖcza wszystkie dostƒôpne analizatory: tokeny + spaCy + URL-e + domeny.

## üóÑÔ∏è Struktura bazy danych

**Wsp√≥lne bazy SQLite:**

- **`data/databases/forums_unified.db`** - Wszystkie fora religijne
- **`data/databases/weeklies_unified.db`** - Wszystkie tygodniki katolickie

Wsp√≥lne bazy danych umo≈ºliwiajƒÖ:

- **Analizy por√≥wnawcze** miƒôdzy forami i tygodnikami
- **≈Åatwiejsze zapytania** SQL
- **Prostsze zarzƒÖdzanie** danymi
- **Efektywniejsze** przechowywanie

### Tabele for√≥w religijnych

#### **forums** - Informacje o forach

```sql
CREATE TABLE forums (
    id TEXT PRIMARY KEY,
    spider_name TEXT NOT NULL,
    title TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

#### **sections** - Sekcje for√≥w

```sql
CREATE TABLE sections (
    id TEXT PRIMARY KEY,
    forum_id TEXT,
    title TEXT,
    url TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (forum_id) REFERENCES forums (id)
);
```

#### **threads** - WƒÖtki dyskusji

```sql
CREATE TABLE threads (
    id TEXT PRIMARY KEY,
    section_id TEXT,
    title TEXT,
    url TEXT,
    author TEXT,
    replies INTEGER,
    views INTEGER,
    last_post_date TEXT,
    last_post_author TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (section_id) REFERENCES sections (id)
);
```

#### **users** - U≈ºytkownicy for√≥w

```sql
CREATE TABLE users (
    id TEXT PRIMARY KEY,
    username TEXT UNIQUE,
    join_date TEXT,
    posts_count INTEGER,
    religion TEXT,
    gender TEXT,
    localization TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

#### **posts** - Posty u≈ºytkownik√≥w

```sql
CREATE TABLE posts (
    id TEXT PRIMARY KEY,
    thread_id TEXT,
    user_id TEXT,
    post_number INTEGER,
    content TEXT,
    content_urls TEXT,  -- JSON z URL-ami
    post_date TEXT,
    url TEXT,
    username TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (thread_id) REFERENCES threads (id),
    FOREIGN KEY (user_id) REFERENCES users (id)
);
```

### Tabele analiz NLP

#### **post_tokens** - Tokeny post√≥w

```sql
CREATE TABLE post_tokens (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,
    token TEXT,
    position INTEGER,
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_token_stats** - Statystyki token√≥w

```sql
CREATE TABLE post_token_stats (
    post_id TEXT PRIMARY KEY,
    total_tokens INTEGER,
    unique_tokens INTEGER,
    avg_token_length REAL,
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_linguistic_analysis** - Analiza morfosyntaktyczna

```sql
CREATE TABLE post_linguistic_analysis (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,
    token TEXT,
    lemma TEXT,           -- Forma podstawowa
    pos TEXT,             -- Czƒô≈õƒá mowy
    tag TEXT,             -- Szczeg√≥≈Çowy tag
    dep TEXT,             -- Relacja sk≈Çadniowa
    morph_features TEXT,  -- Cechy morfologiczne (JSON: przypadek, liczba, rodzaj)
    is_alpha BOOLEAN,     -- Czy alfanumeryczny
    is_stop BOOLEAN,      -- Czy stop word
    is_punct BOOLEAN,     -- Czy interpunkcja
    sentiment_score REAL, -- Wynik sentymentu
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_linguistic_stats** - Statystyki jƒôzykowe

```sql
CREATE TABLE post_linguistic_stats (
    post_id TEXT PRIMARY KEY,
    sentence_count INTEGER,      -- Liczba zda≈Ñ
    word_count INTEGER,          -- Liczba s≈Ç√≥w
    char_count INTEGER,          -- Liczba znak√≥w
    avg_sentence_length REAL,    -- ≈örednia d≈Çugo≈õƒá zdania
    readability_score REAL,      -- Wska≈∫nik czytelno≈õci
    sentiment_polarity REAL,     -- Polarno≈õƒá sentymentu (-1 do 1)
    sentiment_subjectivity REAL, -- Subiektywno≈õƒá (0 do 1)
    language_detected TEXT,      -- Wykryty jƒôzyk
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **domains** - Katalog domen

```sql
CREATE TABLE domains (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    domain TEXT UNIQUE NOT NULL,     -- Nazwa domeny (np. "youtube.com")
    category TEXT,                   -- Kategoria: 'religious', 'media', 'social', 'educational', 'unknown'
    is_religious BOOLEAN DEFAULT 0,  -- Czy domena religijna
    is_media BOOLEAN DEFAULT 0,      -- Czy domena medialna
    is_social BOOLEAN DEFAULT 0,     -- Czy domena spo≈Çeczno≈õciowa
    is_educational BOOLEAN DEFAULT 0,-- Czy domena edukacyjna
    trust_score REAL DEFAULT 0.5,    -- Wska≈∫nik wiarygodno≈õci (0.0-1.0)
    first_seen TIMESTAMP,            -- Pierwsze wystƒÖpienie
    last_seen TIMESTAMP,             -- Ostatnie wystƒÖpienie
    total_references INTEGER DEFAULT 0, -- Liczba odniesie≈Ñ
    created_at TIMESTAMP
);
```

#### **post_urls** - URL-e z post√≥w

```sql
CREATE TABLE post_urls (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,                    -- ID posta
    url TEXT,                        -- Pe≈Çny URL
    domain_id INTEGER,               -- Odniesienie do tabeli domains
    url_type TEXT,                   -- Typ: 'article', 'video', 'image', 'social', 'unknown'
    is_external BOOLEAN DEFAULT 1,   -- Czy link zewnƒôtrzny
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id),
    FOREIGN KEY (domain_id) REFERENCES domains (id)
);
```

#### **post_url_stats** - Statystyki URL-√≥w per post

```sql
CREATE TABLE post_url_stats (
    post_id TEXT PRIMARY KEY,
    total_urls INTEGER DEFAULT 0,       -- Ca≈Çkowita liczba URL-√≥w
    unique_domains INTEGER DEFAULT 0,   -- Liczba unikalnych domen
    religious_urls INTEGER DEFAULT 0,   -- Liczba link√≥w religijnych
    media_urls INTEGER DEFAULT 0,       -- Liczba link√≥w medialnych
    social_urls INTEGER DEFAULT 0,      -- Liczba link√≥w spo≈Çeczno≈õciowych
    educational_urls INTEGER DEFAULT 0, -- Liczba link√≥w edukacyjnych
    unknown_urls INTEGER DEFAULT 0,     -- Liczba niekategoryzowanych link√≥w
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_named_entities** - Named Entities (nazwy w≈Çasne)

```sql
CREATE TABLE post_named_entities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,                    -- ID posta
    entity_text TEXT,                -- Tekst encji (np. "Jan Pawe≈Ç II")
    entity_label TEXT,               -- Typ: PERSON, ORG, GPE, EVENT, etc.
    entity_description TEXT,         -- Opis typu encji
    start_char INTEGER,              -- Pozycja poczƒÖtkowa w tek≈õcie
    end_char INTEGER,                -- Pozycja ko≈Ñcowa w tek≈õcie
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_ner_stats** - Statystyki Named Entities per post

```sql
CREATE TABLE post_ner_stats (
    post_id TEXT PRIMARY KEY,
    total_entities INTEGER DEFAULT 0,    -- Ca≈Çkowita liczba encji
    person_entities INTEGER DEFAULT 0,   -- Osoby (Jan Pawe≈Ç II, ≈õw. Teresa)
    org_entities INTEGER DEFAULT 0,      -- Organizacje (Ko≈õci√≥≈Ç, Watykan)
    gpe_entities INTEGER DEFAULT 0,      -- Miejsca (Rzym, Polska, Krak√≥w)
    event_entities INTEGER DEFAULT 0,    -- Wydarzenia (Wielkanoc, Bo≈ºe Narodzenie)
    other_entities INTEGER DEFAULT 0,    -- Inne encje
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

### Tabele tygodnik√≥w katolickich

#### **weeklies** - Informacje o tygodnikach

```sql
CREATE TABLE weeklies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,
    url TEXT NOT NULL,
    description TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### **issues** - Wydania tygodnik√≥w

```sql
CREATE TABLE issues (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    weekly_name TEXT NOT NULL,
    issue_name TEXT NOT NULL,
    issue_number INTEGER NOT NULL,
    issue_year INTEGER NOT NULL,
    issue_date DATE,
    issue_url TEXT NOT NULL,
    issue_cover_url TEXT,
    issue_description TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(weekly_name, issue_number, issue_year)
);
```

#### **sections** - Sekcje w tygodnikach

```sql
CREATE TABLE sections (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    weekly_name TEXT NOT NULL,
    issue_number INTEGER NOT NULL,
    issue_year INTEGER NOT NULL,
    section_name TEXT NOT NULL,
    section_url TEXT,
    section_description TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(weekly_name, issue_number, issue_year, section_name)
);
```

#### **articles** - Artyku≈Çy w tygodnikach

```sql
CREATE TABLE articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    weekly_name TEXT NOT NULL,
    issue_number INTEGER NOT NULL,
    issue_year INTEGER NOT NULL,
    section_name TEXT,
    article_title TEXT NOT NULL,
    article_intro TEXT,
    article_authors TEXT,
    article_url TEXT NOT NULL,
    article_content TEXT,
    article_tags TEXT,
    article_word_count INTEGER,
    article_image_urls TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(weekly_name, issue_number, issue_year, article_url)
);
```

#### **authors** - Autorzy artyku≈Ç√≥w

```sql
CREATE TABLE authors (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,
    bio TEXT,
    photo_url TEXT,
    social_media TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## üìä Przyk≈Çady analiz SQL

### Podstawowe statystyki

#### **PrzeglƒÖd for√≥w w wsp√≥lnej bazie**

```sql
-- Por√≥wnanie aktywno≈õci miƒôdzy forami
SELECT
    f.spider_name as forum,
    f.title,
    COUNT(DISTINCT p.id) as posts_count,
    COUNT(DISTINCT p.user_id) as users_count,
    COUNT(DISTINCT t.id) as threads_count,
    MIN(p.created_at) as first_post,
    MAX(p.created_at) as last_post
FROM forums f
LEFT JOIN sections s ON f.id = s.forum_id
LEFT JOIN threads t ON s.id = t.section_id
LEFT JOIN posts p ON t.id = p.thread_id
GROUP BY f.id, f.spider_name, f.title
ORDER BY posts_count DESC;
```

#### **Najaktywniejsze sekcje**

```sql
SELECT
    s.title as section,
    COUNT(p.id) as posts_count,
    COUNT(DISTINCT t.id) as threads_count
FROM sections s
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
GROUP BY s.id, s.title
ORDER BY posts_count DESC
LIMIT 10;
```

#### **Top u≈ºytkownicy**

```sql
SELECT
    u.username,
    COUNT(p.id) as posts_count,
    u.join_date,
    u.religion,
    u.gender
FROM users u
JOIN posts p ON u.id = p.user_id
GROUP BY u.id, u.username
ORDER BY posts_count DESC
LIMIT 20;
```

### Analizy jƒôzykowe

#### **Najczƒô≈õciej u≈ºywane s≈Çowa**

```sql
-- Top lematy (formy podstawowe s≈Ç√≥w)
SELECT
    lemma,
    COUNT(*) as frequency,
    COUNT(DISTINCT post_id) as posts_with_word
FROM post_linguistic_analysis
WHERE is_alpha = 1
  AND is_stop = 0
  AND LENGTH(lemma) > 3
GROUP BY lemma
ORDER BY frequency DESC
LIMIT 50;
```

#### **Analiza czƒô≈õci mowy**

```sql
-- Rozk≈Çad czƒô≈õci mowy
SELECT
    pos,
    COUNT(*) as count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage
FROM post_linguistic_analysis
WHERE is_alpha = 1
GROUP BY pos
ORDER BY count DESC;
```

#### **Analiza cech morfologicznych**

```sql
-- Analiza przypadk√≥w rzeczownik√≥w (wykorzystanie morphologizer)
SELECT
    json_extract(morph_features, '$.Case') as case_form,
    COUNT(*) as frequency,
    COUNT(DISTINCT post_id) as posts_count
FROM post_linguistic_analysis
WHERE pos = 'NOUN'
  AND json_extract(morph_features, '$.Case') IS NOT NULL
GROUP BY json_extract(morph_features, '$.Case')
ORDER BY frequency DESC;
```

#### **Analiza rodzaju gramatycznego**

```sql
-- Rozk≈Çad rodzaju gramatycznego w tekstach religijnych
SELECT
    f.spider_name as forum,
    json_extract(pla.morph_features, '$.Gender') as gender,
    COUNT(*) as frequency
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_analysis pla ON p.id = pla.post_id
WHERE pla.pos IN ('NOUN', 'ADJ')
  AND json_extract(pla.morph_features, '$.Gender') IS NOT NULL
GROUP BY f.spider_name, json_extract(pla.morph_features, '$.Gender')
ORDER BY f.spider_name, frequency DESC;
```

#### **Analiza sentymentu**

```sql
-- Posty z najwy≈ºszym sentymentem pozytywnym
SELECT
    f.spider_name as forum,
    p.content,
    pls.sentiment_polarity,
    pls.sentiment_subjectivity,
    u.username,
    p.post_date
FROM posts p
JOIN post_linguistic_stats pls ON p.id = pls.post_id
JOIN users u ON p.user_id = u.id
JOIN threads t ON p.thread_id = t.id
JOIN sections s ON t.section_id = s.id
JOIN forums f ON s.forum_id = f.id
WHERE pls.sentiment_polarity > 0.5
ORDER BY pls.sentiment_polarity DESC
LIMIT 10;
```

#### **Por√≥wnanie sentymentu miƒôdzy forami**

```sql
-- ≈öredni sentyment na forum
SELECT
    f.spider_name as forum,
    COUNT(pls.post_id) as analyzed_posts,
    ROUND(AVG(pls.sentiment_polarity), 3) as avg_sentiment,
    ROUND(AVG(pls.sentiment_subjectivity), 3) as avg_subjectivity,
    ROUND(AVG(pls.readability_score), 1) as avg_readability
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_stats pls ON p.id = pls.post_id
GROUP BY f.id, f.spider_name
HAVING analyzed_posts > 50
ORDER BY avg_sentiment DESC;
```

#### **Statystyki czytelno≈õci**

```sql
-- ≈örednie wska≈∫niki czytelno≈õci po sekcjach
SELECT
    s.title as section,
    COUNT(pls.post_id) as analyzed_posts,
    ROUND(AVG(pls.readability_score), 2) as avg_readability,
    ROUND(AVG(pls.avg_sentence_length), 2) as avg_sentence_len,
    ROUND(AVG(pls.word_count), 0) as avg_words
FROM sections s
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_stats pls ON p.id = pls.post_id
GROUP BY s.id, s.title
HAVING analyzed_posts > 100
ORDER BY avg_readability DESC;
```

#### **Analiza tematyczna przez s≈Çowa kluczowe**

```sql
-- Posty zawierajƒÖce s≈Çowa religijne z podzia≈Çem na fora
SELECT
    f.spider_name as forum,
    COUNT(DISTINCT p.id) as posts_with_keywords,
    COUNT(DISTINCT pla.lemma) as unique_religious_words
FROM posts p
JOIN threads t ON p.thread_id = t.id
JOIN sections s ON t.section_id = s.id
JOIN forums f ON s.forum_id = f.id
JOIN post_linguistic_analysis pla ON p.id = pla.post_id
WHERE pla.lemma IN ('b√≥g', 'jezus', 'chrystus', 'modlitwa', 'wiara', 'ko≈õci√≥≈Ç')
  AND pla.is_alpha = 1
GROUP BY f.id, f.spider_name
ORDER BY posts_with_keywords DESC;
```

#### **Analiza s≈Çownictwa religijnego miƒôdzy forami**

```sql
-- Najczƒô≈õciej u≈ºywane s≈Çowa religijne per forum
SELECT
    f.spider_name as forum,
    pla.lemma,
    COUNT(*) as frequency,
    COUNT(DISTINCT p.id) as posts_count
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_analysis pla ON p.id = pla.post_id
WHERE pla.lemma IN ('b√≥g', 'jezus', 'chrystus', 'modlitwa', 'wiara', 'ko≈õci√≥≈Ç', 'duch', '≈õwiƒôty')
  AND pla.is_alpha = 1
GROUP BY f.spider_name, pla.lemma
ORDER BY f.spider_name, frequency DESC;
```

### Analizy tygodnik√≥w katolickich

#### **PrzeglƒÖd tygodnik√≥w w bazie**

```sql
-- Statystyki tygodnik√≥w
SELECT
    w.name as weekly,
    COUNT(DISTINCT i.id) as issues_count,
    COUNT(DISTINCT a.id) as articles_count,
    COUNT(DISTINCT au.id) as authors_count,
    MIN(i.issue_date) as first_issue,
    MAX(i.issue_date) as last_issue
FROM weeklies w
LEFT JOIN issues i ON w.name = i.weekly_name
LEFT JOIN articles a ON i.weekly_name = a.weekly_name AND i.issue_number = a.issue_number AND i.issue_year = a.issue_year
LEFT JOIN authors au ON a.article_authors LIKE '%' || au.name || '%'
GROUP BY w.name
ORDER BY articles_count DESC;
```

#### **Najaktywniejsze sekcje w tygodnikach**

```sql
-- Top sekcje w tygodnikach
SELECT
    w.name as weekly,
    s.section_name,
    COUNT(a.id) as articles_count,
    AVG(a.article_word_count) as avg_word_count
FROM weeklies w
JOIN issues i ON w.name = i.weekly_name
JOIN sections s ON i.weekly_name = s.weekly_name AND i.issue_number = s.issue_number AND i.issue_year = s.issue_year
JOIN articles a ON s.weekly_name = a.weekly_name AND s.issue_number = a.issue_number AND s.issue_year = a.issue_year
GROUP BY w.name, s.section_name
ORDER BY articles_count DESC
LIMIT 20;
```

#### **Najczƒô≈õciej publikujƒÖcy autorzy**

```sql
-- Top autorzy w tygodnikach
SELECT
    au.name as author,
    COUNT(a.id) as articles_count,
    COUNT(DISTINCT a.weekly_name) as weeklies_count,
    AVG(a.article_word_count) as avg_word_count,
    MIN(a.scraped_at) as first_article,
    MAX(a.scraped_at) as last_article
FROM authors au
JOIN articles a ON a.article_authors LIKE '%' || au.name || '%'
GROUP BY au.name
ORDER BY articles_count DESC
LIMIT 20;
```

#### **Analiza tre≈õci artyku≈Ç√≥w**

```sql
-- Statystyki tre≈õci artyku≈Ç√≥w per tygodnik
SELECT
    w.name as weekly,
    COUNT(a.id) as articles_count,
    AVG(a.article_word_count) as avg_word_count,
    MIN(a.article_word_count) as min_word_count,
    MAX(a.article_word_count) as max_word_count,
    COUNT(CASE WHEN a.article_content IS NOT NULL THEN 1 END) as articles_with_content
FROM weeklies w
JOIN issues i ON w.name = i.weekly_name
JOIN articles a ON i.weekly_name = a.weekly_name AND i.issue_number = a.issue_number AND i.issue_year = a.issue_year
GROUP BY w.name
ORDER BY articles_count DESC;
```

### Analizy domen i URL-√≥w

#### **Najpopularniejsze domeny**

```sql
-- Top domeny linkowane w postach
SELECT
    d.domain,
    d.category,
    d.total_references,
    COUNT(DISTINCT pu.post_id) as posts_with_domain
FROM domains d
JOIN post_urls pu ON d.id = pu.domain_id
GROUP BY d.id, d.domain, d.category
ORDER BY d.total_references DESC
LIMIT 20;
```

#### **Analiza domen religijnych**

```sql
-- Domeny religijne per forum
SELECT
    f.spider_name as forum,
    COUNT(DISTINCT d.domain) as religious_domains,
    SUM(d.total_references) as total_religious_links
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_urls pu ON p.id = pu.post_id
JOIN domains d ON pu.domain_id = d.id
WHERE d.is_religious = 1
GROUP BY f.spider_name
ORDER BY total_religious_links DESC;
```

#### **Typy URL-√≥w per forum**

```sql
-- Rozk≈Çad typ√≥w URL-√≥w
SELECT
    f.spider_name as forum,
    pu.url_type,
    COUNT(*) as count,
    COUNT(DISTINCT pu.post_id) as posts_count
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_urls pu ON p.id = pu.post_id
GROUP BY f.spider_name, pu.url_type
ORDER BY f.spider_name, count DESC;
```

#### **Wiarygodno≈õƒá ≈∫r√≥de≈Ç**

```sql
-- ≈örednia wiarygodno≈õƒá linkowanych domen per forum
SELECT
    f.spider_name as forum,
    COUNT(DISTINCT d.domain) as unique_domains,
    ROUND(AVG(d.trust_score), 3) as avg_trust_score,
    COUNT(CASE WHEN d.trust_score >= 0.8 THEN 1 END) as high_trust_domains,
    COUNT(CASE WHEN d.trust_score < 0.6 THEN 1 END) as low_trust_domains
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_urls pu ON p.id = pu.post_id
JOIN domains d ON pu.domain_id = d.id
GROUP BY f.spider_name
ORDER BY avg_trust_score DESC;
```

### Analizy Named Entities (NER)

#### **Najczƒô≈õciej wymieniane osoby**

```sql
-- Top osoby religijne w dyskusjach
SELECT
    f.spider_name as forum,
    pne.entity_text as person,
    COUNT(*) as mentions,
    COUNT(DISTINCT pne.post_id) as posts_count
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_named_entities pne ON p.id = pne.post_id
WHERE pne.entity_label IN ('PERSON', 'PER')
GROUP BY f.spider_name, pne.entity_text
HAVING mentions > 5
ORDER BY f.spider_name, mentions DESC;
```

#### **Organizacje religijne**

```sql
-- Najczƒô≈õciej wymieniane organizacje
SELECT
    pne.entity_text as organization,
    COUNT(*) as mentions,
    COUNT(DISTINCT f.spider_name) as forums_mentioned
FROM post_named_entities pne
JOIN posts p ON pne.post_id = p.id
JOIN threads t ON p.thread_id = t.id
JOIN sections s ON t.section_id = s.id
JOIN forums f ON s.forum_id = f.id
WHERE pne.entity_label IN ('ORG', 'ORGANIZATION')
GROUP BY pne.entity_text
ORDER BY mentions DESC
LIMIT 15;
```

#### **Miejsca geograficzne**

```sql
-- Miejsca religijne w dyskusjach
SELECT
    f.spider_name as forum,
    pne.entity_text as place,
    COUNT(*) as mentions
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_named_entities pne ON p.id = pne.post_id
WHERE pne.entity_label IN ('GPE', 'LOC', 'LOCATION')
  AND (pne.entity_text LIKE '%Watykan%'
    OR pne.entity_text LIKE '%Rzym%'
    OR pne.entity_text LIKE '%Krak√≥w%'
    OR pne.entity_text LIKE '%Czƒôstochowa%')
GROUP BY f.spider_name, pne.entity_text
ORDER BY f.spider_name, mentions DESC;
```

#### **Statystyki NER per forum**

```sql
-- Por√≥wnanie rozpoznawania encji miƒôdzy forami
SELECT
    f.spider_name as forum,
    COUNT(DISTINCT pns.post_id) as posts_with_entities,
    SUM(pns.total_entities) as total_entities,
    SUM(pns.person_entities) as persons,
    SUM(pns.org_entities) as organizations,
    SUM(pns.gpe_entities) as places,
    ROUND(AVG(pns.total_entities), 2) as avg_entities_per_post
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_ner_stats pns ON p.id = pns.post_id
GROUP BY f.spider_name
ORDER BY total_entities DESC;
```

### Analizy temporalne

#### **Aktywno≈õƒá w czasie**

```sql
-- Liczba post√≥w po miesiƒÖcach
SELECT
    strftime('%Y-%m', created_at) as month,
    COUNT(*) as posts_count
FROM posts
WHERE created_at IS NOT NULL
GROUP BY month
ORDER BY month;
```

#### **Ewolucja sentymentu**

```sql
-- ≈öredni sentyment po miesiƒÖcach
SELECT
    strftime('%Y-%m', p.created_at) as month,
    COUNT(pls.post_id) as posts_analyzed,
    ROUND(AVG(pls.sentiment_polarity), 3) as avg_sentiment,
    ROUND(AVG(pls.readability_score), 1) as avg_readability
FROM posts p
JOIN post_linguistic_stats pls ON p.id = pls.post_id
WHERE p.created_at IS NOT NULL
GROUP BY month
HAVING posts_analyzed > 10
ORDER BY month;
```

## ‚öôÔ∏è Konfiguracja zaawansowana

### Plik konfiguracyjny YAML

#### **Podstawowa konfiguracja**

```yaml
# config.yaml
analysis:
  enabled: true
  analyzers:
    - name: basic_tokenizer
      config:
        lowercase: true
        min_token_length: 2

    - name: token_counter
      config:
        encoding: cl100k_base

  concurrency: 4

scrapy:
  concurrent_requests: 16
  download_delay: 0.5
  autothrottle: true
```

#### **Konfiguracja produkcyjna**

```yaml
# production_config.yaml
analysis:
  enabled: true
  analyzers:
    - name: basic_tokenizer
      config:
        lowercase: true
        min_token_length: 2
        remove_punctuation: false

    - name: token_counter
      config:
        encoding: cl100k_base

    - name: spacy_analyzer
      config:
        model: pl_core_news_lg
        include_sentiment: true
        batch_size: 200
        max_length: 1000000

  concurrency: 8

scrapy:
  concurrent_requests: 32
  concurrent_requests_per_domain: 16
  download_delay: 0.2
  randomize_download_delay: true
  autothrottle: true
  autothrottle_start_delay: 0.1
  autothrottle_max_delay: 2.0
  autothrottle_target_concurrency: 24.0
```

### Optymalizacja wydajno≈õci

#### **Dla szybkiego scrapowania**

```bash
uv run cli scrape \
  --concurrent 64 \
  --delay 0.1 \
  --analysis basic_tokens \
  --forum radio_katolik
```

#### **Dla dok≈Çadnej analizy**

```bash
uv run cli scrape \
  --concurrent 8 \
  --delay 1.0 \
  --batch-size 50 \
  --analysis all \
  --sentiment \
  --spacy-model pl_core_news_lg
```

#### **Dla ograniczonych zasob√≥w**

```bash
uv run cli scrape \
  --concurrent 4 \
  --delay 2.0 \
  --batch-size 25 \
  --analysis basic_tokens \
  --forum dolina_modlitwy
```

## üîß RozwiƒÖzywanie problem√≥w

### Czƒôste problemy

#### **1. B≈ÇƒÖd "spaCy model not found"**

```bash
# RozwiƒÖzanie
python -m spacy download pl_core_news_lg

# Lub wiƒôkszy model (lepszy, ale wolniejszy)
python -m spacy download pl_core_news_lg
```

#### **2. B≈ÇƒÖd "tiktoken not found"**

```bash
# RozwiƒÖzanie
uv pip install tiktoken
```

#### **3. Problemy z pamiƒôciƒÖ podczas analizy spaCy**

```bash
# Zmniejsz batch size
uv run cli scrape --batch-size 25 --analysis spacy_full

# Lub u≈ºyj mniejszego modelu
uv run cli scrape --spacy-model pl_core_news_lg
```

#### **4. Zbyt wolne scrapowanie**

```bash
# Zwiƒôksz r√≥wnoleg≈Ço≈õƒá (ostro≈ºnie!)
uv run cli scrape --concurrent 32 --delay 0.2

# Wy≈ÇƒÖcz analizy dla szybszego scrapowania
uv run cli scrape --analysis none
```

#### **5. B≈Çƒôdy po≈ÇƒÖczenia sieciowego**

```bash
# Zwiƒôksz op√≥≈∫nienia
uv run cli scrape --delay 2.0 --concurrent 8

# Sprawd≈∫ po≈ÇƒÖczenie internetowe
ping google.com
```

### Debugowanie

#### **W≈ÇƒÖcz szczeg√≥≈Çowe logi**

```bash
uv run cli scrape --verbose --forum radio_katolik
```

#### **Testuj bez scrapowania**

```bash
uv run cli scrape --dry-run --analysis all
```

#### **Sprawd≈∫ status baz danych**

```bash
uv run cli status
```

## üìä Monitorowanie podczas scrapowania

### **Real-time monitoring (zalecane)**

#### **Terminal 1: Uruchom scrapowanie**

```bash
uv run cli scrape \
  --forum all \
  --analysis all \
  --sentiment \
  --spacy-model pl_core_news_lg \
  --concurrent 8 \
  --delay 0.3 \
  --verbose
```

#### **Terminal 2: Monitoruj postƒôp bazy danych**

```bash
# Od≈õwie≈ºaj co 10 sekund
watch -n 10 'uv run cli status'

# Lub sprawdzaj rƒôcznie
while true; do
  clear
  echo "=== $(date) ==="
  uv run cli status
  sleep 15
done
```

#### **Terminal 3: Monitoruj zasoby systemowe**

```bash
# Monitoruj proces cli
top -pid $(pgrep -f cli)

# Lub wszystkie procesy Python
top -o cpu -stats pid,command,cpu,mem,time | grep python

# Monitoruj pamiƒôƒá
watch -n 5 'ps aux | grep cli | grep -v grep'
```

#### **Terminal 4: Monitoruj rozmiar bazy**

```bash
# Rozmiar bazy danych
watch -n 30 'ls -lh data/databases/forums_unified.db'

# Szczeg√≥≈Çowe info o pliku
watch -n 30 'stat data/databases/forums_unified.db'
```

### **Monitoring zaawansowany**

#### **Sprawd≈∫ postƒôp scrapowania**

```bash
# Liczba item√≥w w czasie rzeczywistym
watch -n 5 'sqlite3 data/databases/forums_unified.db "
SELECT
  f.spider_name,
  COUNT(p.id) as posts,
  COUNT(DISTINCT t.id) as threads,
  COUNT(DISTINCT u.id) as users
FROM forums f
LEFT JOIN sections s ON f.id = s.forum_id
LEFT JOIN threads t ON s.id = t.section_id
LEFT JOIN posts p ON t.id = p.thread_id
LEFT JOIN users u ON p.user_id = u.id
GROUP BY f.spider_name;"'
```

#### **Monitoruj wydajno≈õƒá analiz**

```bash
# Sprawd≈∫ postƒôp analiz
watch -n 10 'sqlite3 data/databases/forums_unified.db "
SELECT
  \"Tokenizacja\" as analiza, COUNT(*) as przeanalizowane FROM post_token_stats
UNION ALL
SELECT
  \"Jƒôzykowa\" as analiza, COUNT(*) as przeanalizowane FROM post_linguistic_stats
UNION ALL
SELECT
  \"NER\" as analiza, COUNT(*) as przeanalizowane FROM post_ner_stats
UNION ALL
SELECT
  \"URL-e\" as analiza, COUNT(*) as przeanalizowane FROM post_url_stats;"'
```

#### **Sprawd≈∫ b≈Çƒôdy i ostrze≈ºenia**

```bash
# Monitoruj logi Scrapy (je≈õli --verbose)
tail -f scrapy.log

# Lub sprawd≈∫ b≈Çƒôdy systemowe
dmesg | tail -20
```

### **Wska≈∫niki wydajno≈õci**

#### **Optymalne warto≈õci:**

- **CPU**: 60-80% (dla 8 wƒÖtk√≥w)
- **RAM**: 4-8GB (z modelem lg)
- **Items/min**: 100-500 (zale≈ºnie od forum)
- **Requests/min**: 200-1000 (zale≈ºnie od --delay)

#### **Sygna≈Çy problem√≥w:**

- **CPU > 95%**: Zmniejsz `--concurrent` lub `--batch-size`
- **RAM > 12GB**: Zmniejsz `--batch-size` lub u≈ºyj modelu sm
- **Items/min < 50**: Zwiƒôksz `--concurrent` lub zmniejsz `--delay`
- **B≈Çƒôdy HTTP**: Zwiƒôksz `--delay`

### **Komendy diagnostyczne**

#### **Sprawd≈∫ aktywne po≈ÇƒÖczenia**

```bash
netstat -an | grep :80 | wc -l  # HTTP connections
netstat -an | grep :443 | wc -l # HTTPS connections
```

#### **Sprawd≈∫ wykorzystanie dysku**

```bash
df -h .  # Dostƒôpne miejsce
du -sh data/databases/  # Rozmiar katalogu baz
```

#### **Sprawd≈∫ logi b≈Çƒôd√≥w**

```bash
# Ostatnie b≈Çƒôdy
grep -i error ~/.scrapy/logs/* | tail -10

# Sprawd≈∫ czy proces dzia≈Ça
ps aux | grep cli | grep -v grep
```

### **Przyk≈Çad pe≈Çnego workflow'u monitorowania**

#### **Przygotowanie (Terminal 1):**

```bash
# Sprawd≈∫ stan przed rozpoczƒôciem
uv run cli status

# Uruchom kompleksowe scrapowanie
uv run cli scrape \
  --forum all \
  --analysis all \
  --sentiment \
  --spacy-model pl_core_news_lg \
  --concurrent 8 \
  --delay 0.3 \
  --verbose
```

#### **Monitoring (Terminal 2):**

```bash
# Skrypt monitorujƒÖcy - zapisz jako monitor.sh
#!/bin/bash
while true; do
  clear
  echo "=== FORUMS SCRAPER MONITORING - $(date) ==="
  echo

  # Status bazy danych
  echo "üìä STATUS BAZY DANYCH:"
  uv run cli status
  echo

  # Zasoby systemowe
  echo "üíª ZASOBY SYSTEMOWE:"
  ps aux | grep cli | grep -v grep | head -3
  echo

# Rozmiar baz
echo "üìÅ ROZMIAR BAZ:"
ls -lh data/databases/forums_unified.db 2>/dev/null || echo "Baza for√≥w nie istnieje"
ls -lh data/databases/weeklies_unified.db 2>/dev/null || echo "Baza tygodnik√≥w nie istnieje"
  echo

  sleep 30
done

# Uruchom monitoring
chmod +x monitor.sh && ./monitor.sh
```

#### **Analiza postƒôpu (Terminal 3):**

```bash
# Real-time SQL monitoring
watch -n 15 'echo "POSTƒòP SCRAPOWANIA FOR√ìW:" && sqlite3 data/databases/forums_unified.db "
SELECT
  f.spider_name as Forum,
  COUNT(DISTINCT s.id) as Sekcje,
  COUNT(DISTINCT t.id) as WƒÖtki,
  COUNT(DISTINCT p.id) as Posty,
  COUNT(DISTINCT u.id) as U≈ºytkownicy
FROM forums f
LEFT JOIN sections s ON f.id = s.forum_id
LEFT JOIN threads t ON s.id = t.section_id
LEFT JOIN posts p ON t.id = p.thread_id
LEFT JOIN users u ON p.user_id = u.id
GROUP BY f.spider_name
ORDER BY Posty DESC;" 2>/dev/null || echo "Baza for√≥w nie gotowa"'

# Monitoring tygodnik√≥w
watch -n 15 'echo "POSTƒòP SCRAPOWANIA TYGODNIK√ìW:" && sqlite3 data/databases/weeklies_unified.db "
SELECT
  w.name as Tygodnik,
  COUNT(DISTINCT i.id) as Wydania,
  COUNT(DISTINCT a.id) as Artyku≈Çy,
  COUNT(DISTINCT au.id) as Autorzy
FROM weeklies w
LEFT JOIN issues i ON w.name = i.weekly_name
LEFT JOIN articles a ON i.weekly_name = a.weekly_name AND i.issue_number = a.issue_number AND i.issue_year = a.issue_year
LEFT JOIN authors au ON a.article_authors LIKE '%' || au.name || '%'
GROUP BY w.name
ORDER BY Artyku≈Çy DESC;" 2>/dev/null || echo "Baza tygodnik√≥w nie gotowa"'
```

## üèóÔ∏è Architektura systemu

### Struktura projektu

```
forums_scraper/
‚îú‚îÄ‚îÄ üìÑ README.md                    # Kompletna dokumentacja (ten plik)
‚îú‚îÄ‚îÄ ‚öôÔ∏è  pyproject.toml               # Konfiguracja pakietu Python
‚îú‚îÄ‚îÄ üîß scrapy.cfg                   # Konfiguracja Scrapy
‚îú‚îÄ‚îÄ üìÇ data/                        # Wsp√≥lne dane
‚îÇ   ‚îî‚îÄ‚îÄ databases/                  # SQLite bazy danych
‚îÇ       ‚îú‚îÄ‚îÄ forums_unified.db       # Baza for√≥w
‚îÇ       ‚îî‚îÄ‚îÄ weeklies_unified.db     # Baza tygodnik√≥w
‚îú‚îÄ‚îÄ core/                        # Wsp√≥lny rdze≈Ñ systemu ‚ú®
‚îÇ   ‚îú‚îÄ‚îÄ protocol.py                 #   ‚îú‚îÄ‚îÄ Analyzer Protocol
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   #   ‚îú‚îÄ‚îÄ Konfiguracja YAML/TOML
‚îÇ   ‚îú‚îÄ‚îÄ registry.py                 #   ‚îú‚îÄ‚îÄ ≈Åadowanie analizator√≥w
‚îÇ   ‚îî‚îÄ‚îÄ runner.py                   #   ‚îî‚îÄ‚îÄ Orchestracja analiz
‚îú‚îÄ‚îÄ analyzers/                   # üî¨ Wsp√≥lne analizatory NLP
‚îÇ   ‚îú‚îÄ‚îÄ linguistic.py               #   ‚îú‚îÄ‚îÄ SpacyAnalyzer
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py                #   ‚îî‚îÄ‚îÄ TokenCountAnalyzer
‚îÇ   ‚îî‚îÄ‚îÄ url_analyzer.py             #   ‚îî‚îÄ‚îÄ URLAnalyzer, DomainStatsAnalyzer
‚îú‚îÄ‚îÄ cli/                         # üñ•Ô∏è Wsp√≥lne CLI
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     #   ‚îî‚îÄ‚îÄ Entry point
‚îÇ   ‚îî‚îÄ‚îÄ advanced.py                 #   ‚îî‚îÄ‚îÄ Zaawansowane funkcje
‚îú‚îÄ‚îÄ forums_scraper/              # üï∑Ô∏è Scraper for√≥w
‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py              #   ‚îú‚îÄ‚îÄ Middleware
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/                  #   ‚îú‚îÄ‚îÄ Pipeline'y
‚îÇ   ‚îú‚îÄ‚îÄ spiders/                    #   ‚îú‚îÄ‚îÄ Spidery for√≥w
‚îÇ   ‚îú‚îÄ‚îÄ items.py                    #   ‚îú‚îÄ‚îÄ Modele danych
‚îÇ   ‚îú‚îÄ‚îÄ settings.py                 #   ‚îî‚îÄ‚îÄ Ustawienia Scrapy
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                    #   ‚îî‚îÄ‚îÄ Narzƒôdzia pomocnicze
‚îî‚îÄ‚îÄ weeklies_scraper/            # üì∞ Scraper tygodnik√≥w
    ‚îú‚îÄ‚îÄ spiders/                    #   ‚îú‚îÄ‚îÄ Spidery tygodnik√≥w
    ‚îú‚îÄ‚îÄ items.py                    #   ‚îú‚îÄ‚îÄ Modele danych
    ‚îú‚îÄ‚îÄ pipelines.py                #   ‚îú‚îÄ‚îÄ Pipeline'y
    ‚îú‚îÄ‚îÄ middlewares.py              #   ‚îú‚îÄ‚îÄ Middleware
    ‚îú‚îÄ‚îÄ settings.py                 #   ‚îî‚îÄ‚îÄ Ustawienia Scrapy
    ‚îî‚îÄ‚îÄ scrapy.cfg                  #   ‚îî‚îÄ‚îÄ Konfiguracja Scrapy
```

### Przep≈Çyw danych

```mermaid
graph TD
    A[uv run cli scrape] --> B[Scrapy Engine]
    B --> C[Forum Spider]
    C --> D[Web Scraping]
    D --> E[Items]
    E --> F[Analysis Pipeline]
    F --> G[NLP Analyzers]
    G --> H[Database Pipeline]
    H --> I[forums_unified.db]

    J[scrapy crawl weekly] --> K[Weekly Spider]
    K --> L[Web Scraping]
    L --> M[Items]
    M --> N[Analysis Pipeline]
    N --> O[NLP Analyzers]
    O --> P[Database Pipeline]
    P --> Q[weeklies_unified.db]

    R[Config YAML] --> F
    R --> N
    S[Entry Points] --> G
    S --> O
    T[core/] --> F
    T --> N
```

### Wzorce projektowe

- **Plugin Architecture** - Entry points dla analizator√≥w
- **Pipeline Pattern** - Scrapy pipelines dla przetwarzania
- **Strategy Pattern** - R√≥≈ºne typy analiz
- **Observer Pattern** - Progress reporting
- **Factory Pattern** - Tworzenie analizator√≥w
- **Shared Core Pattern** - Wsp√≥lny rdze≈Ñ `core/` dla wszystkich scraper√≥w

## üöÄ Rozw√≥j i wk≈Çad

### Dodawanie nowych analizator√≥w

#### **1. Implementacja analizatora**

```python
# my_analyzer.py
from typing import Any, Dict
from core.protocol import Analyzer

class MyCustomAnalyzer(Analyzer):
    def __init__(self, **config):
        self.config = config

    async def setup(self):
        # Inicjalizacja (≈Çadowanie modeli, etc.)
        pass

    async def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        content = data.get('content', '')

        # Twoja analiza tutaj
        result = self.my_analysis_function(content)

        return {
            'my_analysis': result
        }

    async def close(self):
        # SprzƒÖtanie zasob√≥w
        pass
```

#### **2. Rejestracja w pyproject.toml**

```toml
[project.entry-points."forums_scraper.analyzers"]
my_analyzer = "my_package.my_analyzer:MyCustomAnalyzer"
```

#### **3. U≈ºycie w konfiguracji**

```yaml
analysis:
  enabled: true
  analyzers:
    - name: my_analyzer
      config:
        parameter1: value1
        parameter2: value2
```

### Dodawanie nowych for√≥w

#### **1. Implementacja spidera**

```python
# new_forum_spider.py
import scrapy
from forums_scraper.items import ForumPostItem

class NewForumSpider(scrapy.Spider):
    name = 'new_forum'
    allowed_domains = ['newforum.com']
    start_urls = ['https://newforum.com']

    def parse(self, response):
        # Implementacja scrapowania
        pass
```

### Dodawanie nowych tygodnik√≥w

#### **1. Implementacja spidera tygodnika**

```python
# new_weekly_spider.py
import scrapy
from items import WeeklyItem, IssueItem, ArticleItem

class NewWeeklySpider(scrapy.Spider):
    name = 'new_weekly'
    allowed_domains = ['newweekly.com']
    start_urls = ['https://newweekly.com/archiwum']

    def parse_archive(self, response):
        # Implementacja scrapowania archiwum
        pass
```

#### **2. Uruchomienie tygodnika**

```bash
# Z katalogu weeklies_scraper/
cd weeklies_scraper
scrapy crawl new_weekly -s LOG_LEVEL=INFO -s SQLITE_DATABASE_PATH=../data/databases/weeklies_unified.db
```

#### **3. Aktualizacja CLI**

```python
# W cli/advanced.py
class ForumName(str, Enum):
    # ... istniejƒÖce fora
    NEW_FORUM = "new_forum"

FORUM_SPIDER_MAP = {
    # ... istniejƒÖce mapowania
    ForumName.NEW_FORUM: "new_forum",
}
```

### Testowanie

```bash
# Testy jednostkowe
uv run python -m pytest tests/

# Testy integracyjne
uv run python -m pytest tests/integration/

# Testy analizator√≥w
uv run python -m pytest tests/analyzers/
```

### Code style

```bash
# Formatowanie kodu
uv run black .
uv run isort .

# Linting
uv run flake8 .
uv run mypy .
```

## üìö Zasoby dodatkowe

### Dokumentacja techniczna

- **README.md** (ten plik) - Kompletna dokumentacja
- [API Documentation](docs/api/) - Dokumentacja API (w przygotowaniu)
- [Database Schema](docs/database.md) - Schemat bazy danych (w przygotowaniu)
- **Wsp√≥lny rdze≈Ñ** - `core/` - Wsp√≥lne komponenty dla wszystkich scraper√≥w

### Przyk≈Çady u≈ºycia

- **README.md** (ten plik) - Kompletne przyk≈Çady SQL i konfiguracji
- **CLI** - Automatyczne generowanie konfiguracji: `uv run cli config`
- **Dokumentacja inline** - Wszystkie przyk≈Çady w tym pliku

### Spo≈Çeczno≈õƒá

- **Issues** - [GitHub Issues](https://github.com/username/forums_scraper/issues)
- **Discussions** - [GitHub Discussions](https://github.com/username/forums_scraper/discussions)
- **Wiki** - [Project Wiki](https://github.com/username/forums_scraper/wiki)

## üìÑ Licencja

MIT License - zobacz [LICENSE](LICENSE) dla szczeg√≥≈Ç√≥w.

## üôè Podziƒôkowania

- **Astral Team** - za ultraszybki mened≈ºer pakiet√≥w uv
- **Scrapy Team** - za doskona≈Çy framework scrapowania
- **spaCy Team** - za zaawansowane narzƒôdzia NLP
- **Rich Team** - za piƒôkny interfejs CLI
- **Spo≈Çeczno≈õƒá Open Source** - za inspiracjƒô i wsparcie
- **Wsp√≥lny rdze≈Ñ** - `core/` - za modularnƒÖ architekturƒô

---

**Autor:** alb  
**Wersja:** 0.1.0  
**Python:** 3.10+  
**Licencja:** MIT  
**Architektura:** Wsp√≥lny rdze≈Ñ `core/` dla for√≥w i tygodnik√≥w

_Forums & Weeklies Scraper - Profesjonalne narzƒôdzie do analizy dyskursu religijnego w internecie i mediach_ üï∑Ô∏èüì∞‚ú®
