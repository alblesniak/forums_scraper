# ğŸ•·ï¸ Forums Scraper

**Zaawansowany scraper forÃ³w religijnych z rÃ³wnolegÅ‚ymi analizami NLP i bazami danych SQLite**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Scrapy](https://img.shields.io/badge/scrapy-2.11+-green.svg)](https://scrapy.org/)
[![spaCy](https://img.shields.io/badge/spaCy-3.4+-orange.svg)](https://spacy.io/)
[![Rich CLI](https://img.shields.io/badge/CLI-Rich%20%2B%20Typer-purple.svg)](https://rich.readthedocs.io/)

## ğŸ¯ Opis projektu

Forums Scraper to profesjonalne narzÄ™dzie do scrapowania forÃ³w religijnych z zaawansowanymi funkcjami analizy tekstu. System zostaÅ‚ zaprojektowany z myÅ›lÄ… o badaniach lingwistycznych, analizie sentymentu i modelowaniu tematÃ³w w kontekÅ›cie dyskusji religijnych.

### ğŸ¯ GÅ‚Ã³wne cele

- **Badania naukowe**: Analiza dyskursu religijnego w internecie
- **Analiza sentymentu**: Badanie nastrojÃ³w w spoÅ‚ecznoÅ›ciach religijnych
- **Modelowanie tematÃ³w**: Identyfikacja gÅ‚Ã³wnych tematÃ³w dyskusji
- **Archiwizacja**: DÅ‚ugoterminowe przechowywanie treÅ›ci forÃ³w

## âœ¨ Kluczowe funkcjonalnoÅ›ci

### ğŸ”„ **RÃ³wnolegÅ‚e analizy NLP**

- **Tokenizacja podstawowa** - szybka analiza bez zaleÅ¼noÅ›ci zewnÄ™trznych
- **Liczenie tokenÃ³w OpenAI** - estymacja kosztÃ³w API dla modeli jÄ™zykowych
- **PeÅ‚na analiza spaCy** - lematyzacja, POS tagging, dependency parsing
- **Analiza sentymentu** - wykrywanie emocji w tekÅ›cie
- **Statystyki jÄ™zykowe** - czytelnoÅ›Ä‡, dÅ‚ugoÅ›Ä‡ zdaÅ„, bogactwo sÅ‚ownictwa

### ğŸ—„ï¸ **Zaawansowana baza danych**

- **Osobne bazy SQLite** dla kaÅ¼dego forum
- **PeÅ‚ny schemat relacyjny** - fora, sekcje, wÄ…tki, uÅ¼ytkownicy, posty
- **Tabele analiz** - tokeny, statystyki jÄ™zykowe, analiza morfosyntaktyczna
- **Indeksy wydajnoÅ›ciowe** - szybkie zapytania analityczne
- **Automatyczne backupy** - bezpieczne przechowywanie danych

### ğŸ›ï¸ **Profesjonalny CLI**

- **Rich interface** - kolorowy, interaktywny interfejs
- **Progress tracking** - Å›ledzenie postÄ™pu w czasie rzeczywistym
- **WybÃ³r forÃ³w** - elastyczna selekcja ÅºrÃ³deÅ‚ danych
- **Konfiguracja analiz** - dostosowanie do potrzeb badawczych
- **Dry-run mode** - testowanie bez wykonywania operacji
- **Status monitoring** - przeglÄ…d stanu baz danych

### âš™ï¸ **Elastyczna architektura**

- **Plugin system** - Å‚atwe dodawanie nowych analizatorÃ³w
- **Entry points** - automatyczne wykrywanie dostÄ™pnych analiz
- **YAML/CLI configuration** - wygodna konfiguracja
- **Asynchronous processing** - wydajne przetwarzanie rÃ³wnolegÅ‚e
- **Error handling** - odporna na bÅ‚Ä™dy architektura

## ğŸš€ Instalacja i pierwsze uruchomienie

### Wymagania systemowe

- **Python 3.10+** (sprawdÅº: `python --version`)
- **4GB RAM** (minimum), **8GB RAM** (zalecane dla spaCy)
- **PoÅ‚Ä…czenie internetowe** dla scrapowania
- **~500MB** miejsca na dysku (zaleÅ¼nie od liczby forÃ³w)

### Krok 1: Pobranie kodu

```bash
# Klonowanie repozytorium
git clone https://github.com/username/forums_scraper.git
cd forums_scraper
```

### Krok 2: Instalacja

#### **Opcja A: Instalacja podstawowa** (tylko scrapowanie)

```bash
pip install -e .
```

#### **Opcja B: Z analizatorami podstawowymi** (+ tiktoken)

```bash
pip install -e ".[analyzers-basic]"
```

#### **Opcja C: Z peÅ‚nymi analizatorami** (+ spaCy)

```bash
pip install -e ".[analyzers-linguistic]"
python -m spacy download pl_core_news_sm
```

#### **Opcja D: PeÅ‚na instalacja** (zalecane)

```bash
pip install -e ".[all]"
python -m spacy download pl_core_news_sm
```

### Krok 3: Weryfikacja

```bash
# SprawdÅº czy CLI dziaÅ‚a
fs-cli --help

# Lista dostÄ™pnych forÃ³w
fs-cli list-spiders

# Lista dostÄ™pnych analizatorÃ³w
fs-cli list-analyzers

# Test bez scrapowania
fs-cli scrape --forum radio_katolik --dry-run
```

### Krok 4: Pierwsze uruchomienie

```bash
# Scrapuj jedno forum z podstawowÄ… analizÄ…
fs-cli scrape --forum radio_katolik --analysis basic_tokens

# SprawdÅº wyniki
fs-cli status
```

### RozwiÄ…zywanie problemÃ³w instalacji

#### **BÅ‚Ä…d "fs-cli: command not found"**

```bash
# ZnajdÅº Å›cieÅ¼kÄ™ do skryptu
find ~/.pyenv -name "fs-cli" 2>/dev/null

# UÅ¼yj peÅ‚nej Å›cieÅ¼ki (przykÅ‚ad)
~/.pyenv/versions/3.11.9/bin/fs-cli --help
```

#### **BÅ‚Ä…d "spaCy model not found"**

```bash
python -m spacy download pl_core_news_sm

# Lub wiÄ™kszy model (lepszy, ale wolniejszy)
python -m spacy download pl_core_news_lg
```

#### **BÅ‚Ä…d "tiktoken not found"**

```bash
pip install tiktoken
```

#### **Problemy z pamiÄ™ciÄ…**

```bash
# Zmniejsz batch size dla analiz
fs-cli scrape --batch-size 25
```

## ğŸ® Przewodnik uÅ¼ytkownika

### Podstawowe uÅ¼ycie

#### 1. **Scrapowanie wszystkich forÃ³w** (zalecane dla poczÄ…tkujÄ…cych)

```bash
fs-cli scrape
```

- Scrapuje wszystkie 4 fora
- UÅ¼ywa podstawowej tokenizacji
- Zapisuje do `data/databases/forum_*.db`

#### 2. **Scrapowanie konkretnego forum**

```bash
fs-cli scrape --forum radio_katolik
```

#### 3. **Scrapowanie z analizÄ… spaCy**

```bash
fs-cli scrape --forum wiara --analysis spacy_full --sentiment
```

### Zaawansowane opcje

#### **WybÃ³r wielu forÃ³w i analiz**

```bash
fs-cli scrape \
  --forum wiara \
  --forum dolina_modlitwy \
  --analysis basic_tokens \
  --analysis spacy_full \
  --sentiment
```

#### **Optymalizacja wydajnoÅ›ci**

```bash
fs-cli scrape \
  --concurrent 32 \
  --delay 0.1 \
  --batch-size 200 \
  --forum radio_katolik
```

#### **Tryb testowy (bez scrapowania)**

```bash
fs-cli scrape --forum wiara --analysis all --dry-run
```

### ZarzÄ…dzanie danymi

#### **Status baz danych**

```bash
fs-cli status
```

WyÅ›wietla:

- Rozmiary plikÃ³w baz danych
- Liczba postÃ³w w kaÅ¼dej bazie
- Daty ostatniej modyfikacji

#### **Tworzenie konfiguracji**

```bash
# UtwÃ³rz plik konfiguracyjny
fs-cli config --analysis spacy_full --sentiment --output my_config.yaml

# UÅ¼yj wÅ‚asnej konfiguracji
fs-cli scrape --config my_config.yaml
```

## ğŸ“‹ DostÄ™pne fora

| Forum               | Kod               | Opis                                   | Szacowana wielkoÅ›Ä‡ |
| ------------------- | ----------------- | -------------------------------------- | ------------------ |
| **Dolina Modlitwy** | `dolina_modlitwy` | Forum katolickie, modlitwy i duchowoÅ›Ä‡ | ~50MB              |
| **Radio Katolik**   | `radio_katolik`   | Forum Radia Katolik, aktualnoÅ›ci       | ~30MB              |
| **Wiara.pl**        | `wiara`           | NajwiÄ™ksze polskie forum katolickie    | ~100MB             |
| **Z Chrystusem**    | `z_chrystusem`    | Forum ewangelickie                     | ~40MB              |

### WybÃ³r forÃ³w

```bash
# Pojedyncze forum
--forum radio_katolik

# Wiele forÃ³w
--forum wiara --forum dolina_modlitwy

# Wszystkie fora (domyÅ›lne)
--forum all
```

## ğŸ”¬ Typy analiz NLP

### 1. **Podstawowa tokenizacja** (`basic_tokens`)

```yaml
- name: basic_tokenizer
  config:
    lowercase: true # Konwersja na maÅ‚e litery
    min_token_length: 2 # Minimalna dÅ‚ugoÅ›Ä‡ tokena
    remove_punctuation: false # Zachowanie interpunkcji
```

**Wyniki:**

- Lista tokenÃ³w
- Liczba tokenÃ³w (total/unique)
- Åšrednia dÅ‚ugoÅ›Ä‡ tokena

**Zalety:** Szybka, bez zaleÅ¼noÅ›ci zewnÄ™trznych
**Wady:** Podstawowa jakoÅ›Ä‡ tokenizacji

### 2. **Liczenie tokenÃ³w OpenAI** (`token_count`)

```yaml
- name: token_counter
  config:
    encoding: cl100k_base # Model tokenizacji OpenAI
```

**Wyniki:**

- DokÅ‚adna liczba tokenÃ³w OpenAI
- Estymacja kosztÃ³w API

**Zalety:** Precyzyjne dla modeli OpenAI
**Wady:** Wymaga biblioteki tiktoken

### 3. **PeÅ‚na analiza spaCy** (`spacy_full`)

```yaml
- name: spacy_analyzer
  config:
    model: pl_core_news_sm # Model jÄ™zyka polskiego
    include_sentiment: true # WÅ‚Ä…cz analizÄ™ sentymentu
    batch_size: 100 # Rozmiar batcha
    max_length: 1000000 # Maks. dÅ‚ugoÅ›Ä‡ tekstu
```

**Wyniki linguistyczne:**

- **Lematyzacja** - forma podstawowa sÅ‚Ã³w
- **POS tagging** - czÄ™Å›ci mowy (rzeczownik, czasownik, etc.)
- **Dependency parsing** - relacje skÅ‚adniowe
- **Named Entity Recognition** - rozpoznawanie nazw wÅ‚asnych
- **Analiza sentymentu** - polarnoÅ›Ä‡ emocjonalna

**Statystyki tekstowe:**

- Liczba zdaÅ„, sÅ‚Ã³w, znakÃ³w
- Åšrednia dÅ‚ugoÅ›Ä‡ zdania
- WskaÅºnik czytelnoÅ›ci
- Wykrywanie jÄ™zyka

**Zalety:** NajwyÅ¼sza jakoÅ›Ä‡ analizy
**Wady:** Wymaga spaCy i modelu jÄ™zykowego

### 4. **Wszystkie analizy** (`all`)

WÅ‚Ä…cza wszystkie dostÄ™pne analizatory jednoczeÅ›nie.

## ğŸ—„ï¸ Struktura bazy danych

### Tabele gÅ‚Ã³wne

#### **forums** - Informacje o forach

```sql
CREATE TABLE forums (
    id TEXT PRIMARY KEY,
    spider_name TEXT NOT NULL,
    title TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

#### **sections** - Sekcje forÃ³w

```sql
CREATE TABLE sections (
    id TEXT PRIMARY KEY,
    forum_id TEXT,
    title TEXT,
    url TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (forum_id) REFERENCES forums (id)
);
```

#### **threads** - WÄ…tki dyskusji

```sql
CREATE TABLE threads (
    id TEXT PRIMARY KEY,
    section_id TEXT,
    title TEXT,
    url TEXT,
    author TEXT,
    replies INTEGER,
    views INTEGER,
    last_post_date TEXT,
    last_post_author TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (section_id) REFERENCES sections (id)
);
```

#### **users** - UÅ¼ytkownicy forÃ³w

```sql
CREATE TABLE users (
    id TEXT PRIMARY KEY,
    username TEXT UNIQUE,
    join_date TEXT,
    posts_count INTEGER,
    religion TEXT,
    gender TEXT,
    localization TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

#### **posts** - Posty uÅ¼ytkownikÃ³w

```sql
CREATE TABLE posts (
    id TEXT PRIMARY KEY,
    thread_id TEXT,
    user_id TEXT,
    post_number INTEGER,
    content TEXT,
    content_urls TEXT,  -- JSON z URL-ami
    post_date TEXT,
    url TEXT,
    username TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (thread_id) REFERENCES threads (id),
    FOREIGN KEY (user_id) REFERENCES users (id)
);
```

### Tabele analiz NLP

#### **post_tokens** - Tokeny postÃ³w

```sql
CREATE TABLE post_tokens (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,
    token TEXT,
    position INTEGER,
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_token_stats** - Statystyki tokenÃ³w

```sql
CREATE TABLE post_token_stats (
    post_id TEXT PRIMARY KEY,
    total_tokens INTEGER,
    unique_tokens INTEGER,
    avg_token_length REAL,
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_linguistic_analysis** - Analiza morfosyntaktyczna

```sql
CREATE TABLE post_linguistic_analysis (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,
    token TEXT,
    lemma TEXT,           -- Forma podstawowa
    pos TEXT,             -- CzÄ™Å›Ä‡ mowy
    tag TEXT,             -- SzczegÃ³Å‚owy tag
    dep TEXT,             -- Relacja skÅ‚adniowa
    is_alpha BOOLEAN,     -- Czy alfanumeryczny
    is_stop BOOLEAN,      -- Czy stop word
    is_punct BOOLEAN,     -- Czy interpunkcja
    sentiment_score REAL, -- Wynik sentymentu
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_linguistic_stats** - Statystyki jÄ™zykowe

```sql
CREATE TABLE post_linguistic_stats (
    post_id TEXT PRIMARY KEY,
    sentence_count INTEGER,      -- Liczba zdaÅ„
    word_count INTEGER,          -- Liczba sÅ‚Ã³w
    char_count INTEGER,          -- Liczba znakÃ³w
    avg_sentence_length REAL,    -- Åšrednia dÅ‚ugoÅ›Ä‡ zdania
    readability_score REAL,      -- WskaÅºnik czytelnoÅ›ci
    sentiment_polarity REAL,     -- PolarnoÅ›Ä‡ sentymentu (-1 do 1)
    sentiment_subjectivity REAL, -- SubiektywnoÅ›Ä‡ (0 do 1)
    language_detected TEXT,      -- Wykryty jÄ™zyk
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

## ğŸ“Š PrzykÅ‚ady analiz SQL

### Podstawowe statystyki

#### **PrzeglÄ…d forÃ³w**

```sql
-- Liczba postÃ³w na forum
SELECT
    f.title as forum,
    COUNT(p.id) as posts_count,
    COUNT(DISTINCT p.user_id) as users_count,
    MIN(p.created_at) as first_post,
    MAX(p.created_at) as last_post
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
GROUP BY f.id, f.title;
```

#### **Najaktywniejsze sekcje**

```sql
SELECT
    s.title as section,
    COUNT(p.id) as posts_count,
    COUNT(DISTINCT t.id) as threads_count
FROM sections s
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
GROUP BY s.id, s.title
ORDER BY posts_count DESC
LIMIT 10;
```

#### **Top uÅ¼ytkownicy**

```sql
SELECT
    u.username,
    COUNT(p.id) as posts_count,
    u.join_date,
    u.religion,
    u.gender
FROM users u
JOIN posts p ON u.id = p.user_id
GROUP BY u.id, u.username
ORDER BY posts_count DESC
LIMIT 20;
```

### Analizy jÄ™zykowe

#### **NajczÄ™Å›ciej uÅ¼ywane sÅ‚owa**

```sql
-- Top lematy (formy podstawowe sÅ‚Ã³w)
SELECT
    lemma,
    COUNT(*) as frequency,
    COUNT(DISTINCT post_id) as posts_with_word
FROM post_linguistic_analysis
WHERE is_alpha = 1
  AND is_stop = 0
  AND LENGTH(lemma) > 3
GROUP BY lemma
ORDER BY frequency DESC
LIMIT 50;
```

#### **Analiza czÄ™Å›ci mowy**

```sql
-- RozkÅ‚ad czÄ™Å›ci mowy
SELECT
    pos,
    COUNT(*) as count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage
FROM post_linguistic_analysis
WHERE is_alpha = 1
GROUP BY pos
ORDER BY count DESC;
```

#### **Analiza sentymentu**

```sql
-- Posty z najwyÅ¼szym sentymentem pozytywnym
SELECT
    p.content,
    pls.sentiment_polarity,
    pls.sentiment_subjectivity,
    u.username,
    p.post_date
FROM posts p
JOIN post_linguistic_stats pls ON p.id = pls.post_id
JOIN users u ON p.user_id = u.id
WHERE pls.sentiment_polarity > 0.5
ORDER BY pls.sentiment_polarity DESC
LIMIT 10;
```

#### **Statystyki czytelnoÅ›ci**

```sql
-- Åšrednie wskaÅºniki czytelnoÅ›ci po sekcjach
SELECT
    s.title as section,
    COUNT(pls.post_id) as analyzed_posts,
    ROUND(AVG(pls.readability_score), 2) as avg_readability,
    ROUND(AVG(pls.avg_sentence_length), 2) as avg_sentence_len,
    ROUND(AVG(pls.word_count), 0) as avg_words
FROM sections s
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_stats pls ON p.id = pls.post_id
GROUP BY s.id, s.title
HAVING analyzed_posts > 100
ORDER BY avg_readability DESC;
```

#### **Analiza tematyczna przez sÅ‚owa kluczowe**

```sql
-- Posty zawierajÄ…ce sÅ‚owa religijne
SELECT DISTINCT
    p.id,
    p.content,
    u.username,
    s.title as section
FROM posts p
JOIN users u ON p.user_id = u.id
JOIN threads t ON p.thread_id = t.id
JOIN sections s ON t.section_id = s.id
JOIN post_linguistic_analysis pla ON p.id = pla.post_id
WHERE pla.lemma IN ('bÃ³g', 'jezus', 'chrystus', 'modlitwa', 'wiara', 'koÅ›ciÃ³Å‚')
  AND pla.is_alpha = 1
LIMIT 20;
```

### Analizy temporalne

#### **AktywnoÅ›Ä‡ w czasie**

```sql
-- Liczba postÃ³w po miesiÄ…cach
SELECT
    strftime('%Y-%m', created_at) as month,
    COUNT(*) as posts_count
FROM posts
WHERE created_at IS NOT NULL
GROUP BY month
ORDER BY month;
```

#### **Ewolucja sentymentu**

```sql
-- Åšredni sentyment po miesiÄ…cach
SELECT
    strftime('%Y-%m', p.created_at) as month,
    COUNT(pls.post_id) as posts_analyzed,
    ROUND(AVG(pls.sentiment_polarity), 3) as avg_sentiment,
    ROUND(AVG(pls.readability_score), 1) as avg_readability
FROM posts p
JOIN post_linguistic_stats pls ON p.id = pls.post_id
WHERE p.created_at IS NOT NULL
GROUP BY month
HAVING posts_analyzed > 10
ORDER BY month;
```

## âš™ï¸ Konfiguracja zaawansowana

### Plik konfiguracyjny YAML

#### **Podstawowa konfiguracja**

```yaml
# config.yaml
analysis:
  enabled: true
  analyzers:
    - name: basic_tokenizer
      config:
        lowercase: true
        min_token_length: 2

    - name: token_counter
      config:
        encoding: cl100k_base

  concurrency: 4

scrapy:
  concurrent_requests: 16
  download_delay: 0.5
  autothrottle: true
```

#### **Konfiguracja produkcyjna**

```yaml
# production_config.yaml
analysis:
  enabled: true
  analyzers:
    - name: basic_tokenizer
      config:
        lowercase: true
        min_token_length: 2
        remove_punctuation: false

    - name: token_counter
      config:
        encoding: cl100k_base

    - name: spacy_analyzer
      config:
        model: pl_core_news_sm
        include_sentiment: true
        batch_size: 200
        max_length: 1000000

  concurrency: 8

scrapy:
  concurrent_requests: 32
  concurrent_requests_per_domain: 16
  download_delay: 0.2
  randomize_download_delay: true
  autothrottle: true
  autothrottle_start_delay: 0.1
  autothrottle_max_delay: 2.0
  autothrottle_target_concurrency: 24.0
```

### Optymalizacja wydajnoÅ›ci

#### **Dla szybkiego scrapowania**

```bash
fs-cli scrape \
  --concurrent 64 \
  --delay 0.1 \
  --analysis basic_tokens \
  --forum radio_katolik
```

#### **Dla dokÅ‚adnej analizy**

```bash
fs-cli scrape \
  --concurrent 8 \
  --delay 1.0 \
  --batch-size 50 \
  --analysis all \
  --sentiment \
  --spacy-model pl_core_news_lg
```

#### **Dla ograniczonych zasobÃ³w**

```bash
fs-cli scrape \
  --concurrent 4 \
  --delay 2.0 \
  --batch-size 25 \
  --analysis basic_tokens \
  --forum dolina_modlitwy
```

## ğŸ”§ RozwiÄ…zywanie problemÃ³w

### CzÄ™ste problemy

#### **1. BÅ‚Ä…d "spaCy model not found"**

```bash
# RozwiÄ…zanie
python -m spacy download pl_core_news_sm

# Lub wiÄ™kszy model (lepszy, ale wolniejszy)
python -m spacy download pl_core_news_lg
```

#### **2. BÅ‚Ä…d "tiktoken not found"**

```bash
# RozwiÄ…zanie
pip install tiktoken
```

#### **3. Problemy z pamiÄ™ciÄ… podczas analizy spaCy**

```bash
# Zmniejsz batch size
fs-cli scrape --batch-size 25 --analysis spacy_full

# Lub uÅ¼yj mniejszego modelu
fs-cli scrape --spacy-model pl_core_news_sm
```

#### **4. Zbyt wolne scrapowanie**

```bash
# ZwiÄ™ksz rÃ³wnolegÅ‚oÅ›Ä‡ (ostroÅ¼nie!)
fs-cli scrape --concurrent 32 --delay 0.2

# WyÅ‚Ä…cz analizy dla szybszego scrapowania
fs-cli scrape --analysis none
```

#### **5. BÅ‚Ä™dy poÅ‚Ä…czenia sieciowego**

```bash
# ZwiÄ™ksz opÃ³Åºnienia
fs-cli scrape --delay 2.0 --concurrent 8

# SprawdÅº poÅ‚Ä…czenie internetowe
ping google.com
```

### Debugowanie

#### **WÅ‚Ä…cz szczegÃ³Å‚owe logi**

```bash
fs-cli scrape --verbose --forum radio_katolik
```

#### **Testuj bez scrapowania**

```bash
fs-cli scrape --dry-run --analysis all
```

#### **SprawdÅº status baz danych**

```bash
fs-cli status
```

### Monitorowanie wydajnoÅ›ci

#### **SprawdÅº wykorzystanie zasobÃ³w**

```bash
# Podczas scrapowania w drugim terminalu
top -p $(pgrep -f fs-cli)
```

#### **Monitoruj rozmiar baz danych**

```bash
watch -n 5 'ls -lh data/databases/*.db'
```

## ğŸ—ï¸ Architektura systemu

### Struktura projektu

```
forums_scraper/
â”œâ”€â”€ ğŸ“„ README.md                    # Kompletna dokumentacja (ten plik)
â”œâ”€â”€ âš™ï¸  pyproject.toml               # Konfiguracja pakietu Python
â”œâ”€â”€ ğŸ“‚ examples/                    # PrzykÅ‚ady konfiguracji
â”‚   â””â”€â”€ forums_scraper.yaml
â”œâ”€â”€ ğŸ“‚ data/                        # Bazy danych i wyniki
â”‚   â””â”€â”€ databases/                  # SQLite bazy danych
â”œâ”€â”€ ğŸ“‚ forums_scraper/              # GÅ‚Ã³wny pakiet Python
â”‚   â”œâ”€â”€ analyzers_basic/            # ğŸ”¬ Analizatory NLP
â”‚   â”‚   â”œâ”€â”€ linguistic.py           #   â”œâ”€â”€ SpacyAnalyzer
â”‚   â”‚   â””â”€â”€ tokenizer.py            #   â””â”€â”€ TokenCountAnalyzer
â”‚   â”œâ”€â”€ fs_cli/                     # ğŸ›ï¸ Interfejs CLI
â”‚   â”‚   â”œâ”€â”€ advanced.py             #   â””â”€â”€ Rich + Typer UI
â”‚   â”‚   â””â”€â”€ main.py                 #
â”‚   â”œâ”€â”€ fs_core/                    # âš™ï¸ RdzeÅ„ systemu
â”‚   â”‚   â”œâ”€â”€ config.py               #   â”œâ”€â”€ Konfiguracja YAML
â”‚   â”‚   â”œâ”€â”€ protocol.py             #   â”œâ”€â”€ Interfejsy
â”‚   â”‚   â”œâ”€â”€ registry.py             #   â”œâ”€â”€ Entry points
â”‚   â”‚   â””â”€â”€ runner.py               #   â””â”€â”€ Async runner
â”‚   â””â”€â”€ scraper/                    # ğŸ•·ï¸ Silnik Scrapy
â”‚       â”œâ”€â”€ items.py                #   â”œâ”€â”€ Modele danych
â”‚       â”œâ”€â”€ middlewares.py          #   â”œâ”€â”€ Middleware
â”‚       â”œâ”€â”€ pipelines/              #   â”œâ”€â”€ Pipeline'y
â”‚       â”‚   â”œâ”€â”€ analysis.py         #   â”‚   â”œâ”€â”€ Analiza
â”‚       â”‚   â””â”€â”€ database.py         #   â”‚   â””â”€â”€ Baza danych
â”‚       â”œâ”€â”€ settings.py             #   â”œâ”€â”€ Ustawienia
â”‚       â”œâ”€â”€ spiders/                #   â””â”€â”€ Spidery forÃ³w
â”‚       â””â”€â”€ utils.py
```

### PrzepÅ‚yw danych

```mermaid
graph TD
    A[fs-cli scrape] --> B[Scrapy Engine]
    B --> C[Spider]
    C --> D[Web Scraping]
    D --> E[Items]
    E --> F[Analysis Pipeline]
    F --> G[NLP Analyzers]
    G --> H[Database Pipeline]
    H --> I[SQLite Database]

    J[Config YAML] --> F
    K[Entry Points] --> G
```

### Wzorce projektowe

- **Plugin Architecture** - Entry points dla analizatorÃ³w
- **Pipeline Pattern** - Scrapy pipelines dla przetwarzania
- **Strategy Pattern** - RÃ³Å¼ne typy analiz
- **Observer Pattern** - Progress reporting
- **Factory Pattern** - Tworzenie analizatorÃ³w

## ğŸš€ RozwÃ³j i wkÅ‚ad

### Dodawanie nowych analizatorÃ³w

#### **1. Implementacja analizatora**

```python
# my_analyzer.py
from typing import Any, Dict
from forums_scraper.fs_core.protocol import Analyzer

class MyCustomAnalyzer(Analyzer):
    def __init__(self, **config):
        self.config = config

    async def setup(self):
        # Inicjalizacja (Å‚adowanie modeli, etc.)
        pass

    async def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        content = data.get('content', '')

        # Twoja analiza tutaj
        result = self.my_analysis_function(content)

        return {
            'my_analysis': result
        }

    async def close(self):
        # SprzÄ…tanie zasobÃ³w
        pass
```

#### **2. Rejestracja w pyproject.toml**

```toml
[project.entry-points."forums_scraper.analyzers"]
my_analyzer = "my_package.my_analyzer:MyCustomAnalyzer"
```

#### **3. UÅ¼ycie w konfiguracji**

```yaml
analysis:
  enabled: true
  analyzers:
    - name: my_analyzer
      config:
        parameter1: value1
        parameter2: value2
```

### Dodawanie nowych forÃ³w

#### **1. Implementacja spidera**

```python
# new_forum_spider.py
import scrapy
from forums_scraper.scraper.items import ForumPostItem

class NewForumSpider(scrapy.Spider):
    name = 'new_forum'
    allowed_domains = ['newforum.com']
    start_urls = ['https://newforum.com']

    def parse(self, response):
        # Implementacja scrapowania
        pass
```

#### **2. Aktualizacja CLI**

```python
# W fs_cli/advanced.py
class ForumName(str, Enum):
    # ... istniejÄ…ce fora
    NEW_FORUM = "new_forum"

FORUM_SPIDER_MAP = {
    # ... istniejÄ…ce mapowania
    ForumName.NEW_FORUM: "new_forum",
}
```

### Testowanie

```bash
# Testy jednostkowe
python -m pytest tests/

# Testy integracyjne
python -m pytest tests/integration/

# Testy analizatorÃ³w
python -m pytest tests/analyzers/
```

### Code style

```bash
# Formatowanie kodu
black forums_scraper/
isort forums_scraper/

# Linting
flake8 forums_scraper/
mypy forums_scraper/
```

## ğŸ“š Zasoby dodatkowe

### Dokumentacja techniczna

- **README.md** (ten plik) - Kompletna dokumentacja
- [API Documentation](docs/api/) - Dokumentacja API (w przygotowaniu)
- [Database Schema](docs/database.md) - Schemat bazy danych (w przygotowaniu)

### PrzykÅ‚ady uÅ¼ycia

- [Jupyter Notebooks](examples/notebooks/) - Analizy przykÅ‚adowe
- [SQL Queries](examples/sql/) - Gotowe zapytania
- [Configuration Files](examples/configs/) - PrzykÅ‚adowe konfiguracje

### SpoÅ‚ecznoÅ›Ä‡

- **Issues** - [GitHub Issues](https://github.com/username/forums_scraper/issues)
- **Discussions** - [GitHub Discussions](https://github.com/username/forums_scraper/discussions)
- **Wiki** - [Project Wiki](https://github.com/username/forums_scraper/wiki)

## ğŸ“„ Licencja

MIT License - zobacz [LICENSE](LICENSE) dla szczegÃ³Å‚Ã³w.

## ğŸ™ PodziÄ™kowania

- **Scrapy Team** - za doskonaÅ‚y framework scrapowania
- **spaCy Team** - za zaawansowane narzÄ™dzia NLP
- **Rich Team** - za piÄ™kny interfejs CLI
- **SpoÅ‚ecznoÅ›Ä‡ Open Source** - za inspiracjÄ™ i wsparcie

---

**Autor:** alb  
**Wersja:** 0.1.0  
**Python:** 3.10+  
**Licencja:** MIT

_Forums Scraper - Profesjonalne narzÄ™dzie do analizy dyskursu religijnego w internecie_ ğŸ•·ï¸âœ¨
