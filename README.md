# Forums Scraper

Aplikacja do scrapowania for√≥w religijnych zbudowana w Scrapy z zaawansowanym modu≈Çem analizy token√≥w.

## Struktura projektu

```
forums_scraper/
‚îú‚îÄ‚îÄ README.md                    # Ten plik
‚îú‚îÄ‚îÄ pyproject.toml              # Konfiguracja projektu Python
‚îú‚îÄ‚îÄ uv.lock                     # Lock file dla uv
‚îú‚îÄ‚îÄ scrapy.cfg                  # Konfiguracja Scrapy
‚îú‚îÄ‚îÄ scraper/                    # G≈Ç√≥wny modu≈Ç Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ items.py               # Definicje element√≥w danych
‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py         # Middleware Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py           # Pipeline do przetwarzania danych
‚îÇ   ‚îú‚îÄ‚îÄ settings.py            # Ustawienia Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ utils.py               # Funkcje pomocnicze
‚îÇ   ‚îî‚îÄ‚îÄ spiders/               # PajƒÖki Scrapy
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ dolina_modlitwy.py
‚îÇ       ‚îú‚îÄ‚îÄ radio_katolik.py
‚îÇ       ‚îú‚îÄ‚îÄ wiara.py
‚îÇ       ‚îî‚îÄ‚îÄ z_chrystusem.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ databases/              # Bazy danych SQLite (per-spider, merged, analysis)
‚îÇ   ‚îú‚îÄ‚îÄ logs/                   # Logi (scrapy, analysis)
‚îÇ   ‚îî‚îÄ‚îÄ topics/                 # Artefakty Top2Vec: models/, results/, logs/
‚îú‚îÄ‚îÄ logs/                       # Logi aplikacji
‚îú‚îÄ‚îÄ scripts/                    # Skrypty pomocnicze
‚îÇ   ‚îú‚îÄ‚îÄ run_all_scrapers.py    # Uruchamianie wszystkich scraper√≥w
‚îÇ   ‚îú‚îÄ‚îÄ merge_databases.py     # ≈ÅƒÖczenie baz danych
‚îÇ   ‚îú‚îÄ‚îÄ test_scraper.py        # Testowanie pojedynczego scrapera
‚îÇ   ‚îî‚îÄ‚îÄ run_analysis_with_spiders.py  # Uruchamianie analizy r√≥wnolegle ze spiderami
‚îú‚îÄ‚îÄ analysis/                   # Modu≈Ç analizy token√≥w
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            # Inicjalizacja modu≈Çu
‚îÇ   ‚îú‚îÄ‚îÄ README.md              # G≈Ç√≥wna dokumentacja modu≈Çu
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Konfiguracja
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                 # Interfejs wiersza polece≈Ñ
‚îÇ   ‚îú‚îÄ‚îÄ run_analysis_daemon.py # Skrypt daemon
‚îÇ   ‚îú‚îÄ‚îÄ test_analysis.py       # Testy modu≈Çu
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt        # Zale≈ºno≈õci
‚îÇ   ‚îî‚îÄ‚îÄ tokenization/          # Modu≈Ç tokenizacji
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py        # Inicjalizacja tokenizacji
‚îÇ       ‚îú‚îÄ‚îÄ token_analyzer.py  # G≈Ç√≥wny analizator token√≥w
‚îÇ       ‚îú‚îÄ‚îÄ install_spacy_model.py # Instalacja spaCy
‚îÇ       ‚îú‚îÄ‚îÄ compare_tokenizers.py  # Por√≥wnanie algorytm√≥w
‚îÇ       ‚îî‚îÄ‚îÄ README.md          # Dokumentacja tokenizacji
‚îî‚îÄ‚îÄ requirements.txt            # Zale≈ºno≈õci Python
```

## Instalacja

```bash
# U≈ºywajƒÖc uv (zalecane)
uv sync

# Lub tradycyjnie
pip install -r requirements.txt
```

## Uruchomienie

### Jednolity pipeline (zalecane)

```bash
# Wykonaj pe≈Çny pipeline: scrapowanie -> analiza token√≥w -> modelowanie temat√≥w -> czyszczenie
python scripts/pipeline.py all

# Lub poszczeg√≥lne kroki
python scripts/pipeline.py scrape   # uruchamia wszystkie spidery i ≈ÇƒÖczy bazy
python scripts/pipeline.py analyze  # tworzy analysis_forums.db i liczy tokeny
python scripts/pipeline.py topics   # modelowanie tematyczne (Top2Vec)
python scripts/pipeline.py clean    # czy≈õci po≈õrednie bazy wg configu

# Z w≈ÇasnƒÖ konfiguracjƒÖ
python scripts/pipeline.py all --config scripts/pipeline.config.json
```

### Generator taksonomii i promptu (Excel + LLM)

Modu≈Ç generuje 3‚ÄëpoziomowƒÖ taksonomiƒô i finalny prompt systemowy wy≈ÇƒÖcznie na podstawie pliku Excel z postami. Pipeline dzia≈Ça wieloetapowo: streszczenia + cytat, propozycje g≈Ç√≥wnych kategorii, konsolidacja, indukcja poziom√≥w 2/3, numeracja, przypisania post√≥w, eksport artefakt√≥w, wygenerowanie docelowego `system_message` (PL).

#### Wymagania (OpenRouter + DeepSeek R1 0528)

- Zale≈ºno≈õƒá: `openai` (u≈ºywamy klienta z `base_url`)
- Konfiguracja LLM w `analysis/config.py` (`LLM_CONFIG`) jest domy≈õlnie ustawiona na OpenRouter i model `deepseek/deepseek-r1-0528:free`.
- Ustaw klucz ≈õrodowiskowy:

```bash
export OPENROUTER_API_KEY="sk-or-..."
# opcjonalnie (dla statystyk w OpenRouter):
export OPENROUTER_HTTP_REFERER="https://twoja-aplikacja.example"
export OPENROUTER_APP_TITLE="Forums Scraper"
```

Mo≈ºesz nadpisaƒá endpoint/model przez zmienne:

```bash
export OPENROUTER_BASE_URL="https://openrouter.ai/api/v1"
export LLM_MODEL="deepseek/deepseek-r1-0528:free"
export LLM_TEMPERATURE=0.7
export LLM_MAX_TOKENS=1000
```

≈πr√≥d≈Ço modelu i przyk≈Çady u≈ºycia w API: [DeepSeek: R1 0528 (free) ‚Äî OpenRouter](https://openrouter.ai/deepseek/deepseek-r1-0528:free)

#### Dane wej≈õciowe (Excel)

Minimalne kolumny:

- `post_id` (je≈õli brak, zostanie nadany sekwencyjnie)
- `content` (tre≈õƒá posta)

Domy≈õlnie wykorzystujemy przyk≈Çad:

```
data/topics/results/20250821/M/ALL/185832/examples/topic_2_pi_pis_sld.xlsx
```

#### Uruchomienie

```bash
# Domy≈õlne ≈õcie≈ºki i temat: polityka
python scripts/pipeline.py taxonomy --taxonomy-theme polityka

# Parametry opcjonalne
python scripts/pipeline.py taxonomy \
  --taxonomy-excel /pe≈Çna/≈õcie≈ºka.xlsx \
  --taxonomy-output /Users/alb/repos/forums_scraper/data/topics \
  --taxonomy-theme polityka \
  --taxonomy-batch 50 \
  --taxonomy-max-posts 500
```

#### Co powstaje (artefakty)

Artefakty zapisujƒÖ siƒô do katalogu:

```
data/topics/taxonomies/<YYYYMMDD>/<theme_slug>_<HHMMSS>/
```

WewnƒÖtrz znajdziesz:

- `taxonomy.json` ‚Äì znumerowana taksonomia 3‚Äëpoziomowa (`1`, `1.1`, `1.1.1`)
- `prompt_system_message.md` ‚Äì gotowy system_message (PL) z Twoimi zasadami i pe≈ÇnƒÖ listƒÖ kategorii
- `assignments.jsonl` ‚Äì przypisania post√≥w: `post_id`, `path` (np. `3.2.5`), `confidence`
- `labeled_<oryginalny_plik>.xlsx` ‚Äì Excel z dodatkowymi kolumnami: `summary`, `quote`, `path`, `confidence`

#### Jak to dzia≈Ça (skr√≥t procesu)

1. Streszczenia: dla ka≈ºdego posta powstaje kr√≥tkie podsumowanie (1‚Äì2 zdania) i 1 cytat
2. Propozycje g≈Ç√≥wnych kategorii per post (1‚Äì3 kandydatury)
3. Konsolidacja kategorii g≈Ç√≥wnych (8‚Äì15 sp√≥jnych nazw + kr√≥tkie opisy)
4. Indukcja podkategorii (poziom 2) i pod‚Äëpodkategorii (poziom 3)
5. Numeracja hierarchii (1, 1.1, 1.1.1)
6. Przypisania post√≥w do pe≈Çnych ≈õcie≈ºek (`X.Y.Z`) z `confidence`
7. Eksport artefakt√≥w i wygenerowanie `system_message`

#### Dobre praktyki

- Zadbaj o sensowny dob√≥r tematu (`--taxonomy-theme`) ‚Äì wp≈Çywa na jako≈õƒá nazw
- Batch (`--taxonomy-batch`) dostosuj do limit√≥w token√≥w ‚Äì zwykle 50‚Äì100 dzia≈Ça dobrze
- Je≈ºeli `confidence` czƒôsto < 0.6 lub du≈ºo ‚ÄûInne‚Äù, uruchom proces na wiƒôkszym zbiorze lub zawƒô≈∫ temat

#### RozwiƒÖzywanie problem√≥w

- B≈ÇƒÖd klucza: sprawd≈∫ `LLM_CONFIG['api_key']` w `analysis/config.py`
- Brak kolumny `content`: dostosuj nag≈Ç√≥wki w Excelu (mo≈ºesz u≈ºyƒá `text`, `post`, `message`, `body` ‚Äì zostanƒÖ zmapowane)
- Brak `openai`: zainstaluj zale≈ºno≈õƒá (`uv sync` lub `pip install -r requirements.txt`)

#### Automatyczny slug tematu

- Je≈õli podasz `--taxonomy-theme`, zostanie on zsanityzowany do slug'a (ma≈Çe litery, my≈õlniki).
- Je≈õli nie podasz, slug zostanie automatycznie wyciƒÖgniƒôty z nazwy pliku Excela (zsanityzowany).

Konfiguracja domy≈õlna jest wbudowana w `scripts/pipeline.py`. Mo≈ºesz jƒÖ nadpisaƒá przez `scripts/pipeline.config.json` lub zmienne ≈õrodowiskowe dla modelowania tematycznego. Artefakty trzymamy w `data/`:

```bash
export TOPIC_DATABASE_PATH=databases/analysis_forums.db
export TOPIC_OUTPUT_DIR=data/topics
export TOPIC_FORUMS=wiara
export TOPIC_GENDERS=M,K
```

### Pojedynczy scraper (alternatywnie)

```bash
cd scraper
scrapy crawl nazwa_spidera
```

### Mini test E2E: jedna strona konkretnego wƒÖtku (szybko)

Dla szybkiej weryfikacji end-to-end dodano tryb `only_thread_url` w spiderach: `wiara`, `radio_katolik`, `dolina_modlitwy`, `z_chrystusem`.

1. Pobierz jednƒÖ stronƒô wƒÖtku i zapisz do lokalnej bazy danego spidera:

```bash
python -m scrapy crawl wiara -s SETTINGS_MODULE=scraper.settings \
  -a only_thread_url='https://forum.wiara.pl/viewtopic.php?f=37&t=42547' \
  -s CLOSESPIDER_PAGECOUNT=1 -s LOG_LEVEL=INFO
```

Analogicznie dla pozosta≈Çych for√≥w (podaj URL `viewtopic.php?...` z danego forum). Poni≈ºej przyk≈Çady i wskaz√≥wki, jak znale≈∫ƒá linki:

#### Radio Katolik

- Jak znale≈∫ƒá: otw√≥rz `https://dyskusje.radiokatolik.pl/`, wejd≈∫ do dowolnej sekcji, skopiuj adres wƒÖtku (zawiera `viewtopic.php?...`).
- Przyk≈Çad (zamie≈Ñ na realny URL wƒÖtku):

```bash
python -m scrapy crawl radio_katolik -s SETTINGS_MODULE=scraper.settings \
  -a only_thread_url='https://dyskusje.radiokatolik.pl/viewtopic.php?f=1&t=1' \
  -s CLOSESPIDER_PAGECOUNT=1 -s LOG_LEVEL=INFO
```

#### Dolina Modlitwy

- Jak znale≈∫ƒá: otw√≥rz `https://dolinamodlitwy.pl/forum/`, wejd≈∫ do sekcji, skopiuj link wƒÖtku `viewtopic.php?...`.
- Przyk≈Çad (zamie≈Ñ na realny URL wƒÖtku):

```bash
python -m scrapy crawl dolina_modlitwy -s SETTINGS_MODULE=scraper.settings \
  -a only_thread_url='https://dolinamodlitwy.pl/forum/viewtopic.php?f=2&t=4' \
  -s CLOSESPIDER_PAGECOUNT=1 -s LOG_LEVEL=INFO
```

#### Z Chrystusem

- Jak znale≈∫ƒá: otw√≥rz `https://zchrystusem.pl/`, wejd≈∫ do sekcji, skopiuj link wƒÖtku `viewtopic.php?...`.
- Przyk≈Çad (zamie≈Ñ na realny URL wƒÖtku):

```bash
python -m scrapy crawl z_chrystusem -s SETTINGS_MODULE=scraper.settings \
  -a only_thread_url='https://zchrystusem.pl/viewtopic.php?f=2&t=4' \
  -s CLOSESPIDER_PAGECOUNT=1 -s LOG_LEVEL=INFO
```

2. Zmerguj bazy do jednej (na potrzeby analizy):

```bash
python scripts/merge_all_databases.py --target data/databases/merged_forums.db --no-offset
```

3. Uruchom szybkƒÖ analizƒô na ma≈Çej pr√≥bce i poka≈º podsumowanie:

```bash
python analysis/cli.py --source-db data/databases/merged_forums.db \
  --analysis-db data/databases/analysis_forums.db \
  --create-db --batch 50 --summary
```

### Mened≈ºer scraper√≥w (alternatywnie)

```bash
# Z poziomu g≈Ç√≥wnego katalogu
python run_scrapers.py

# Lub bezpo≈õrednio
python scripts/run_all_scrapers_final.py
```

## Dostƒôpne pajƒÖki

- `dolina_modlitwy` - Forum Dolina Modlitwy
- `radio_katolik` - Forum Radio Katolik
- `wiara` - Forum Wiara
- `z_chrystusem` - Forum Z Chrystusem

## Konfiguracja

G≈Ç√≥wne ustawienia znajdujƒÖ siƒô w `scraper/settings.py`. Mo≈ºna tam dostosowaƒá:

- Op√≥≈∫nienia miƒôdzy requestami
- Liczbƒô r√≥wnoczesnych request√≥w
- Ustawienia bazy danych
- Poziom logowania

## Bazy danych

Ka≈ºdy scraper tworzy osobnƒÖ bazƒô SQLite w katalogu `data/databases/`. Format nazwy: `forum_[nazwa_spidera].db`

### Po≈ÇƒÖczona baza danych

Skrypt `scripts/merge_all_databases.py` ≈ÇƒÖczy wszystkie bazy w jednƒÖ `merged_forums.db` w `data/databases/`.

### Baza analizy

Modu≈Ç analizy tworzy kopiƒô bazy danych (`analysis_forums.db`) w `data/databases/` z dodatkowymi tabelami:

- `token_analysis` - wyniki analizy token√≥w dla ka≈ºdego posta
- `analysis_stats` - statystyki dzienne analizy

## Tokenizacja spaCy (uv + CLI)

Poni≈ºsze komendy uruchomisz z katalogu g≈Ç√≥wnego. WykorzystujƒÖ CLI `token-analysis` dodane w `pyproject.toml` i dzia≈ÇajƒÖ w ≈õrodowisku `uv`.

### 1) Po≈ÇƒÖcz bazy poszczeg√≥lnych for√≥w do jednej

```bash
uv run merge-databases --target data/databases/merged_forums.db
```

### 2) Utw√≥rz bazƒô do analizy (kopiƒô ≈∫r√≥d≈Çowej bazy)

```bash
uv run --with spacy --with pl-core-news-sm \
  token-analysis \
  --source-db data/databases/merged_forums.db \
  --analysis-db data/databases/analysis_forums.db \
  --forums radio_katolik,dolina_modlitwy \
  --create-db
```

### 3) Uruchom analizƒô token√≥w (spaCy)

```bash
uv run --with spacy --with pl-core-news-sm \
  token-analysis \
  --source-db data/databases/merged_forums.db \
  --analysis-db data/databases/analysis_forums.db \
  --forums radio_katolik,dolina_modlitwy \
  --all
```

### 4) Podsumowanie wynik√≥w

```bash
uv run --with spacy --with pl-core-news-sm \
  token-analysis \
  --source-db data/databases/merged_forums.db \
  --analysis-db data/databases/analysis_forums.db \
  --forums radio_katolik,dolina_modlitwy \
  --summary
```

### Opcja: instalacja spaCy i modelu na sta≈Çe

Zamiast `--with ...` mo≈ºesz dodaƒá zale≈ºno≈õci trwale i pobraƒá model:

```bash
uv add spacy
uv run python -m spacy download pl_core_news_sm
```

Po tej instalacji `--with spacy --with pl-core-news-sm` nie jest ju≈º potrzebne.

## Logi

Logi sƒÖ zapisywane w katalogu `data/logs/` z timestampami i poziomami logowania.

### Logi analizy

- `data/logs/token_analysis.log` - G≈Ç√≥wne logi analizy token√≥w
- `data/logs/analysis_daemon.log` - Logi daemon analizy

## Modu≈Ç analizy token√≥w

Modu≈Ç analizy zapewnia bezpieczne przeliczanie token√≥w w postach forum bez zak≈Ç√≥cania pracy spider√≥w:

### üéØ Funkcjonalno≈õci

- **Bezpieczna analiza**: Dzia≈Ça na kopii bazy danych
- **Przeliczanie token√≥w**: Automatyczne obliczanie liczby token√≥w, s≈Ç√≥w i znak√≥w
- **CiƒÖg≈Çe monitorowanie**: Mo≈ºe dzia≈Çaƒá w tle jako daemon
- **Inteligentne przetwarzanie**: Analizuje tylko nowe lub zmienione posty
- **Statystyki**: Szczeg√≥≈Çowe raporty i metryki analizy

### üöÄ Szybki start

```bash
# Z pipeline (zalecane)
python scripts/pipeline.py analyze

# Bezpo≈õrednio z CLI analizy
python analysis/cli.py --create-db --all --summary
```

### üìä Metryki

- **Dok≈Çadna tokenizacja**: U≈ºywa spaCy z modelem polskim dla precyzyjnego liczenia token√≥w
- **Fallback**: Automatyczne prze≈ÇƒÖczanie na prosty algorytm je≈õli spaCy nie dzia≈Ça
- **Liczba token√≥w, s≈Ç√≥w i znak√≥w** w ka≈ºdym po≈õcie
- **Postƒôp analizy** (procent przeanalizowanych post√≥w)
- **Statystyki dzienne** i wydajno≈õciowe
- **Hash tre≈õci** dla ≈õledzenia zmian
- **Statystyki tokenizacji**: Procent u≈ºycia spaCy vs prostego algorytmu

Wszystkie artefakty trzymamy w `data/`. Katalog `analysis/topic_modeling` zawiera tylko kod (bez artefakt√≥w).
Wiƒôcej informacji w [dokumentacji modu≈Çu analizy](analysis/README.md).
