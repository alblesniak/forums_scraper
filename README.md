# üï∑Ô∏è Forums Scraper

**Zaawansowany scraper for√≥w religijnych z r√≥wnoleg≈Çymi analizami NLP i bazami danych SQLite**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![uv](https://img.shields.io/badge/uv-enabled-red.svg)](https://docs.astral.sh/uv/)
[![Scrapy](https://img.shields.io/badge/scrapy-2.11+-green.svg)](https://scrapy.org/)
[![spaCy](https://img.shields.io/badge/spaCy-3.4+-orange.svg)](https://spacy.io/)
[![Rich CLI](https://img.shields.io/badge/CLI-Rich%20%2B%20Typer-purple.svg)](https://rich.readthedocs.io/)

## üéØ Opis projektu

Forums Scraper to profesjonalne narzƒôdzie do scrapowania for√≥w religijnych z zaawansowanymi funkcjami analizy tekstu. System zosta≈Ç zaprojektowany z my≈õlƒÖ o badaniach lingwistycznych, analizie sentymentu i modelowaniu temat√≥w w kontek≈õcie dyskusji religijnych.

### üéØ G≈Ç√≥wne cele

- **Badania naukowe**: Analiza dyskursu religijnego w internecie
- **Analiza sentymentu**: Badanie nastroj√≥w w spo≈Çeczno≈õciach religijnych
- **Modelowanie temat√≥w**: Identyfikacja g≈Ç√≥wnych temat√≥w dyskusji
- **Archiwizacja**: D≈Çugoterminowe przechowywanie tre≈õci for√≥w

## ‚ú® Kluczowe funkcjonalno≈õci

### üîÑ **R√≥wnoleg≈Çe analizy NLP**

- **Tokenizacja podstawowa** - szybka analiza bez zale≈ºno≈õci zewnƒôtrznych
- **Liczenie token√≥w OpenAI** - estymacja koszt√≥w API dla modeli jƒôzykowych
- **Pe≈Çna analiza spaCy** - lematyzacja, POS tagging, dependency parsing
- **Analiza sentymentu** - wykrywanie emocji w tek≈õcie
- **Statystyki jƒôzykowe** - czytelno≈õƒá, d≈Çugo≈õƒá zda≈Ñ, bogactwo s≈Çownictwa

### üóÑÔ∏è **Zaawansowana baza danych**

- **Osobne bazy SQLite** dla ka≈ºdego forum
- **Pe≈Çny schemat relacyjny** - fora, sekcje, wƒÖtki, u≈ºytkownicy, posty
- **Tabele analiz** - tokeny, statystyki jƒôzykowe, analiza morfosyntaktyczna, URL-e i domeny
- **Indeksy wydajno≈õciowe** - szybkie zapytania analityczne
- **Automatyczne backupy** - bezpieczne przechowywanie danych

### üéõÔ∏è **Profesjonalny CLI**

- **Rich interface** - kolorowy, interaktywny interfejs
- **Progress tracking** - ≈õledzenie postƒôpu w czasie rzeczywistym
- **Wyb√≥r for√≥w** - elastyczna selekcja ≈∫r√≥de≈Ç danych
- **Konfiguracja analiz** - dostosowanie do potrzeb badawczych
- **Dry-run mode** - testowanie bez wykonywania operacji
- **Status monitoring** - przeglƒÖd stanu baz danych

### ‚öôÔ∏è **Elastyczna architektura**

- **Plugin system** - ≈Çatwe dodawanie nowych analizator√≥w
- **Entry points** - automatyczne wykrywanie dostƒôpnych analiz
- **YAML/CLI configuration** - wygodna konfiguracja
- **Asynchronous processing** - wydajne przetwarzanie r√≥wnoleg≈Çe
- **Error handling** - odporna na b≈Çƒôdy architektura

## üöÄ Instalacja i pierwsze uruchomienie

### Wymagania systemowe

- **Python 3.10+** (sprawd≈∫: `python --version`)
- **uv** - nowoczesny mened≈ºer pakiet√≥w Python ([instalacja](https://docs.astral.sh/uv/getting-started/installation/))
- **4GB RAM** (minimum), **8GB RAM** (zalecane dla spaCy)
- **Po≈ÇƒÖczenie internetowe** dla scrapowania
- **~500MB** miejsca na dysku (zale≈ºnie od liczby for√≥w)

### Krok 1: Instalacja uv (je≈õli nie masz)

```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows (PowerShell)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Sprawd≈∫ instalacjƒô
uv --version
```

### Krok 2: Pobranie kodu

```bash
# Klonowanie repozytorium
git clone https://github.com/username/forums_scraper.git
cd forums_scraper
```

### Krok 3: Instalacja z uv

#### **Opcja A: Instalacja podstawowa** (tylko scrapowanie)

```bash
uv pip install -e .
```

#### **Opcja B: Z analizatorami podstawowymi** (+ tiktoken)

```bash
uv pip install -e ".[analyzers-basic]"
```

#### **Opcja C: Z pe≈Çnymi analizatorami** (+ spaCy)

```bash
uv pip install -e ".[analyzers-linguistic]"
uv run python -m spacy download pl_core_news_lg
```

#### **Opcja D: Pe≈Çna instalacja** (zalecane)

```bash
uv pip install -e ".[all]"
uv run python -m spacy download pl_core_news_lg
```

#### **Opcja E: Z wirtualnym ≈õrodowiskiem** (najbezpieczniejsze)

```bash
# Utw√≥rz i aktywuj venv
uv venv
source .venv/bin/activate  # Linux/macOS
# lub .venv\Scripts\activate  # Windows

# Zainstaluj z pe≈Çnymi funkcjami
uv pip install -e ".[all]"
uv run python -m spacy download pl_core_news_lg
```

### Krok 4: Weryfikacja

```bash
# Sprawd≈∫ czy CLI dzia≈Ça
uv run fs-cli --help

# Lista dostƒôpnych for√≥w
uv run fs-cli list-spiders

# Lista dostƒôpnych analizator√≥w
uv run fs-cli list-analyzers

# Test bez scrapowania
uv run fs-cli scrape --forum radio_katolik --dry-run
```

### Krok 5: Pierwsze uruchomienie

```bash
# Scrapuj jedno forum z podstawowƒÖ analizƒÖ
uv run fs-cli scrape --forum radio_katolik --analysis basic_tokens

# Sprawd≈∫ wyniki
uv run fs-cli status
```

### RozwiƒÖzywanie problem√≥w instalacji

#### **B≈ÇƒÖd "uv: command not found"**

```bash
# Zainstaluj uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc  # lub ~/.zshrc

# Sprawd≈∫ instalacjƒô
uv --version
```

#### **B≈ÇƒÖd "fs-cli: command not found"**

```bash
# U≈ºyj uv run zamiast bezpo≈õredniego wywo≈Çania
uv run fs-cli --help

# Je≈õli u≈ºywasz venv, upewnij siƒô ≈ºe jest aktywowany
source .venv/bin/activate
uv run fs-cli --help
```

#### **B≈ÇƒÖd "spaCy model not found"**

```bash
uv run python -m spacy download pl_core_news_lg

# Lub wiƒôkszy model (lepszy, ale wolniejszy)
uv run python -m spacy download pl_core_news_lg
```

#### **B≈ÇƒÖd "tiktoken not found"**

```bash
uv uv pip install tiktoken
```

#### **Problemy z pamiƒôciƒÖ**

```bash
# Zmniejsz batch size dla analiz
uv run fs-cli scrape --batch-size 25
```

#### **Problemy z wirtualnym ≈õrodowiskiem**

```bash
# Usu≈Ñ i utw√≥rz ponownie venv
rm -rf .venv
uv venv
source .venv/bin/activate
uv pip install -e ".[all]"
```

### Dlaczego uv?

- **‚ö° Szybko≈õƒá** - do 100x szybsza instalacja pakiet√≥w ni≈º pip
- **üîí Bezpiecze≈Ñstwo** - automatyczna weryfikacja integralno≈õci pakiet√≥w
- **üéØ Prostota** - jednolity interfejs dla wszystkich operacji Python
- **üåç Kompatybilno≈õƒá** - pe≈Çna zgodno≈õƒá z pip i PyPI
- **üíæ Efektywno≈õƒá** - inteligentne cache'owanie i deduplikacja

## üéÆ Przewodnik u≈ºytkownika

### Podstawowe u≈ºycie

#### 1. **Scrapowanie wszystkich for√≥w** (zalecane dla poczƒÖtkujƒÖcych)

```bash
uv run fs-cli scrape
```

- Scrapuje wszystkie 4 fora
- U≈ºywa podstawowej tokenizacji
- Zapisuje do wsp√≥lnej bazy: `data/databases/forums_unified.db`

#### 2. **Scrapowanie konkretnego forum**

```bash
uv run fs-cli scrape --forum radio_katolik
```

#### 3. **Scrapowanie z analizƒÖ spaCy**

```bash
uv run fs-cli scrape --forum wiara --analysis spacy_full --sentiment
```

### Zaawansowane opcje

#### **Wyb√≥r wielu for√≥w i analiz**

```bash
uv run fs-cli scrape \
  --forum wiara \
  --forum dolina_modlitwy \
  --analysis basic_tokens \
  --analysis spacy_full \
  --sentiment
```

#### **Optymalizacja wydajno≈õci**

```bash
uv run fs-cli scrape \
  --concurrent 32 \
  --delay 0.1 \
  --batch-size 200 \
  --forum radio_katolik
```

#### **Tryb testowy (bez scrapowania)**

```bash
uv run fs-cli scrape --forum wiara --analysis all --dry-run
```

### ZarzƒÖdzanie danymi

#### **Status baz danych**

```bash
uv run fs-cli status
```

Wy≈õwietla:

- Rozmiary plik√≥w baz danych
- Liczba post√≥w w ka≈ºdej bazie
- Daty ostatniej modyfikacji

#### **Tworzenie konfiguracji**

```bash
# Utw√≥rz plik konfiguracyjny
uv run fs-cli config --analysis spacy_full --sentiment --output my_config.yaml

# U≈ºyj w≈Çasnej konfiguracji
uv run fs-cli scrape --config my_config.yaml
```

## üìã Dostƒôpne fora

| Forum               | Kod               | Opis                                   | Szacowana wielko≈õƒá |
| ------------------- | ----------------- | -------------------------------------- | ------------------ |
| **Dolina Modlitwy** | `dolina_modlitwy` | Forum katolickie, modlitwy i duchowo≈õƒá | ~50MB              |
| **Radio Katolik**   | `radio_katolik`   | Forum Radia Katolik, aktualno≈õci       | ~30MB              |
| **Wiara.pl**        | `wiara`           | Najwiƒôksze polskie forum katolickie    | ~100MB             |
| **Z Chrystusem**    | `z_chrystusem`    | Forum ewangelickie                     | ~40MB              |

### Wyb√≥r for√≥w

```bash
# Pojedyncze forum
--forum radio_katolik

# Wiele for√≥w
--forum wiara --forum dolina_modlitwy

# Wszystkie fora (domy≈õlne)
--forum all
```

## üî¨ Typy analiz NLP

### 1. **Podstawowa tokenizacja** (`basic_tokens`)

```yaml
- name: basic_tokenizer
  config:
    lowercase: true # Konwersja na ma≈Çe litery
    min_token_length: 2 # Minimalna d≈Çugo≈õƒá tokena
    remove_punctuation: false # Zachowanie interpunkcji
```

**Wyniki:**

- Lista token√≥w
- Liczba token√≥w (total/unique)
- ≈örednia d≈Çugo≈õƒá tokena

**Zalety:** Szybka, bez zale≈ºno≈õci zewnƒôtrznych
**Wady:** Podstawowa jako≈õƒá tokenizacji

### 2. **Liczenie token√≥w OpenAI** (`token_count`)

```yaml
- name: token_counter
  config:
    encoding: cl100k_base # Model tokenizacji OpenAI
```

**Wyniki:**

- Dok≈Çadna liczba token√≥w OpenAI
- Estymacja koszt√≥w API

**Zalety:** Precyzyjne dla modeli OpenAI
**Wady:** Wymaga biblioteki tiktoken

### 3. **Pe≈Çna analiza spaCy** (`spacy_full`)

```yaml
- name: spacy_analyzer
  config:
    model: pl_core_news_lg # Model jƒôzyka polskiego (du≈ºy, najlepszy)
    include_sentiment: true # W≈ÇƒÖcz analizƒô sentymentu
    batch_size: 100 # Rozmiar batcha
    max_length: 1000000 # Maks. d≈Çugo≈õƒá tekstu
```

**Komponenty spaCy wykorzystywane:**

- **tok2vec** - wektoryzacja token√≥w (automatycznie)
- **morphologizer** - cechy morfologiczne (przypadek, liczba, rodzaj)
- **parser** - analiza sk≈Çadniowa (dependency parsing)
- **lemmatizer** - lematyzacja (formy podstawowe)
- **tagger** - tagowanie czƒô≈õci mowy (POS)
- **senter** - segmentacja zda≈Ñ
- **ner** - rozpoznawanie nazw w≈Çasnych

**Wyniki linguistyczne:**

- **Lematyzacja** - forma podstawowa s≈Ç√≥w
- **POS tagging** - czƒô≈õci mowy (rzeczownik, czasownik, etc.)
- **Dependency parsing** - relacje sk≈Çadniowe
- **Named Entity Recognition** - osoby, organizacje, miejsca, wydarzenia
- **Analiza sentymentu** - polarno≈õƒá emocjonalna z rozszerzonym s≈Çownictwem religijnym

**Statystyki tekstowe:**

- Liczba zda≈Ñ, s≈Ç√≥w, znak√≥w
- ≈örednia d≈Çugo≈õƒá zdania
- Wska≈∫nik czytelno≈õci
- Wykrywanie jƒôzyka

**Zalety:** Najwy≈ºsza jako≈õƒá analizy
**Wady:** Wymaga spaCy i modelu jƒôzykowego

### 4. **Analiza URL-√≥w** (`url_analysis`)

```yaml
- name: url_analyzer
  config:
    include_domain_analysis: true # Kategoryzacja domen
    include_url_categorization: true # Klasyfikacja URL-√≥w
    max_urls_per_post: 50 # Limit URL-√≥w per post
```

**Funkcje:**

- **Kategoryzacja domen** - religijne, media, spo≈Çeczno≈õciowe, edukacyjne
- **Klasyfikacja URL-√≥w** - artyku≈Çy, wideo, obrazy, social media
- **Ocena wiarygodno≈õci** - trust score dla domen (0.0-1.0)
- **Statystyki** - liczba domen, typy link√≥w per post
- **Deduplikacja** - unikalne domeny w bazie

**Kategorie domen:**

- **Religijne**: catholic.pl, vatican.va, opoka.org.pl, radiomaryja.pl
- **Media**: youtube.com, tvp.pl, gazeta.pl, onet.pl
- **Spo≈Çeczno≈õciowe**: facebook.com, twitter.com, instagram.com
- **Edukacyjne**: wikipedia.org, academia.edu, scholar.google.com

**Zalety:** Szczeg√≥≈Çowa analiza link√≥w, kategorie polskie
**Wady:** Brak

### 5. **Statystyki domen** (`domain_stats`)

```yaml
- name: domain_stats
  config:
    track_popularity: true # ≈öledzenie popularno≈õci domen
```

**Funkcje:**

- Podstawowe liczenie URL-√≥w i domen
- Szybka analiza bez kategoryzacji
- Deduplikacja domen

**Zalety:** Szybka, lekka
**Wady:** Brak kategoryzacji

### 6. **Wszystkie analizy** (`all`)

W≈ÇƒÖcza wszystkie dostƒôpne analizatory: tokeny + spaCy + URL-e + domeny.

## üóÑÔ∏è Struktura bazy danych

**Wsp√≥lna baza SQLite:** `data/databases/forums_unified.db`

Wszystkie fora, posty i analizy sƒÖ przechowywane w jednej bazie danych, co umo≈ºliwia:

- **Analizy por√≥wnawcze** miƒôdzy forami
- **≈Åatwiejsze zapytania** SQL
- **Prostsze zarzƒÖdzanie** danymi
- **Efektywniejsze** przechowywanie

### Tabele g≈Ç√≥wne

#### **forums** - Informacje o forach

```sql
CREATE TABLE forums (
    id TEXT PRIMARY KEY,
    spider_name TEXT NOT NULL,
    title TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

#### **sections** - Sekcje for√≥w

```sql
CREATE TABLE sections (
    id TEXT PRIMARY KEY,
    forum_id TEXT,
    title TEXT,
    url TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (forum_id) REFERENCES forums (id)
);
```

#### **threads** - WƒÖtki dyskusji

```sql
CREATE TABLE threads (
    id TEXT PRIMARY KEY,
    section_id TEXT,
    title TEXT,
    url TEXT,
    author TEXT,
    replies INTEGER,
    views INTEGER,
    last_post_date TEXT,
    last_post_author TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (section_id) REFERENCES sections (id)
);
```

#### **users** - U≈ºytkownicy for√≥w

```sql
CREATE TABLE users (
    id TEXT PRIMARY KEY,
    username TEXT UNIQUE,
    join_date TEXT,
    posts_count INTEGER,
    religion TEXT,
    gender TEXT,
    localization TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

#### **posts** - Posty u≈ºytkownik√≥w

```sql
CREATE TABLE posts (
    id TEXT PRIMARY KEY,
    thread_id TEXT,
    user_id TEXT,
    post_number INTEGER,
    content TEXT,
    content_urls TEXT,  -- JSON z URL-ami
    post_date TEXT,
    url TEXT,
    username TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    FOREIGN KEY (thread_id) REFERENCES threads (id),
    FOREIGN KEY (user_id) REFERENCES users (id)
);
```

### Tabele analiz NLP

#### **post_tokens** - Tokeny post√≥w

```sql
CREATE TABLE post_tokens (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,
    token TEXT,
    position INTEGER,
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_token_stats** - Statystyki token√≥w

```sql
CREATE TABLE post_token_stats (
    post_id TEXT PRIMARY KEY,
    total_tokens INTEGER,
    unique_tokens INTEGER,
    avg_token_length REAL,
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_linguistic_analysis** - Analiza morfosyntaktyczna

```sql
CREATE TABLE post_linguistic_analysis (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,
    token TEXT,
    lemma TEXT,           -- Forma podstawowa
    pos TEXT,             -- Czƒô≈õƒá mowy
    tag TEXT,             -- Szczeg√≥≈Çowy tag
    dep TEXT,             -- Relacja sk≈Çadniowa
    morph_features TEXT,  -- Cechy morfologiczne (JSON: przypadek, liczba, rodzaj)
    is_alpha BOOLEAN,     -- Czy alfanumeryczny
    is_stop BOOLEAN,      -- Czy stop word
    is_punct BOOLEAN,     -- Czy interpunkcja
    sentiment_score REAL, -- Wynik sentymentu
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_linguistic_stats** - Statystyki jƒôzykowe

```sql
CREATE TABLE post_linguistic_stats (
    post_id TEXT PRIMARY KEY,
    sentence_count INTEGER,      -- Liczba zda≈Ñ
    word_count INTEGER,          -- Liczba s≈Ç√≥w
    char_count INTEGER,          -- Liczba znak√≥w
    avg_sentence_length REAL,    -- ≈örednia d≈Çugo≈õƒá zdania
    readability_score REAL,      -- Wska≈∫nik czytelno≈õci
    sentiment_polarity REAL,     -- Polarno≈õƒá sentymentu (-1 do 1)
    sentiment_subjectivity REAL, -- Subiektywno≈õƒá (0 do 1)
    language_detected TEXT,      -- Wykryty jƒôzyk
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **domains** - Katalog domen

```sql
CREATE TABLE domains (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    domain TEXT UNIQUE NOT NULL,     -- Nazwa domeny (np. "youtube.com")
    category TEXT,                   -- Kategoria: 'religious', 'media', 'social', 'educational', 'unknown'
    is_religious BOOLEAN DEFAULT 0,  -- Czy domena religijna
    is_media BOOLEAN DEFAULT 0,      -- Czy domena medialna
    is_social BOOLEAN DEFAULT 0,     -- Czy domena spo≈Çeczno≈õciowa
    is_educational BOOLEAN DEFAULT 0,-- Czy domena edukacyjna
    trust_score REAL DEFAULT 0.5,    -- Wska≈∫nik wiarygodno≈õci (0.0-1.0)
    first_seen TIMESTAMP,            -- Pierwsze wystƒÖpienie
    last_seen TIMESTAMP,             -- Ostatnie wystƒÖpienie
    total_references INTEGER DEFAULT 0, -- Liczba odniesie≈Ñ
    created_at TIMESTAMP
);
```

#### **post_urls** - URL-e z post√≥w

```sql
CREATE TABLE post_urls (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,                    -- ID posta
    url TEXT,                        -- Pe≈Çny URL
    domain_id INTEGER,               -- Odniesienie do tabeli domains
    url_type TEXT,                   -- Typ: 'article', 'video', 'image', 'social', 'unknown'
    is_external BOOLEAN DEFAULT 1,   -- Czy link zewnƒôtrzny
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id),
    FOREIGN KEY (domain_id) REFERENCES domains (id)
);
```

#### **post_url_stats** - Statystyki URL-√≥w per post

```sql
CREATE TABLE post_url_stats (
    post_id TEXT PRIMARY KEY,
    total_urls INTEGER DEFAULT 0,       -- Ca≈Çkowita liczba URL-√≥w
    unique_domains INTEGER DEFAULT 0,   -- Liczba unikalnych domen
    religious_urls INTEGER DEFAULT 0,   -- Liczba link√≥w religijnych
    media_urls INTEGER DEFAULT 0,       -- Liczba link√≥w medialnych
    social_urls INTEGER DEFAULT 0,      -- Liczba link√≥w spo≈Çeczno≈õciowych
    educational_urls INTEGER DEFAULT 0, -- Liczba link√≥w edukacyjnych
    unknown_urls INTEGER DEFAULT 0,     -- Liczba niekategoryzowanych link√≥w
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_named_entities** - Named Entities (nazwy w≈Çasne)

```sql
CREATE TABLE post_named_entities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    post_id TEXT,                    -- ID posta
    entity_text TEXT,                -- Tekst encji (np. "Jan Pawe≈Ç II")
    entity_label TEXT,               -- Typ: PERSON, ORG, GPE, EVENT, etc.
    entity_description TEXT,         -- Opis typu encji
    start_char INTEGER,              -- Pozycja poczƒÖtkowa w tek≈õcie
    end_char INTEGER,                -- Pozycja ko≈Ñcowa w tek≈õcie
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

#### **post_ner_stats** - Statystyki Named Entities per post

```sql
CREATE TABLE post_ner_stats (
    post_id TEXT PRIMARY KEY,
    total_entities INTEGER DEFAULT 0,    -- Ca≈Çkowita liczba encji
    person_entities INTEGER DEFAULT 0,   -- Osoby (Jan Pawe≈Ç II, ≈õw. Teresa)
    org_entities INTEGER DEFAULT 0,      -- Organizacje (Ko≈õci√≥≈Ç, Watykan)
    gpe_entities INTEGER DEFAULT 0,      -- Miejsca (Rzym, Polska, Krak√≥w)
    event_entities INTEGER DEFAULT 0,    -- Wydarzenia (Wielkanoc, Bo≈ºe Narodzenie)
    other_entities INTEGER DEFAULT 0,    -- Inne encje
    created_at TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts (id)
);
```

## üìä Przyk≈Çady analiz SQL

### Podstawowe statystyki

#### **PrzeglƒÖd for√≥w w wsp√≥lnej bazie**

```sql
-- Por√≥wnanie aktywno≈õci miƒôdzy forami
SELECT
    f.spider_name as forum,
    f.title,
    COUNT(DISTINCT p.id) as posts_count,
    COUNT(DISTINCT p.user_id) as users_count,
    COUNT(DISTINCT t.id) as threads_count,
    MIN(p.created_at) as first_post,
    MAX(p.created_at) as last_post
FROM forums f
LEFT JOIN sections s ON f.id = s.forum_id
LEFT JOIN threads t ON s.id = t.section_id
LEFT JOIN posts p ON t.id = p.thread_id
GROUP BY f.id, f.spider_name, f.title
ORDER BY posts_count DESC;
```

#### **Najaktywniejsze sekcje**

```sql
SELECT
    s.title as section,
    COUNT(p.id) as posts_count,
    COUNT(DISTINCT t.id) as threads_count
FROM sections s
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
GROUP BY s.id, s.title
ORDER BY posts_count DESC
LIMIT 10;
```

#### **Top u≈ºytkownicy**

```sql
SELECT
    u.username,
    COUNT(p.id) as posts_count,
    u.join_date,
    u.religion,
    u.gender
FROM users u
JOIN posts p ON u.id = p.user_id
GROUP BY u.id, u.username
ORDER BY posts_count DESC
LIMIT 20;
```

### Analizy jƒôzykowe

#### **Najczƒô≈õciej u≈ºywane s≈Çowa**

```sql
-- Top lematy (formy podstawowe s≈Ç√≥w)
SELECT
    lemma,
    COUNT(*) as frequency,
    COUNT(DISTINCT post_id) as posts_with_word
FROM post_linguistic_analysis
WHERE is_alpha = 1
  AND is_stop = 0
  AND LENGTH(lemma) > 3
GROUP BY lemma
ORDER BY frequency DESC
LIMIT 50;
```

#### **Analiza czƒô≈õci mowy**

```sql
-- Rozk≈Çad czƒô≈õci mowy
SELECT
    pos,
    COUNT(*) as count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage
FROM post_linguistic_analysis
WHERE is_alpha = 1
GROUP BY pos
ORDER BY count DESC;
```

#### **Analiza cech morfologicznych**

```sql
-- Analiza przypadk√≥w rzeczownik√≥w (wykorzystanie morphologizer)
SELECT
    json_extract(morph_features, '$.Case') as case_form,
    COUNT(*) as frequency,
    COUNT(DISTINCT post_id) as posts_count
FROM post_linguistic_analysis
WHERE pos = 'NOUN'
  AND json_extract(morph_features, '$.Case') IS NOT NULL
GROUP BY json_extract(morph_features, '$.Case')
ORDER BY frequency DESC;
```

#### **Analiza rodzaju gramatycznego**

```sql
-- Rozk≈Çad rodzaju gramatycznego w tekstach religijnych
SELECT
    f.spider_name as forum,
    json_extract(pla.morph_features, '$.Gender') as gender,
    COUNT(*) as frequency
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_analysis pla ON p.id = pla.post_id
WHERE pla.pos IN ('NOUN', 'ADJ')
  AND json_extract(pla.morph_features, '$.Gender') IS NOT NULL
GROUP BY f.spider_name, json_extract(pla.morph_features, '$.Gender')
ORDER BY f.spider_name, frequency DESC;
```

#### **Analiza sentymentu**

```sql
-- Posty z najwy≈ºszym sentymentem pozytywnym
SELECT
    f.spider_name as forum,
    p.content,
    pls.sentiment_polarity,
    pls.sentiment_subjectivity,
    u.username,
    p.post_date
FROM posts p
JOIN post_linguistic_stats pls ON p.id = pls.post_id
JOIN users u ON p.user_id = u.id
JOIN threads t ON p.thread_id = t.id
JOIN sections s ON t.section_id = s.id
JOIN forums f ON s.forum_id = f.id
WHERE pls.sentiment_polarity > 0.5
ORDER BY pls.sentiment_polarity DESC
LIMIT 10;
```

#### **Por√≥wnanie sentymentu miƒôdzy forami**

```sql
-- ≈öredni sentyment na forum
SELECT
    f.spider_name as forum,
    COUNT(pls.post_id) as analyzed_posts,
    ROUND(AVG(pls.sentiment_polarity), 3) as avg_sentiment,
    ROUND(AVG(pls.sentiment_subjectivity), 3) as avg_subjectivity,
    ROUND(AVG(pls.readability_score), 1) as avg_readability
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_stats pls ON p.id = pls.post_id
GROUP BY f.id, f.spider_name
HAVING analyzed_posts > 50
ORDER BY avg_sentiment DESC;
```

#### **Statystyki czytelno≈õci**

```sql
-- ≈örednie wska≈∫niki czytelno≈õci po sekcjach
SELECT
    s.title as section,
    COUNT(pls.post_id) as analyzed_posts,
    ROUND(AVG(pls.readability_score), 2) as avg_readability,
    ROUND(AVG(pls.avg_sentence_length), 2) as avg_sentence_len,
    ROUND(AVG(pls.word_count), 0) as avg_words
FROM sections s
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_stats pls ON p.id = pls.post_id
GROUP BY s.id, s.title
HAVING analyzed_posts > 100
ORDER BY avg_readability DESC;
```

#### **Analiza tematyczna przez s≈Çowa kluczowe**

```sql
-- Posty zawierajƒÖce s≈Çowa religijne z podzia≈Çem na fora
SELECT
    f.spider_name as forum,
    COUNT(DISTINCT p.id) as posts_with_keywords,
    COUNT(DISTINCT pla.lemma) as unique_religious_words
FROM posts p
JOIN threads t ON p.thread_id = t.id
JOIN sections s ON t.section_id = s.id
JOIN forums f ON s.forum_id = f.id
JOIN post_linguistic_analysis pla ON p.id = pla.post_id
WHERE pla.lemma IN ('b√≥g', 'jezus', 'chrystus', 'modlitwa', 'wiara', 'ko≈õci√≥≈Ç')
  AND pla.is_alpha = 1
GROUP BY f.id, f.spider_name
ORDER BY posts_with_keywords DESC;
```

#### **Analiza s≈Çownictwa religijnego miƒôdzy forami**

```sql
-- Najczƒô≈õciej u≈ºywane s≈Çowa religijne per forum
SELECT
    f.spider_name as forum,
    pla.lemma,
    COUNT(*) as frequency,
    COUNT(DISTINCT p.id) as posts_count
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_linguistic_analysis pla ON p.id = pla.post_id
WHERE pla.lemma IN ('b√≥g', 'jezus', 'chrystus', 'modlitwa', 'wiara', 'ko≈õci√≥≈Ç', 'duch', '≈õwiƒôty')
  AND pla.is_alpha = 1
GROUP BY f.spider_name, pla.lemma
ORDER BY f.spider_name, frequency DESC;
```

### Analizy domen i URL-√≥w

#### **Najpopularniejsze domeny**

```sql
-- Top domeny linkowane w postach
SELECT
    d.domain,
    d.category,
    d.total_references,
    COUNT(DISTINCT pu.post_id) as posts_with_domain
FROM domains d
JOIN post_urls pu ON d.id = pu.domain_id
GROUP BY d.id, d.domain, d.category
ORDER BY d.total_references DESC
LIMIT 20;
```

#### **Analiza domen religijnych**

```sql
-- Domeny religijne per forum
SELECT
    f.spider_name as forum,
    COUNT(DISTINCT d.domain) as religious_domains,
    SUM(d.total_references) as total_religious_links
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_urls pu ON p.id = pu.post_id
JOIN domains d ON pu.domain_id = d.id
WHERE d.is_religious = 1
GROUP BY f.spider_name
ORDER BY total_religious_links DESC;
```

#### **Typy URL-√≥w per forum**

```sql
-- Rozk≈Çad typ√≥w URL-√≥w
SELECT
    f.spider_name as forum,
    pu.url_type,
    COUNT(*) as count,
    COUNT(DISTINCT pu.post_id) as posts_count
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_urls pu ON p.id = pu.post_id
GROUP BY f.spider_name, pu.url_type
ORDER BY f.spider_name, count DESC;
```

#### **Wiarygodno≈õƒá ≈∫r√≥de≈Ç**

```sql
-- ≈örednia wiarygodno≈õƒá linkowanych domen per forum
SELECT
    f.spider_name as forum,
    COUNT(DISTINCT d.domain) as unique_domains,
    ROUND(AVG(d.trust_score), 3) as avg_trust_score,
    COUNT(CASE WHEN d.trust_score >= 0.8 THEN 1 END) as high_trust_domains,
    COUNT(CASE WHEN d.trust_score < 0.6 THEN 1 END) as low_trust_domains
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_urls pu ON p.id = pu.post_id
JOIN domains d ON pu.domain_id = d.id
GROUP BY f.spider_name
ORDER BY avg_trust_score DESC;
```

### Analizy Named Entities (NER)

#### **Najczƒô≈õciej wymieniane osoby**

```sql
-- Top osoby religijne w dyskusjach
SELECT
    f.spider_name as forum,
    pne.entity_text as person,
    COUNT(*) as mentions,
    COUNT(DISTINCT pne.post_id) as posts_count
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_named_entities pne ON p.id = pne.post_id
WHERE pne.entity_label IN ('PERSON', 'PER')
GROUP BY f.spider_name, pne.entity_text
HAVING mentions > 5
ORDER BY f.spider_name, mentions DESC;
```

#### **Organizacje religijne**

```sql
-- Najczƒô≈õciej wymieniane organizacje
SELECT
    pne.entity_text as organization,
    COUNT(*) as mentions,
    COUNT(DISTINCT f.spider_name) as forums_mentioned
FROM post_named_entities pne
JOIN posts p ON pne.post_id = p.id
JOIN threads t ON p.thread_id = t.id
JOIN sections s ON t.section_id = s.id
JOIN forums f ON s.forum_id = f.id
WHERE pne.entity_label IN ('ORG', 'ORGANIZATION')
GROUP BY pne.entity_text
ORDER BY mentions DESC
LIMIT 15;
```

#### **Miejsca geograficzne**

```sql
-- Miejsca religijne w dyskusjach
SELECT
    f.spider_name as forum,
    pne.entity_text as place,
    COUNT(*) as mentions
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_named_entities pne ON p.id = pne.post_id
WHERE pne.entity_label IN ('GPE', 'LOC', 'LOCATION')
  AND (pne.entity_text LIKE '%Watykan%'
    OR pne.entity_text LIKE '%Rzym%'
    OR pne.entity_text LIKE '%Krak√≥w%'
    OR pne.entity_text LIKE '%Czƒôstochowa%')
GROUP BY f.spider_name, pne.entity_text
ORDER BY f.spider_name, mentions DESC;
```

#### **Statystyki NER per forum**

```sql
-- Por√≥wnanie rozpoznawania encji miƒôdzy forami
SELECT
    f.spider_name as forum,
    COUNT(DISTINCT pns.post_id) as posts_with_entities,
    SUM(pns.total_entities) as total_entities,
    SUM(pns.person_entities) as persons,
    SUM(pns.org_entities) as organizations,
    SUM(pns.gpe_entities) as places,
    ROUND(AVG(pns.total_entities), 2) as avg_entities_per_post
FROM forums f
JOIN sections s ON f.id = s.forum_id
JOIN threads t ON s.id = t.section_id
JOIN posts p ON t.id = p.thread_id
JOIN post_ner_stats pns ON p.id = pns.post_id
GROUP BY f.spider_name
ORDER BY total_entities DESC;
```

### Analizy temporalne

#### **Aktywno≈õƒá w czasie**

```sql
-- Liczba post√≥w po miesiƒÖcach
SELECT
    strftime('%Y-%m', created_at) as month,
    COUNT(*) as posts_count
FROM posts
WHERE created_at IS NOT NULL
GROUP BY month
ORDER BY month;
```

#### **Ewolucja sentymentu**

```sql
-- ≈öredni sentyment po miesiƒÖcach
SELECT
    strftime('%Y-%m', p.created_at) as month,
    COUNT(pls.post_id) as posts_analyzed,
    ROUND(AVG(pls.sentiment_polarity), 3) as avg_sentiment,
    ROUND(AVG(pls.readability_score), 1) as avg_readability
FROM posts p
JOIN post_linguistic_stats pls ON p.id = pls.post_id
WHERE p.created_at IS NOT NULL
GROUP BY month
HAVING posts_analyzed > 10
ORDER BY month;
```

## ‚öôÔ∏è Konfiguracja zaawansowana

### Plik konfiguracyjny YAML

#### **Podstawowa konfiguracja**

```yaml
# config.yaml
analysis:
  enabled: true
  analyzers:
    - name: basic_tokenizer
      config:
        lowercase: true
        min_token_length: 2

    - name: token_counter
      config:
        encoding: cl100k_base

  concurrency: 4

scrapy:
  concurrent_requests: 16
  download_delay: 0.5
  autothrottle: true
```

#### **Konfiguracja produkcyjna**

```yaml
# production_config.yaml
analysis:
  enabled: true
  analyzers:
    - name: basic_tokenizer
      config:
        lowercase: true
        min_token_length: 2
        remove_punctuation: false

    - name: token_counter
      config:
        encoding: cl100k_base

    - name: spacy_analyzer
      config:
        model: pl_core_news_lg
        include_sentiment: true
        batch_size: 200
        max_length: 1000000

  concurrency: 8

scrapy:
  concurrent_requests: 32
  concurrent_requests_per_domain: 16
  download_delay: 0.2
  randomize_download_delay: true
  autothrottle: true
  autothrottle_start_delay: 0.1
  autothrottle_max_delay: 2.0
  autothrottle_target_concurrency: 24.0
```

### Optymalizacja wydajno≈õci

#### **Dla szybkiego scrapowania**

```bash
uv run fs-cli scrape \
  --concurrent 64 \
  --delay 0.1 \
  --analysis basic_tokens \
  --forum radio_katolik
```

#### **Dla dok≈Çadnej analizy**

```bash
uv run fs-cli scrape \
  --concurrent 8 \
  --delay 1.0 \
  --batch-size 50 \
  --analysis all \
  --sentiment \
  --spacy-model pl_core_news_lg
```

#### **Dla ograniczonych zasob√≥w**

```bash
uv run fs-cli scrape \
  --concurrent 4 \
  --delay 2.0 \
  --batch-size 25 \
  --analysis basic_tokens \
  --forum dolina_modlitwy
```

## üîß RozwiƒÖzywanie problem√≥w

### Czƒôste problemy

#### **1. B≈ÇƒÖd "spaCy model not found"**

```bash
# RozwiƒÖzanie
python -m spacy download pl_core_news_lg

# Lub wiƒôkszy model (lepszy, ale wolniejszy)
python -m spacy download pl_core_news_lg
```

#### **2. B≈ÇƒÖd "tiktoken not found"**

```bash
# RozwiƒÖzanie
uv pip install tiktoken
```

#### **3. Problemy z pamiƒôciƒÖ podczas analizy spaCy**

```bash
# Zmniejsz batch size
uv run fs-cli scrape --batch-size 25 --analysis spacy_full

# Lub u≈ºyj mniejszego modelu
uv run fs-cli scrape --spacy-model pl_core_news_lg
```

#### **4. Zbyt wolne scrapowanie**

```bash
# Zwiƒôksz r√≥wnoleg≈Ço≈õƒá (ostro≈ºnie!)
uv run fs-cli scrape --concurrent 32 --delay 0.2

# Wy≈ÇƒÖcz analizy dla szybszego scrapowania
uv run fs-cli scrape --analysis none
```

#### **5. B≈Çƒôdy po≈ÇƒÖczenia sieciowego**

```bash
# Zwiƒôksz op√≥≈∫nienia
uv run fs-cli scrape --delay 2.0 --concurrent 8

# Sprawd≈∫ po≈ÇƒÖczenie internetowe
ping google.com
```

### Debugowanie

#### **W≈ÇƒÖcz szczeg√≥≈Çowe logi**

```bash
uv run fs-cli scrape --verbose --forum radio_katolik
```

#### **Testuj bez scrapowania**

```bash
uv run fs-cli scrape --dry-run --analysis all
```

#### **Sprawd≈∫ status baz danych**

```bash
uv run fs-cli status
```

## üìä Monitorowanie podczas scrapowania

### **Real-time monitoring (zalecane)**

#### **Terminal 1: Uruchom scrapowanie**

```bash
uv run fs-cli scrape \
  --forum all \
  --analysis all \
  --sentiment \
  --spacy-model pl_core_news_lg \
  --concurrent 8 \
  --delay 0.3 \
  --verbose
```

#### **Terminal 2: Monitoruj postƒôp bazy danych**

```bash
# Od≈õwie≈ºaj co 10 sekund
watch -n 10 'uv run fs-cli status'

# Lub sprawdzaj rƒôcznie
while true; do
  clear
  echo "=== $(date) ==="
  uv run fs-cli status
  sleep 15
done
```

#### **Terminal 3: Monitoruj zasoby systemowe**

```bash
# Monitoruj proces fs-cli
top -pid $(pgrep -f fs-cli)

# Lub wszystkie procesy Python
top -o cpu -stats pid,command,cpu,mem,time | grep python

# Monitoruj pamiƒôƒá
watch -n 5 'ps aux | grep fs-cli | grep -v grep'
```

#### **Terminal 4: Monitoruj rozmiar bazy**

```bash
# Rozmiar bazy danych
watch -n 30 'ls -lh data/databases/forums_unified.db'

# Szczeg√≥≈Çowe info o pliku
watch -n 30 'stat data/databases/forums_unified.db'
```

### **Monitoring zaawansowany**

#### **Sprawd≈∫ postƒôp scrapowania**

```bash
# Liczba item√≥w w czasie rzeczywistym
watch -n 5 'sqlite3 data/databases/forums_unified.db "
SELECT
  f.spider_name,
  COUNT(p.id) as posts,
  COUNT(DISTINCT t.id) as threads,
  COUNT(DISTINCT u.id) as users
FROM forums f
LEFT JOIN sections s ON f.id = s.forum_id
LEFT JOIN threads t ON s.id = t.section_id
LEFT JOIN posts p ON t.id = p.thread_id
LEFT JOIN users u ON p.user_id = u.id
GROUP BY f.spider_name;"'
```

#### **Monitoruj wydajno≈õƒá analiz**

```bash
# Sprawd≈∫ postƒôp analiz
watch -n 10 'sqlite3 data/databases/forums_unified.db "
SELECT
  \"Tokenizacja\" as analiza, COUNT(*) as przeanalizowane FROM post_token_stats
UNION ALL
SELECT
  \"Jƒôzykowa\" as analiza, COUNT(*) as przeanalizowane FROM post_linguistic_stats
UNION ALL
SELECT
  \"NER\" as analiza, COUNT(*) as przeanalizowane FROM post_ner_stats
UNION ALL
SELECT
  \"URL-e\" as analiza, COUNT(*) as przeanalizowane FROM post_url_stats;"'
```

#### **Sprawd≈∫ b≈Çƒôdy i ostrze≈ºenia**

```bash
# Monitoruj logi Scrapy (je≈õli --verbose)
tail -f scrapy.log

# Lub sprawd≈∫ b≈Çƒôdy systemowe
dmesg | tail -20
```

### **Wska≈∫niki wydajno≈õci**

#### **Optymalne warto≈õci:**

- **CPU**: 60-80% (dla 8 wƒÖtk√≥w)
- **RAM**: 4-8GB (z modelem lg)
- **Items/min**: 100-500 (zale≈ºnie od forum)
- **Requests/min**: 200-1000 (zale≈ºnie od --delay)

#### **Sygna≈Çy problem√≥w:**

- **CPU > 95%**: Zmniejsz `--concurrent` lub `--batch-size`
- **RAM > 12GB**: Zmniejsz `--batch-size` lub u≈ºyj modelu sm
- **Items/min < 50**: Zwiƒôksz `--concurrent` lub zmniejsz `--delay`
- **B≈Çƒôdy HTTP**: Zwiƒôksz `--delay`

### **Komendy diagnostyczne**

#### **Sprawd≈∫ aktywne po≈ÇƒÖczenia**

```bash
netstat -an | grep :80 | wc -l  # HTTP connections
netstat -an | grep :443 | wc -l # HTTPS connections
```

#### **Sprawd≈∫ wykorzystanie dysku**

```bash
df -h .  # Dostƒôpne miejsce
du -sh data/databases/  # Rozmiar katalogu baz
```

#### **Sprawd≈∫ logi b≈Çƒôd√≥w**

```bash
# Ostatnie b≈Çƒôdy
grep -i error ~/.scrapy/logs/* | tail -10

# Sprawd≈∫ czy proces dzia≈Ça
ps aux | grep fs-cli | grep -v grep
```

### **Przyk≈Çad pe≈Çnego workflow'u monitorowania**

#### **Przygotowanie (Terminal 1):**

```bash
# Sprawd≈∫ stan przed rozpoczƒôciem
uv run fs-cli status

# Uruchom kompleksowe scrapowanie
uv run fs-cli scrape \
  --forum all \
  --analysis all \
  --sentiment \
  --spacy-model pl_core_news_lg \
  --concurrent 8 \
  --delay 0.3 \
  --verbose
```

#### **Monitoring (Terminal 2):**

```bash
# Skrypt monitorujƒÖcy - zapisz jako monitor.sh
#!/bin/bash
while true; do
  clear
  echo "=== FORUMS SCRAPER MONITORING - $(date) ==="
  echo

  # Status bazy danych
  echo "üìä STATUS BAZY DANYCH:"
  uv run fs-cli status
  echo

  # Zasoby systemowe
  echo "üíª ZASOBY SYSTEMOWE:"
  ps aux | grep fs-cli | grep -v grep | head -3
  echo

  # Rozmiar bazy
  echo "üìÅ ROZMIAR BAZY:"
  ls -lh data/databases/forums_unified.db 2>/dev/null || echo "Baza nie istnieje"
  echo

  sleep 30
done

# Uruchom monitoring
chmod +x monitor.sh && ./monitor.sh
```

#### **Analiza postƒôpu (Terminal 3):**

```bash
# Real-time SQL monitoring
watch -n 15 'echo "POSTƒòP SCRAPOWANIA:" && sqlite3 data/databases/forums_unified.db "
SELECT
  f.spider_name as Forum,
  COUNT(DISTINCT s.id) as Sekcje,
  COUNT(DISTINCT t.id) as WƒÖtki,
  COUNT(DISTINCT p.id) as Posty,
  COUNT(DISTINCT u.id) as U≈ºytkownicy
FROM forums f
LEFT JOIN sections s ON f.id = s.forum_id
LEFT JOIN threads t ON s.id = t.section_id
LEFT JOIN posts p ON t.id = p.thread_id
LEFT JOIN users u ON p.user_id = u.id
GROUP BY f.spider_name
ORDER BY Posty DESC;" 2>/dev/null || echo "Baza nie gotowa"'
```

## üèóÔ∏è Architektura systemu

### Struktura projektu

```
forums_scraper/
‚îú‚îÄ‚îÄ üìÑ README.md                    # Kompletna dokumentacja (ten plik)
‚îú‚îÄ‚îÄ ‚öôÔ∏è  pyproject.toml               # Konfiguracja pakietu Python
‚îú‚îÄ‚îÄ üìÇ data/                        # Bazy danych i wyniki
‚îÇ   ‚îî‚îÄ‚îÄ databases/                  # SQLite bazy danych
‚îú‚îÄ‚îÄ üìÇ forums_scraper/              # G≈Ç√≥wny pakiet Python
‚îÇ   ‚îú‚îÄ‚îÄ analyzers_basic/            # üî¨ Analizatory NLP
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ linguistic.py           #   ‚îú‚îÄ‚îÄ SpacyAnalyzer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenizer.py            #   ‚îî‚îÄ‚îÄ TokenCountAnalyzer
‚îÇ   ‚îú‚îÄ‚îÄ fs_cli/                     # üéõÔ∏è Interfejs CLI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced.py             #   ‚îî‚îÄ‚îÄ Rich + Typer UI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py                 #
‚îÇ   ‚îú‚îÄ‚îÄ fs_core/                    # ‚öôÔ∏è Rdze≈Ñ systemu
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py               #   ‚îú‚îÄ‚îÄ Konfiguracja YAML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ protocol.py             #   ‚îú‚îÄ‚îÄ Interfejsy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ registry.py             #   ‚îú‚îÄ‚îÄ Entry points
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ runner.py               #   ‚îî‚îÄ‚îÄ Async runner
‚îÇ   ‚îî‚îÄ‚îÄ scraper/                    # üï∑Ô∏è Silnik Scrapy
‚îÇ       ‚îú‚îÄ‚îÄ items.py                #   ‚îú‚îÄ‚îÄ Modele danych
‚îÇ       ‚îú‚îÄ‚îÄ middlewares.py          #   ‚îú‚îÄ‚îÄ Middleware
‚îÇ       ‚îú‚îÄ‚îÄ pipelines/              #   ‚îú‚îÄ‚îÄ Pipeline'y
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ analysis.py         #   ‚îÇ   ‚îú‚îÄ‚îÄ Analiza
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ database.py         #   ‚îÇ   ‚îî‚îÄ‚îÄ Baza danych
‚îÇ       ‚îú‚îÄ‚îÄ settings.py             #   ‚îú‚îÄ‚îÄ Ustawienia
‚îÇ       ‚îú‚îÄ‚îÄ spiders/                #   ‚îî‚îÄ‚îÄ Spidery for√≥w
‚îÇ       ‚îî‚îÄ‚îÄ utils.py
```

### Przep≈Çyw danych

```mermaid
graph TD
    A[uv run fs-cli scrape] --> B[Scrapy Engine]
    B --> C[Spider]
    C --> D[Web Scraping]
    D --> E[Items]
    E --> F[Analysis Pipeline]
    F --> G[NLP Analyzers]
    G --> H[Database Pipeline]
    H --> I[SQLite Database]

    J[Config YAML] --> F
    K[Entry Points] --> G
```

### Wzorce projektowe

- **Plugin Architecture** - Entry points dla analizator√≥w
- **Pipeline Pattern** - Scrapy pipelines dla przetwarzania
- **Strategy Pattern** - R√≥≈ºne typy analiz
- **Observer Pattern** - Progress reporting
- **Factory Pattern** - Tworzenie analizator√≥w

## üöÄ Rozw√≥j i wk≈Çad

### Dodawanie nowych analizator√≥w

#### **1. Implementacja analizatora**

```python
# my_analyzer.py
from typing import Any, Dict
from forums_scraper.fs_core.protocol import Analyzer

class MyCustomAnalyzer(Analyzer):
    def __init__(self, **config):
        self.config = config

    async def setup(self):
        # Inicjalizacja (≈Çadowanie modeli, etc.)
        pass

    async def analyze(self, data: Dict[str, Any]) -> Dict[str, Any]:
        content = data.get('content', '')

        # Twoja analiza tutaj
        result = self.my_analysis_function(content)

        return {
            'my_analysis': result
        }

    async def close(self):
        # SprzƒÖtanie zasob√≥w
        pass
```

#### **2. Rejestracja w pyproject.toml**

```toml
[project.entry-points."forums_scraper.analyzers"]
my_analyzer = "my_package.my_analyzer:MyCustomAnalyzer"
```

#### **3. U≈ºycie w konfiguracji**

```yaml
analysis:
  enabled: true
  analyzers:
    - name: my_analyzer
      config:
        parameter1: value1
        parameter2: value2
```

### Dodawanie nowych for√≥w

#### **1. Implementacja spidera**

```python
# new_forum_spider.py
import scrapy
from forums_scraper.scraper.items import ForumPostItem

class NewForumSpider(scrapy.Spider):
    name = 'new_forum'
    allowed_domains = ['newforum.com']
    start_urls = ['https://newforum.com']

    def parse(self, response):
        # Implementacja scrapowania
        pass
```

#### **2. Aktualizacja CLI**

```python
# W fs_cli/advanced.py
class ForumName(str, Enum):
    # ... istniejƒÖce fora
    NEW_FORUM = "new_forum"

FORUM_SPIDER_MAP = {
    # ... istniejƒÖce mapowania
    ForumName.NEW_FORUM: "new_forum",
}
```

### Testowanie

```bash
# Testy jednostkowe
uv run python -m pytest tests/

# Testy integracyjne
uv run python -m pytest tests/integration/

# Testy analizator√≥w
uv run python -m pytest tests/analyzers/
```

### Code style

```bash
# Formatowanie kodu
uv run black forums_scraper/
uv run isort forums_scraper/

# Linting
uv run flake8 forums_scraper/
uv run mypy forums_scraper/
```

## üìö Zasoby dodatkowe

### Dokumentacja techniczna

- **README.md** (ten plik) - Kompletna dokumentacja
- [API Documentation](docs/api/) - Dokumentacja API (w przygotowaniu)
- [Database Schema](docs/database.md) - Schemat bazy danych (w przygotowaniu)

### Przyk≈Çady u≈ºycia

- **README.md** (ten plik) - Kompletne przyk≈Çady SQL i konfiguracji
- **CLI** - Automatyczne generowanie konfiguracji: `uv run fs-cli config`
- **Dokumentacja inline** - Wszystkie przyk≈Çady w tym pliku

### Spo≈Çeczno≈õƒá

- **Issues** - [GitHub Issues](https://github.com/username/forums_scraper/issues)
- **Discussions** - [GitHub Discussions](https://github.com/username/forums_scraper/discussions)
- **Wiki** - [Project Wiki](https://github.com/username/forums_scraper/wiki)

## üìÑ Licencja

MIT License - zobacz [LICENSE](LICENSE) dla szczeg√≥≈Ç√≥w.

## üôè Podziƒôkowania

- **Astral Team** - za ultraszybki mened≈ºer pakiet√≥w uv
- **Scrapy Team** - za doskona≈Çy framework scrapowania
- **spaCy Team** - za zaawansowane narzƒôdzia NLP
- **Rich Team** - za piƒôkny interfejs CLI
- **Spo≈Çeczno≈õƒá Open Source** - za inspiracjƒô i wsparcie

---

**Autor:** alb  
**Wersja:** 0.1.0  
**Python:** 3.10+  
**Licencja:** MIT

_Forums Scraper - Profesjonalne narzƒôdzie do analizy dyskursu religijnego w internecie_ üï∑Ô∏è‚ú®
