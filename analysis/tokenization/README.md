# Modu Tokenizacji

Modu odpowiedzialny za dokadn analiz i liczenie token贸w w postach forum.

##  Funkcjonalnoci

- **Dokadna tokenizacja** u偶ywajc spaCy z modelem polskim
- **Automatyczny fallback** do prostego algorytmu
- **Inteligentne filtrowanie** token贸w (interpunkcja, biae znaki)
- **Statystyki tokenizacji** (spaCy vs prosty algorytm)
- **Optymalizacja wydajnoci** z cache'owaniem

##  Struktura

```
tokenization/
 __init__.py              # Inicjalizacja moduu
 token_analyzer.py        # G贸wny analizator token贸w
 install_spacy_model.py   # Instalacja spaCy i modelu polskiego
 compare_tokenizers.py    # Por贸wnanie algorytm贸w tokenizacji
 README.md                # Ta dokumentacja
```

##  Szybki start

### Instalacja spaCy

```bash
# Automatyczna instalacja
python analysis/tokenization/install_spacy_model.py

# Lub rcznie
pip install spacy>=3.5.0
python -m spacy download pl_core_news_sm
```

### U偶ycie w kodzie

```python
from analysis.tokenization import TokenAnalyzer

# Utw贸rz analizator
analyzer = TokenAnalyzer()

# Analizuj tekst
result = analyzer.calculate_tokens("To jest test polskiego tekstu.")
print(f"Tokeny: {result['tokens']}, Sowa: {result['words']}")
```

### Por贸wnanie algorytm贸w

```bash
python analysis/tokenization/compare_tokenizers.py
```

##  Konfiguracja

### Parametry tokenizacji

```python
TOKENIZATION_CONFIG = {
    'tokenizer': 'spacy',           # 'spacy', 'simple', 'hybrid'
    'spacy_model': 'pl_core_news_sm',  # Model spaCy
    'fallback_to_simple': True,     # U偶yj prostego algorytmu jako fallback
    'include_punctuation': False,   # Czy liczy interpunkcj
    'include_whitespace': False,    # Czy liczy biae znaki
    'min_word_length': 2,           # Minimalna dugo tokenu
}
```

##  Algorytmy Tokenizacji

### 1. spaCy (G贸wny)
- **Model**: `pl_core_news_sm` - specjalistyczny dla jzyka polskiego
- **Dokadno**: Wysoka - prawdziwa analiza lingwistyczna
- **Wydajno**: rednia - wymaga zaadowania modelu

### 2. Prosty Algorytm (Fallback)
- **Metoda**: `words * 0.75` (dostosowane dla jzyka polskiego)
- **Dokadno**: Niska - przybli偶enie
- **Wydajno**: Wysoka - szybkie obliczenia

##  Metryki i Statystyki

### Statystyki tokenizacji

```python
stats = {
    'spacy_tokens': 1250,      # Posty przetworzone przez spaCy
    'simple_tokens': 50,       # Posty przetworzone przez prosty algorytm
    'total_processed': 1300,   # czna liczba przetworzonych post贸w
}
```

## И Testowanie

### Testy jednostkowe

```bash
# Uruchom testy moduu
python -m analysis.test_analysis

# Testy tokenizacji
python analysis/tokenization/compare_tokenizers.py
```

##  Rozwizywanie Problem贸w

### Bd: "spaCy nie jest dostpne"

```bash
# Sprawd藕 instalacj
python -c "import spacy; print('spaCy OK')"

# Zainstaluj ponownie
python analysis/tokenization/install_spacy_model.py
```

##  API Reference

### TokenAnalyzer

```python
class TokenAnalyzer:
    def __init__(self, source_db: str, analysis_db: str, config: Dict = None)
    def calculate_tokens(self, text: str) -> Dict[str, int]
    def _calculate_tokens_spacy(self, text: str) -> Optional[int]
    def _calculate_tokens_simple(self, text: str) -> int
```

### TokenAnalysisResult

```python
@dataclass
class TokenAnalysisResult:
    post_id: int
    token_count: int
    word_count: int
    character_count: int
    analysis_hash: str
    analyzed_at: datetime
    processing_time_ms: float
```
