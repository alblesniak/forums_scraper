#!/usr/bin/env python3
"""
FINALNA wersja skryptu do uruchamiania wszystkich scraper√≥w r√≥wnolegle z paskami postƒôpu
i ≈ÇƒÖczenia danych w wsp√≥lnej bazie merged_forums.db

G≈Ç√≥wne funkcje:
- 4 paski postƒôpu dla ka≈ºdego scrapera
- Rzeczywiste monitorowanie proces√≥w
- Lepsza obs≈Çuga d≈Çugotrwa≈Çych proces√≥w
- Automatyczne ≈ÇƒÖczenie baz danych
- Szczeg√≥≈Çowe statystyki
"""

import subprocess
import threading
import time
import os
import sys
import sqlite3
from datetime import datetime
import signal
import psutil
from tqdm import tqdm
import queue
import json
import re
from pathlib import Path

class FinalScraperManager:
    def __init__(self):
        self.scrapers = [
            {
                'name': 'z_chrystusem',
                'command': ['scrapy', 'crawl', 'z_chrystusem', '-s', 'SETTINGS_MODULE=scraper.settings'],
                'db_file': 'data/databases/forum_z_chrystusem.db',
                'description': 'Z Chrystusem Forum'
            },
            {
                'name': 'radio_katolik',
                'command': ['scrapy', 'crawl', 'radio_katolik', '-s', 'SETTINGS_MODULE=scraper.settings'],
                'db_file': 'data/databases/forum_radio_katolik.db',
                'description': 'Radio Katolik Forum'
            },
            {
                'name': 'wiara',
                'command': ['scrapy', 'crawl', 'wiara', '-s', 'SETTINGS_MODULE=scraper.settings'],
                'db_file': 'data/databases/forum_wiara.db',
                'description': 'Wiara Forum'
            },
            {
                'name': 'dolina_modlitwy',
                'command': ['scrapy', 'crawl', 'dolina_modlitwy', '-s', 'SETTINGS_MODULE=scraper.settings'],
                'db_file': 'data/databases/forum_dolina_modlitwy.db',
                'description': 'Dolina Modlitwy Forum'
            }
        ]
        
        self.processes = {}
        self.progress_bars = {}
        self.stats = {}
        self.running = True
        self.completed_scrapers = set()
        self.failed_scrapers = set()
        
        # Statystyki dla ka≈ºdego scrapera
        for scraper in self.scrapers:
            self.stats[scraper['name']] = {
                'start_time': None,
                'end_time': None,
                'threads_found': 0,
                'posts_found': 0,
                'status': 'waiting',
                'progress': 0,
                'last_activity': None
            }
        
        # Obs≈Çuga sygna≈Ç√≥w dla bezpiecznego zamykania
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        
        # Katalogi log√≥w
        self.log_dir = Path('data/logs')
        self.log_dir.mkdir(exist_ok=True)
    
    def signal_handler(self, signum, frame):
        """Obs≈Çuga sygna≈Ç√≥w do bezpiecznego zamykania"""
        print("\n\nüõë Otrzymano sygna≈Ç zamykania. Zatrzymujƒô wszystkie scrapery...")
        self.stop_all_scrapers()
        self.save_final_stats()
        sys.exit(0)
    
    def create_progress_bar(self, scraper_name, description, position):
        """Tworzy pasek postƒôpu dla scrapera"""
        return tqdm(
            total=100,
            desc=f"{description:25}",
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
            position=position,
            leave=True,
            colour='green'
        )
    
    def update_progress_from_activity(self, scraper_name, progress_bar):
        """Aktualizuje postƒôp na podstawie aktywno≈õci procesu"""
        try:
            stats = self.stats[scraper_name]
            
            # Sprawd≈∫ czy proces nadal dzia≈Ça
            if scraper_name in self.processes:
                process = self.processes[scraper_name]
                
                # Sprawd≈∫ czy proces jest aktywny
                if process.poll() is None:
                    # Proces dzia≈Ça - zwiƒôksz postƒôp stopniowo
                    if stats['progress'] < 90:
                        # Symuluj postƒôp na podstawie czasu dzia≈Çania
                        elapsed = (datetime.now() - stats['start_time']).total_seconds()
                        
                        # Faza 1: Uruchamianie (0-20%) - szybko
                        if stats['progress'] < 20:
                            stats['progress'] = min(20, elapsed * 2)
                        
                        # Faza 2: Znalezienie wƒÖtk√≥w (20-50%) - ≈õrednio
                        elif stats['progress'] < 50:
                            stats['progress'] = min(50, 20 + (elapsed - 10) * 0.5)
                        
                        # Faza 3: Znalezienie post√≥w (50-80%) - wolno
                        elif stats['progress'] < 80:
                            stats['progress'] = min(80, 50 + (elapsed - 30) * 0.3)
                        
                        # Faza 4: Zapis (80-90%) - bardzo wolno
                        elif stats['progress'] < 90:
                            stats['progress'] = min(90, 80 + (elapsed - 60) * 0.1)
                        
                        # Aktualizuj pasek postƒôpu
                        progress_bar.n = int(stats['progress'])
                        progress_bar.refresh()
                        
                        stats['last_activity'] = datetime.now()
                
                else:
                    # Proces zako≈Ñczony
                    if process.returncode == 0:
                        stats['progress'] = 100
                        progress_bar.n = 100
                        progress_bar.set_description(f"‚úÖ {scraper_name:25}")
                        progress_bar.refresh()
                        self.completed_scrapers.add(scraper_name)
                        stats['status'] = 'completed'
                        stats['end_time'] = datetime.now()
                    else:
                        progress_bar.set_description(f"‚ùå {scraper_name:25}")
                        progress_bar.refresh()
                        self.failed_scrapers.add(scraper_name)
                        stats['status'] = 'failed'
                        stats['end_time'] = datetime.now()
                
        except Exception as e:
            pass  # Ignoruj b≈Çƒôdy aktualizacji postƒôpu
    
    def monitor_scraper_process(self, scraper_name, process, progress_bar):
        """Monitoruje proces scrapera i aktualizuje pasek postƒôpu"""
        try:
            # Czekaj na zako≈Ñczenie procesu
            process.wait()
            
            # Finalna aktualizacja
            if process.returncode == 0:
                self.completed_scrapers.add(scraper_name)
                self.stats[scraper_name]['status'] = 'completed'
                self.stats[scraper_name]['progress'] = 100
                progress_bar.n = 100
                progress_bar.set_description(f"‚úÖ {scraper_name:25}")
                progress_bar.refresh()
            else:
                self.failed_scrapers.add(scraper_name)
                self.stats[scraper_name]['status'] = 'failed'
                progress_bar.set_description(f"‚ùå {scraper_name:25}")
                progress_bar.refresh()
            
            # Zatrzymaj symulacjƒô postƒôpu
            self.stats[scraper_name]['end_time'] = datetime.now()
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd w monitorowaniu {scraper_name}: {e}")
    
    def run_scraper(self, scraper_info):
        """Uruchamia pojedynczy scraper w osobnym procesie"""
        scraper_name = scraper_info['name']
        command = scraper_info['command']
        description = scraper_info['description']
        
        try:
            print(f"üöÄ Uruchamiam {description}...")
            
            # Ustaw czas rozpoczƒôcia
            self.stats[scraper_name]['start_time'] = datetime.now()
            self.stats[scraper_name]['status'] = 'running'
            self.stats[scraper_name]['last_activity'] = datetime.now()
            
            # Uruchom proces scrapera
            process = subprocess.Popen(
                command,
                stdout=subprocess.DEVNULL,  # Ukryj stdout aby nie za≈õmiecaƒá terminala
                stderr=subprocess.DEVNULL,  # Ukryj stderr
                text=True
            )
            
            self.processes[scraper_name] = process
            
            # Utw√≥rz pasek postƒôpu
            position = len(self.progress_bars)
            progress_bar = self.create_progress_bar(scraper_name, description, position)
            self.progress_bars[scraper_name] = progress_bar
            
            # Uruchom monitorowanie procesu w osobnym wƒÖtku
            monitor_thread = threading.Thread(
                target=self.monitor_scraper_process,
                args=(scraper_name, process, progress_bar)
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd uruchamiania {description}: {e}")
            self.stats[scraper_name]['errors'] = [str(e)]
    
    def run_all_scrapers(self):
        """Uruchamia wszystkie scrapery r√≥wnolegle"""
        print("üöÄ Uruchamianie wszystkich scraper√≥w r√≥wnolegle...")
        print("=" * 80)
        
        # Uruchom wszystkie scrapery w osobnych wƒÖtkach
        threads = []
        for scraper_info in self.scrapers:
            thread = threading.Thread(
                target=self.run_scraper,
                args=(scraper_info,)
            )
            thread.daemon = True
            thread.start()
            threads.append(thread)
        
        # G≈Ç√≥wna pƒôtla monitorowania
        print("\nüìä Monitorowanie postƒôpu...")
        try:
            while self.running and len(self.completed_scrapers) + len(self.failed_scrapers) < len(self.scrapers):
                # Aktualizuj postƒôp dla wszystkich aktywnych scraper√≥w
                for scraper_name, progress_bar in self.progress_bars.items():
                    if scraper_name not in self.completed_scrapers and scraper_name not in self.failed_scrapers:
                        self.update_progress_from_activity(scraper_name, progress_bar)
                
                time.sleep(1)
                
        except KeyboardInterrupt:
            print("\nüõë Przerwano przez u≈ºytkownika")
            self.stop_all_scrapers()
        
        print("\n" + "=" * 80)
        print("üèÅ Wszystkie scrapery zako≈Ñczone")
        
        # Poka≈º podsumowanie
        self.show_summary()
    
    def stop_all_scrapers(self):
        """Zatrzymuje wszystkie uruchomione scrapery"""
        self.running = False
        
        for scraper_name, process in self.processes.items():
            try:
                if process.poll() is None:
                    print(f"üõë Zatrzymujƒô {scraper_name}...")
                    process.terminate()
                    
                    # Czekaj na zako≈Ñczenie
                    try:
                        process.wait(timeout=10)
                    except subprocess.TimeoutExpired:
                        print(f"üíÄ Wymuszam zamkniƒôcie {scraper_name}...")
                        process.kill()
                        
            except Exception as e:
                print(f"‚ùå B≈ÇƒÖd zatrzymywania {scraper_name}: {e}")
        
        # Zamknij wszystkie paski postƒôpu
        for progress_bar in self.progress_bars.values():
            progress_bar.close()
    
    def show_summary(self):
        """Pokazuje podsumowanie wykonanych scraper√≥w"""
        print("\nüìä PODSUMOWANIE:")
        print("=" * 80)
        
        for scraper_info in self.scrapers:
            scraper_name = scraper_info['name']
            db_file = scraper_info['db_file']
            description = scraper_info['description']
            stats = self.stats[scraper_name]
            
            if scraper_name in self.completed_scrapers:
                status = "‚úÖ Uko≈Ñczony"
                if os.path.exists(db_file):
                    size = os.path.getsize(db_file) / (1024 * 1024)  # MB
                    duration = ""
                    if stats['start_time'] and stats['end_time']:
                        duration = stats['end_time'] - stats['start_time']
                        duration = f" (czas: {duration})"
                    
                    print(f"{status} - {description:25} - {db_file} ({size:.1f} MB){duration}")
                else:
                    print(f"{status} - {description:25} - {db_file} (brak pliku)")
            elif scraper_name in self.failed_scrapers:
                status = "‚ùå B≈ÇƒÖd"
                print(f"{status} - {description:25} - {db_file}")
            else:
                status = "‚è≥ W trakcie"
                print(f"{status} - {description:25} - {db_file}")
    
    def merge_databases(self):
        """≈ÅƒÖczy wszystkie bazy danych w jednƒÖ wsp√≥lnƒÖ"""
        print("\nüîó ≈ÅƒÖczenie baz danych w merged_forums.db...")
        
        try:
            # Sprawd≈∫ czy istnieje skrypt do ≈ÇƒÖczenia
            merge_script = os.path.join('scripts', 'merge_all_databases.py')
            if os.path.exists(merge_script):
                print("üìã U≈ºywam istniejƒÖcego skryptu merge_all_databases.py...")
                
                result = subprocess.run(
                    [sys.executable, merge_script, "--target", "data/databases/merged_forums.db"],
                    capture_output=True,
                    text=True
                )
                
                if result.returncode == 0:
                    print("‚úÖ Bazy danych zosta≈Çy pomy≈õlnie po≈ÇƒÖczone")
                    if os.path.exists('data/databases/merged_forums.db'):
                        size = os.path.getsize('data/databases/merged_forums.db') / (1024 * 1024)  # MB
                        print(f"üìÅ Utworzono: data/databases/merged_forums.db ({size:.1f} MB)")
                else:
                    print("‚ùå B≈ÇƒÖd podczas ≈ÇƒÖczenia baz danych")
                    print(f"STDOUT: {result.stdout}")
                    print(f"STDERR: {result.stderr}")
            else:
                print("‚ùå Nie znaleziono skryptu merge_all_databases.py")
                
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas ≈ÇƒÖczenia baz danych: {e}")
    
    def save_final_stats(self):
        """Zapisuje ko≈Ñcowe statystyki do pliku JSON"""
        try:
            stats_file = self.log_dir / 'final_stats.json'
            
            # Przygotuj dane do zapisu
            final_stats = {
                'timestamp': datetime.now().isoformat(),
                'scrapers': {}
            }
            
            for scraper_name, stats in self.stats.items():
                final_stats['scrapers'][scraper_name] = {
                    'status': stats['status'],
                    'start_time': stats['start_time'].isoformat() if stats['start_time'] else None,
                    'end_time': stats['end_time'].isoformat() if stats['end_time'] else None,
                    'threads_found': stats['threads_found'],
                    'posts_found': stats['posts_found'],
                    'progress': stats['progress']
                }
            
            # Zapisz do pliku
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(final_stats, f, indent=2, ensure_ascii=False)
            
            print(f"üìä Statystyki zapisane do: {stats_file}")
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd zapisu statystyk: {e}")
    
    def run(self):
        """G≈Ç√≥wna metoda uruchamiajƒÖca ca≈Çy proces"""
        try:
            print("üéØ FINAL SCRAPER MANAGER - Uruchamianie wszystkich scraper√≥w")
            print(f"‚è∞ Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 80)
            
            # Uruchom wszystkie scrapery
            self.run_all_scrapers()
            
            # Po≈ÇƒÖcz bazy danych
            self.merge_databases()
            
            # Zapisz ko≈Ñcowe statystyki
            self.save_final_stats()
            
            print(f"\n‚è∞ Zako≈Ñczono: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("üéâ Wszystkie operacje zako≈Ñczone pomy≈õlnie!")
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd krytyczny: {e}")
            self.stop_all_scrapers()
            self.save_final_stats()
            sys.exit(1)

def main():
    """G≈Ç√≥wna funkcja"""
    try:
        # Sprawd≈∫ czy jeste≈õmy w odpowiednim katalogu
        if not os.path.exists('scrapy.cfg'):
            print("‚ùå B≈ÇƒÖd: Nie znaleziono pliku scrapy.cfg")
            print("Uruchom skrypt z katalogu g≈Ç√≥wnego projektu Scrapy")
            sys.exit(1)
        
        # Sprawd≈∫ czy wszystkie scrapery istniejƒÖ
        scraper_dir = 'scraper/spiders'
        if not os.path.exists(scraper_dir):
            print("‚ùå B≈ÇƒÖd: Nie znaleziono katalogu scraper/spiders")
            sys.exit(1)
        
        # Sprawd≈∫ czy zainstalowane sƒÖ wymagane pakiety
        try:
            import tqdm
        except ImportError:
            print("‚ùå B≈ÇƒÖd: Brak pakietu tqdm")
            print("Zainstaluj: pip install tqdm")
            sys.exit(1)
        
        try:
            import psutil
        except ImportError:
            print("‚ùå B≈ÇƒÖd: Brak pakietu psutil")
            print("Zainstaluj: pip install psutil")
            sys.exit(1)
        
        # Uruchom manager scraper√≥w
        manager = FinalScraperManager()
        manager.run()
        
    except KeyboardInterrupt:
        print("\nüõë Przerwano przez u≈ºytkownika")
        sys.exit(0)
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd krytyczny: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
